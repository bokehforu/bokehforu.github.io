var store = [{
        "title": "ELEC202 Lec 7&8 FM&PM",
        "excerpt":"mainly talk about FM&amp;PM in ELEC202 at Lec 7&amp;8   信号的过程   Components      Source encoder converts message into message signal.   Transmitter converts message signal or bits into format appropriate for channel transmission (analog/digital signal).   Channel introduces distortion, noise, and interference.   Receiver decodes received signal back to message signal.   Source decoder decodes message signal back into original message.   Communication System   COMP202 Lec 7&amp;8 FM&amp;PM           PM即时频率：$f_i(t) = f_c + k_p \\frac{dm}{dt}$，其中 $f_c$ 是载波频率，$k_p$ 是相位调制的灵敏度，$m(t)$ 是消息信号，$\\frac{dm}{dt}$ 是消息信号的导数。            FM即时频率：$f_i(t) = f_c + k_f m(t)$，其中 $f_c$ 是载波频率，$k_f$ 是频率调制的灵敏度，$m(t)$ 是消息信号。                                       频率偏差：$\\Delta f = \\max           f_i - f_c           $，频率偏差是指在频率调制中，载波频率最大的偏离程度。                                单音频FM信号：$X_{FM}(t) = A_c \\cos[2\\pi f_c t + \\beta \\sin(2\\pi f_m t)]$，这是一个单音频的频率调制信号的公式。其中 $A_c$ 是载波幅度，$f_c$ 是载波频率，$\\beta$ 是调制指数，$f_m$ 是消息信号的频率。            调制指数：$\\beta = \\frac{\\Delta f}{f_m}$，调制指数是频率偏差和消息频率的比值，在频率调制中，它是描述调制深度的关键参数。            卡森规则：$BW_{FM} \\approx 2(1 + \\beta)f_m$，这是卡森规则，用于估计频率调制信号的带宽。其中 $BW_{FM}$ 是频率调制信号的带宽，$\\beta$ 是调制指数，$f_m$ 是消息信号的频率。       如果 $\\beta « 1$，那么可以使用近似公式：$BW_{NB} \\approx 2f_m$，这是一个在调制指数很小的情况下用于估计频率调制信号带宽的近似公式。   LEC 7   FM频率调制   关于FM和PM，首先要知道三角函数的背景知识，定义一个正弦波的公式为   \\[\\mathrm{y}=\\mathrm{A} \\sin (\\mathrm{Bx}+\\mathrm{C})+\\mathrm{D}\\]     $\\mathrm{A}$ 是幅值   $\\mathrm{B}$是由$2 \\pi f$组成的   $C$ is the value of the phase   $D$ is the shifting of the $Y$ vaule in coordinate axes   So we could say that the trigonometric functions can be showed below  \\(x=A \\sin \\left(\\frac{2 \\pi}{T} t+\\varphi\\right)=A \\sin (2 \\pi f t+\\varphi)\\)   The key point here is I need understand the meaning and detail of phase and frequency. After it , we can talk about FM and PM right now.在上述等式中，T没有被讨论到。   FM定义  频率调制 调制是将载波信号的频率与载波信号的频率进行线性变化的过程，也是与信息信号进行线性变化的过程。 在PPT中，Dr.W give us that the definition of the FM which is a envaluation showing below. \\(f_i=f_c+k_f m(t)\\)     $k_f$ is the modulation sensitivity   modulation sensitivity 本质上是一个常数，调制灵敏度通常由偏差灵敏度来衡量，即输入信号频率给定变化时输出信号频率的变化。偏差灵敏度以赫兹/伏特（Hz/V）为单位表示，是衡量调频解调器的频率与电压传递函数的斜率。   在PPT中说，If the unit of $m(t)$ is volts, the unit of $kf$ is Hz/V. 我感觉$m(t)$的单位在课程背景中就是$v$   $fi$ is the final wave function的频率   $f{c}$ is the carrier frequncy which we need add in.   FM的瞬时频率  瞬时频率的 需要注意的是在这里需要明白频率和相位的关系。 频率是指信号在单位时间内完成的完整周期数，比如在一秒内完成的值。在这里举一个扩充的例子，$sin{（x）}$这个周期正弦波中，its $f$ is $1/2\\pi$. It is from the front fact about $B$. It indicates that in 1s, 在x轴上，这个函数前进了$1/2\\pi$的长度。这就是对频率$f$的定义。计算过程是 \\(B=1\\)   \\[B=2\\pi f\\]  \\[f=1/2\\pi\\]  因此，对于正弦波而言，频率可以表示为相位关于时间的导数，即f=dθ/dt。但是，这个关系式仅仅适用于正弦波这种特定形式的周期信号，并且仅仅在信号周期为定值时成立。 Instantaneous frequency: \\(f_i(t)=\\frac{1}{2 \\pi} \\frac{d \\theta(t)}{d t}=f_c+\\frac{1}{2 \\pi} \\frac{d \\varphi(t)}{dt}\\) 对于这个式子的解释是$\\theta(t)$是例如$Sin\\theta(t)$中的幅度角的值。由我们上面得出的正弦波的幅度和相位的关系和$B$的组成，可以知FM的调制方法后的信号的瞬时频率是 \\(f_i(t)=\\frac{1}{2 \\pi} \\frac{d \\theta(t)}{d t}\\) 解释一下就是由当前调制后信号的角度，经过$2\\pi f$得出$f$后，求导得出此时的瞬时频率，同时由于FM的调制方法，我们可以得出载波信号的频率是不会改变的，在变化的是我们的$m(t)$.$m(t)$有自己的幅度角，对其求导，得出关系式的后半部分。 \\(f_i(t)=f_c+\\frac{1}{2 \\pi} \\frac{d \\varphi(t)}{dt}\\) 由于我们是FM，所以对于载波信号是直接改变信号的频率的，而由频率的变化转化成幅度角的变化是由之前的关系得出的，所以对于FM中我们已知的变化的信号对应的frequency，我们需要使用   **相位 **   \\(\\varphi(t)=k_f \\int_{-\\infty}^t m(\\tau) d \\tau\\) 来得出我们需要的角幅度，再在(8)中被求导，来得出我们需要的瞬时频率。   PM的瞬时频率公式   Instantaneous frequency: \\(f_i(t)=\\frac{1}{2 \\pi} \\frac{d \\theta(t)}{d t}=f_c+\\frac{1}{2 \\pi} \\frac{d \\varphi(t)}{dt}\\)   PM调制直接是变化幅度角度，所以在使用此公式求瞬时frequency的时候，我们使用 \\(\\varphi(t)=k_p*m(t)\\)   Maximum-freq. deviation deviation in in FM   在频率调制（FM）中，最大频率偏差是指在信号传输过程中，载波频率被调制或偏离其原始频率的最大数量。最大频率偏差通常被指定为调频信号的一个参数，被定义为调制信号的瞬时频率与载波频率之间的最大差异。它通常以赫兹（Hz）为单位表示，与调制信号的振幅成正比。   最大频率偏差决定了调频信号的带宽。   公式如下,载波频率被调制或偏离其原始频率的最大数量。可以看到m(t)就是原始数据的函数表达 \\(\\Delta f_{\\max }=\\max \\left(f_i-f_c\\right)=k_f \\max [|m(t)|]\\)   FM modulation  index   \\[\\beta_f=\\frac{\\Delta f_{\\max }(调制指数)}{W(基带)}\\]  where W  is the bandwidth of m(t).   PM Signal Representation   PM调制是一种模拟调制技术，是指通过改变载波信号的相位来传输模拟信号。在相位调制中，相位与调制信号的瞬时振幅成正比。 \\(x(t)=A_c \\cos \\theta(t)=A_c \\cos \\left(2 \\pi f_c t+\\varphi(t)\\right)\\) 这里要重新提什么是相位，就是最开头的C，而对于x(t)来说，这个等式描述的是调制后的信号图像，可以看到，有$f_c$和$\\varphi(t)$ 这个本身在变化的信息上的角的度数。   这个公式同时对FM和PM适用，用来求出他们的实际调制后的函数图像。我们使用不同的$\\varphi(t)$来对应不同的调制方法。   FM   $\\varphi(t)$等于 \\(\\varphi(t)=k_f \\int_{-\\infty}^t m(\\tau) d \\tau\\) 原因之前有提过，FM我们只有变化的频率值，而在这我们需要知道变化的频率对应的角度，所以使用积分求出对应的角度   PM   在PM中，我们直接改变的就是相位角，使用$k_p* angle$的方法，就是我们改变的大小，所以在求的时候$\\varphi(t)$不用经过任何处理   $\\varphi(t)$等于 \\(\\varphi(t)=k_p\\times m(t)\\)   $k_p$and $k_f$ are deviation constants for PM and FM, respectively.The maximum-phase deviation in PM system is given by \\(\\Delta \\varphi_{\\max }=k_P \\max [|m(t)|]\\)   PM modulation index   \\[\\beta_P=\\Delta \\varphi_{\\text {max }}\\]  上面是什么  零零碎碎记的东西   小细节   Frequency or phase of the carrier varies according to the message signal. This is further divided into frequency and phase modulation.   Frequency Modulation is the process of varying the frequency of the carrier signal linearly with the message signal. Phase Modulation is the process of varying the phase of the carrier signal linearly with the message signal.   AM和FM的优劣   与调幅 Amplitude (AM)相比，角度Angle(AM)调制的优点：   更有效地使用功率： 像FM和PM这样的角度调制技术比AM更有效地使用功率，因为它们通过改变载波信号的频率或相位，而不是振幅来传输信息。载波信号的振幅保持不变，这意味着传输功率保持不变，从而更有效地利用功率。 更好的信号质量： 与调幅相比，角调制技术更不容易受到噪音和干扰的影响。角度调制信号的信噪比（SNR）更高，这意味着信号质量更好。这是因为噪声和干扰主要影响信号的振幅，而在角度调制技术中，振幅保持不变。   更大的带宽：   角度Angle(AM)调制比AM(classic)的缺点：   通常更大的带宽：角度调制技术通常需要比AM更大的带宽。这是因为载波信号的频率或相位被调制，这导致边带在频率上扩散，导致更宽的带宽。 较小的范围： 与调幅相比，角度调制技术的范围较小。这是因为接收器需要一个更高的信噪比（SNR）来解调信号。频率偏差越高，所需的信噪比就越高，这就限制了传输的范围。此外，角度调制信号受多径传播的影响，会造成干扰并降低范围。   角度调制特性   角度调制的特点有：      FM/PM信号的功率不随调制而变化。   FM/PM信号没有能够复制调制的包络。   载波频偏与调制信号的幅度成正比。   简要解释：      在角度调制中，频率和相位被用来调制载波信号，而不是振幅。因此，信号的功率不受调制影响，与原始载波信号的功率相同。   由于调制是通过改变相位或频率来实现的，因此无法在FM/PM信号的包络中复制出调制信号的形状。这与振幅调制(AM)不同，因为在AM中，信号的包络可以准确地复制出原始信号的形状。   调制信号的幅度变化会导致载波频率的偏移，这种频率偏移与调制信号的幅度成正比。因此，当调制信号的幅度变化较大时，载波的频率偏移也会相应地变大。   可以再放一个图片解释FM和PM的调制过程   LEC 8   Bandwidth estimation for FM, Carson’s Rule – Narrowband FM  – Wideband FM   FM Demodulation   – Slope detector  – Zero-crossing detector   Angle Modulation : recap   Modulated signal调制过的信号 is： $s(t)=A_c \\cos \\theta(t)$  $\\theta(t)$ is the angle:            $\\theta(t)=f(m(t))$   Standard FM:           $\\theta(t)=2 \\pi \\mathrm{f}{\\mathrm{c}} t+k{\\mathrm{f}} \\int \\mathrm{m}(\\tau) d \\tau$   频率调制 调制是将载波信号的频率与载波信号的频率进行线性变化的过程，也是与信息信号进行线性变化的过程。 在PPT中，Dr.W give us that the definition of the FM which is a envaluation showing below. \\(f_i=f_c+k_f m(t)\\) 所以  \\(s(t)=A_c \\cos (2 \\pi \\mathrm{f}_{\\mathrm{c}} t+k_{\\mathrm{f}} \\int \\mathrm{m}(\\tau) d \\tau)\\) Single tone FM signal \\(x_{F M}(t)=A_c \\cos \\left[2 \\pi f_c t+\\beta \\sin \\left(2 \\pi f_m t\\right)\\right]\\) 瞬时频率 \\(f_i(t)=\\frac{1}{2 \\pi} \\frac{d \\theta(t)}{d t}=f_c+\\frac{1}{2 \\pi} \\frac{d \\varphi(t)}{dt}\\)   对于这个函数来说，$ \\beta \\sin \\left(2 \\pi f_m t\\right)$ 和$k_{\\mathrm{f}} \\int \\mathrm{m}(\\tau) d \\tau$ 是相等的   Instantaneous frequency:    $f_i=f_c+k_f m(t)$   $kf$ is just a constant – modulation sensitivity.   phase in the integral of frequency, and frequency is the derivative of the phase   Instantaneous frequency = derivative of the angle   (for PM) $\\omega_i(t)=\\omega_c+k_p m^{\\prime}(t)$   (for FM) $\\omega_i(t)=\\omega_c+k_f m(t)$   Frequency deviation $=$ frequency sensitivity $\\times \\max $(message)   Frequency swing $=2 \\times$ frequency deviation   Modulation index $=$ deviation ratio $=\\beta$   $\\beta$$\\begin{aligned} &amp; =\\text { maximum frequency deviation } \\div \\text { baseband bandwidth } \\ &amp; =\\Delta f / f_{\\mathrm{m}}\\end{aligned}$   Bandwidth   Narrowband FM   Demodulation   ","categories": [],
        "tags": [],
        "url": "/comp-202/",
        "teaser": null
      },{
        "title": "ELEC211: Digital Electronics & Microprocessor Systems",
        "excerpt":"    The content contains two note-files, which only included the content from the ARM M0 teached by Dr. Pineapple Pizza. :)    The content has not been edited or corrected and may contain some errors.           The content of this file should be correct. I use this file to prepare for the final.                         ELEC 211       Digital Electronics &amp; Micro       88       Pass                               211       Written Exam (75%)       86                                 211.1       Course Work (10%)       100                                 211.2       Course Work (10%)       90                                 211.3       Course Work (5%)       88                          ","categories": [],
        "tags": [],
        "url": "/elec211pdf/",
        "teaser": null
      },{
        "title": "Proof of Cramer’s Rule",
        "excerpt":"The article contains three proofs.   证明一   证明：假设A是一个可逆的n × n矩阵，我们考虑线性系统AX = b。这个系统有一个唯一的解：   转换不了解，第二步转换 感觉是基础问题不知道 \\(X = A^{-1}b = \\frac{1}{|A|}\\text{adj}(A) \\cdot b = \\frac{1}{|A|} \\left[ \\begin{array}{cccc} A_{11} &amp; A_{21} &amp; \\cdots &amp; A_{n1} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ A_{1n} &amp; A_{2n} &amp; \\cdots &amp; A_{nn} \\end{array} \\right] b\\)   这里的 $\\cdot$ 表示矩阵乘法。通过矩阵乘法的公式，第k个未知数$x_k$可以写成：   \\[x_k = \\frac{1}{|A|} \\sum_{i=1}^{n} A_{ik}b_i = \\frac{1}{|A|} \\text{det} [c_1, \\ldots, c_{k-1}, b, c_{k+1}, \\ldots, c_n]\\]  这里$c_i$表示A的第i列。第二个等式来自于对矩阵 $[c_1, \\ldots, c_{k-1}, b, c_{k+1}, \\ldots, c_n]$ 沿第k列进行的余子式展开。   证明二   首先，假设我们有一个可逆的矩阵$A$，且$\\det(A) \\neq 0$，并且$Ax = b$是一个线性方程。我们声明这个方程有一个唯一解。注意到$A^{-1}b$是一个解，因为$A(A^{-1}b) = (AA^{-1})b = b$，所以解是存在的。   假设$s$是这个方程的一个任意解，所以$As = b$。但是，我们可以得到$s = (A^{-1}A)s = A^{-1}(As) = A^{-1}b$，所以$A^{-1}b$是唯一的解。   对于每一个整数$i, 1 \\leq i \\leq n$，让$a_i$表示$A$的第$i$列，$e_i$表示单位矩阵$I_n$的第$i$列，并且让$X_i$表示从$I_n$通过替换第$i$列为列向量$x$得到的矩阵。   我们知道对于任意的矩阵$A, B$，乘积$AB$的第$k$列是$A$和$B$的第$k$列的乘积。同时，观察到$Ae_k = a_k$对于$k = 1, …, n$。因此，通过乘法，我们有：   \\[\\begin{aligned} A X_i &amp; =A\\left(e_1, \\ldots, e_{i-1}, x, e_{i+1}, \\ldots, e_n\\right) \\\\ &amp; =\\left(A e_1, \\ldots, A e_{i-1}, A x, A e_{i+1}, \\ldots, A e_n\\right) \\\\ &amp; =\\left(a_1, \\ldots, a_{i-1}, b, a_{i+1}, \\ldots, a_n\\right) \\\\ &amp; =M_i \\end{aligned}\\]  因为$X_i$是用$x$替换了$I_n$的第$i$列，通过余子式展开计算$X_i$的行列式我们得到 ： \\(det(X_i) = (-1)^{i+i}x_i\\det(I_{n-1}) = 1 \\cdot x_i \\cdot 1 = x_i\\)   因此，根据行列式的乘法性质，我们有 \\(det(M_i) = \\det(AX_i) = \\det(A)\\det(X_i) = \\det(A)x_i\\)   所以，$x_i = \\frac{\\det(M_i)}{\\det(A)}$，这就是我们所需要的。   证明三   形式化总结   线性方程的矩阵表示 \\(Ax=c\\) 其中，系数矩阵 $A$ 是一个$  n×n$ 的方阵，$ x=(x_1,x_2,\\cdots,x_n)^T$是一个长度为$n$ 的列向量，$c= (c_1,c_2,\\cdots, C_n)^T$也是一个长度为n的列向量。   根据克拉默法则，方程的解为： \\(x_i=\\frac{D_i}{D},i=1,2,\\cdots,n\\)   其中， $Di$ 是用 $C$替换了$A$ 的第$i$列得到的矩阵的行列式,$D$则是$A$的行列式。可以看到，当$D$等于0时，原方程无解。   证明过程   将线性方程组 $A x=c$ 两边同时左乘 $A$ 的逆, 得到 $A^{-1} A x=A^{-1} c$, 即： \\(x=A^{-1} c \\text { 。 }\\) 我们将系数矩阵表示成列向量的形式, 即: \\(A=\\left(p_1, p_2, \\ldots, p_n\\right)\\) 因此原方程又可以写成: \\(\\sum_{k=1}^n x_k p_k=c\\) 则行列式 $D_i$ 的值可以表示为:第二个等号到第三个等号，可以理解为行和列等价，提出一个scalar？  \\(\\begin{aligned} D_i &amp; =\\operatorname{det}\\left(p_1, \\ldots, p_{i-1}, c, p_{i+1} \\ldots, p_n\\right) \\\\ &amp; =\\operatorname{det}\\left(p_1, \\ldots, p_{i-1}, \\sum_{k=1}^n x_k p_k, p_{i+1} \\ldots, p_n\\right) \\\\ &amp; =\\sum_{k=1}^n x_k \\operatorname{det}\\left(p_1, \\ldots, p_{i-1}, p_k, p_{i+1} \\ldots, p_n\\right) \\\\ &amp; =x_1 \\operatorname{det}\\left(p_1, \\ldots, p_{i-1}, p_1, p_{i+1} \\ldots, p_n\\right)+x_2 \\operatorname{det}\\left(p_1, \\ldots, p_{i-1}, p_2, p_{i+1} \\ldots, p_n\\right)+\\ldots \\\\ &amp; \\quad+x_n \\operatorname{det}\\left(p_1, \\ldots, p_{i-1}, p_n, p_{i+1} \\ldots, p_n\\right) \\\\ &amp; =x_i \\cdot \\operatorname{det}(A) \\quad \\\\ &amp; =x_i \\cdot D \\quad \\end{aligned}\\) 所以 $x_i=\\frac{D_i}{D}$   总结   克拉默法则的表述                  数域 $K$ 上 $\\mathrm{n}$ 个方程的 $\\mathrm{n}$ 元线性方程组有唯一解的充要条件 $       A       \\neq 0$, 且解为 $\\left(\\frac{\\left       B_1\\right       }{       A       }, \\frac{\\left       B_2\\right       }{       A       }, \\cdots, \\frac{\\left       B_n\\right       }{       A       }\\right)$, 其中 $B_i$ 为用常数列替换 $A$ 第i列后的矩阵。           线性空间角度                  若 $\\alpha_1, \\alpha_2, \\ldots, \\alpha_n$ 是 $K^n$ 的一组基, $\\beta \\in K^n$ ，则 $\\beta=\\sum_{i=1}^n \\frac{\\left       B_i\\right       }{       A       } \\alpha_i$ 。           问题 如何理解这一结果?                                      $\\frac{\\left           B_i\\right           }{           A           }$ 有什么意义?                           行列式有什么意义?                                   为什么是 $\\frac{\\left           B_i\\right           }{           A           }$ ?                           对任意线性空间是否成立类似的结论?   能否类比定义域 $F$ 上的线性空间 $V$ 上的行列式 $\\Delta: V^n \\rightarrow F$, 从而有类似结论?   ​   几何角度   在 $K^n$ 中将一标架作非退化仿射变换 $\\sigma$ : \\(\\left\\{\\begin{array}{c} K^n \\rightarrow K^n \\\\ \\left\\{x_1, x_2, \\ldots, x_n\\right\\} \\mapsto\\left\\{\\sum_{j=1}^n f_1\\left(\\alpha_j\\right), \\sum_{j=1}^n f_2\\left(\\alpha_j\\right), \\ldots, \\sum_{j=1}^n f_n\\left(\\alpha_j\\right)\\right\\} \\end{array}\\right.\\) ,设 $\\delta=\\left(x_1, x_2, \\ldots, x_n\\right)^T$, 即坐标变换公式: $\\delta=A \\delta^{\\prime}$, 那么有 $\\delta^{\\prime}=\\left(\\frac{\\left|B_1\\right|}{|A|}, \\frac{\\left|B_2\\right|}{|A|}, \\ldots, \\frac{\\left|B_n\\right|}{|A|}\\right)^T$   表层意义   也就是说, 克拉默法则的几何理解就是它给出了非退化仿射变换变换后的新坐标公式。有了这个工具, 在做非退化仿射变换的时候就能更方便地求新坐标了。   本质意义   进一步地, 当 $A$ 是一个正交矩阵, 即做的是正交变换（以同向为例），那么有  \\(\\delta^{\\prime}=A^{\\prime} \\delta=\\left(\\left|B_1\\right|,\\left|B_2\\right|, \\ldots,\\left|B_n\\right|\\right)^T\\)   于是有 \\(\\alpha_i \\cdot \\delta=\\left|B_i\\right|(i=1,2, \\ldots, n)\\)   考虑原标架是右手直角标架, 此时等号右边表示 $\\delta$ 在第 $\\mathrm{i}$个纬度上的分量为“高”, 其他单位坐 标向量为“底”的几何体的“体积”。而“底”的面积为1, 所以这个“体积”就等于“高”。易知等号左边则表示 $\\delta$ 在第 $\\mathrm{i}$ 个纬度上的分量。   考虑原标架是任意标架，而且$ A$ 是任意可逆矩阵，那么可以类比地理解：一个向量在用一组新向量线性表出时在每个新向量的维度上的分量是以它为“高”、别的新坐标向量为“底”的几何体的体积比上新向量组构成的几何体的体积。   关于证明           行列式的定义和应用：行列式是一个从数域的$n^2$维向量空间映射到数域自身的函数。在一个数域$F$上，我们可以定义一个行列式$\\Delta$如下：   \\[\\Delta : F^{n^2} \\rightarrow F\\]      这个函数在$n$维线性空间$F^n$上是有效的。            正交单位化：在一个数域$F$上，我们可以对$n$维线性空间$F^n$的任何基进行正交单位化。这可以把基转换为一个右手直角坐标系。            克拉默法则的本质和应用：克拉默法则是一个几何直观的抽象，它在度量线性空间中有效。度量线性空间能表示诸如”面积”、”体积”这样的几何概念，而非度量空间不一定可以。            线性方程组与线性空间的关系：线性方程组背后隐含了线性空间的结构，而二维和三维的几何可以帮助我们直观地理解数量关系（高维几何则需要依赖代数来理解）。       结   “克拉默法则的本质是将几何直观抽象到度量线性空间。”   reference   https://ccjou.wordpress.com/2009/11/10/%E5%85%8B%E6%8B%89%E7%91%AA%E5%85%AC%E5%BC%8F%E7%9A%84%E8%AD%89%E6%98%8E/   https://zhuanlan.zhihu.com/p/57944485   https://zhuanlan.zhihu.com/p/262988664   https://planetmath.org/proofofcramersrule  ","categories": [],
        "tags": [],
        "url": "/proof-of-Cramer-s-rule/",
        "teaser": null
      },{
        "title": "MIT missing Cour",
        "excerpt":"进度条…(2/8)   Lecture 1 Shell           bash中如果想提供一个包含空格的参数，可以用‘或”“把它们引起来，或者用转义字符\\，比如My\\ Photos来将空格转义            $PATH是环境变量，即在本地文件夹下没有找到对应可以执行的程序时会自动在环境变量的路径中寻找相应的程序。which提示了能够运行本指令的程序路径，比如       ~$ which echo /bin/echo             ls -l表示以详细方式列出当前文件夹下的文件       missing:~$ ls -l /home drwxr-xr-x 1 missing  users  4096 Jun 15  2019 missing        第1个字符表明这个文件的类型。d表示这是一个文件夹，如果是-表示这是一个普通文件，l表示这是一个链接文件，类似于windows下的快捷方式，b表示这是一个块设备文件，一般置于/dev目录下，没有文件大小，只有一个主设备号和辅设备号。块设备是一次传输一整块数据的设备，比如硬盘。c表示这是一个字符设备文件，一般置于/dev目录下，字符设备是一次只传输一个字符数据的设备，比如键盘。p表示这是一个命令管道文件，与shell编程有关，s表示这是一个socks文件，与shell编程有关       d后面有3*3个标志，表示不同的身份对该文件的权限。r表示可读权限，w表示可写权限，x表示可执行权限，-表示无相应权限。第一组表示该文件的所有者的权限，第二组表示文件所有者同组用户的权限，第三组表示其他用户的权限       权限后面的第一个数字表示1. 当这是一个文件时，为硬连接数，即有几个文件硬链接到了这个文件 2. 当这是一个文件夹时，为链接占用的节点，即该目录中包含的子目录的个数       对于一个文件夹来说，为了进入这个文件夹，必须拥有”search”权限，也就是拥有对这个文件夹以及其所有父路径文件夹的x权限。为了ls这个文件夹，必须拥有这个文件夹的r权限            重定向输入输出流：&lt; file将输入设定为文件，&gt; file将结果输出到文件，原先文件的内容会被覆盖       missing:~$ echo hello &gt; hello.txt missing:~$ cat hello.txt hello missing:~$ cat &lt; hello.txt hello missing:~$ cat &lt; hello.txt &gt; hello2.txt missing:~$ cat hello2.txt hello        &gt;&gt;可以来向文件附加数据。|是管道符号，可以将前一个命令的输出作为下一个命令的输入。       curl --head --silent google.com | grep --ignore-case content-length | cut --delimiter=' ' -f2 # 查看向google发送HTTP GET请求的头文件中的content length属性的值        文件描述符：一般情况下每个unix命令运行时都会打开3个文件：              标准输入文件(stdin)：文件描述符为0，unix默认从stdin输入数据       标准输出文件(stdout)：文件描述符为1，unix默认向stdout输出数据       标准错误文件(stderr)：文件描述符为2，unix会向stderr流中写入错误消息           默认情况下，command&gt;file将stdout重定向到file，command&lt;file将stdin重定向到file       command 2&gt;&gt;file # 将stderr追加到文件末尾//        /dev/null文件是一种特殊的文件，写入到它的内容都会被丢弃，也无法从中读取到任何内容，如果希望执行某个命令，但是不希望在屏幕上显示输出结果，可以将输出重定向到/dev/null       command &gt;&gt; /dev/null 2&gt;&amp;1 # 屏蔽stdout和stderr        command &gt;&gt; /dev/null已经将标准输出重定向，2&gt;&amp;1中的&amp;表示等同，2&gt;表示错误输出，2&gt;&amp;1表示错误输出重定向的对象等同于标准输出重定向的对象，即/dev/null            sysfs：Linux内核下基于内存的文件系统，可以将很多内核参数以文件形式暴露，从而可以方便地修改kernel。比如笔记本电脑的屏幕亮度可以以文件的形式在sys/class/backlight下被暴露       $ sudo find -L /sys/class/backlight -maxdepth 2 -name '*brightness*' /sys/class/backlight/thinkpad_screen/brightness $ cd /sys/class/backlight/thinkpad_screen $ sudo echo 3 &gt; brightness  # permission denied，因为&gt;重定向符号之前的sudo并不能被后面观察到，也就是说写入brightness这个操作实际上并没有执行sudo $ echo 3 | sudo tee brightness  # success，tee这个命令是获取标准输入，将内容输出成文件，并将其打印到屏幕上             chmod(change mode)来控制用户对文件的权限的命令。只有文件拥有者(owner)和超级用户(super user)可以修改文件或者目录的权限       chmod [-cfvR] [--help] [--version] mode file... # mode格式为 [ugoa...][[+-=][rwxX]...][,...] # u表示该文件的拥有者，g表示与该文件的拥有者属于同一个group者，o表示其他人，a表示这三者皆是 # +表示增加权限，-表示取消权限，=表示唯一设定权限 # r表示可读取，w表示可写入，x表示可执行，X表示只有当该文件是个子目录或者该文件已经被设定过为可执行 # 示例:将file1.txt设定为所有人皆可读取 chmod ugo+r file1.txt        也可以采用八进制的方法来规定权限                                  #           权限           rwx           二进制                                           7           读+写+可执行           rwx           111                             6           读+写           rw-           110                             5           读+执行           r-x           101                             4           只读           r–           100                             3           写+执行           -wx           011                             2           只写           -w-           010                             1           只执行           –x           001                             0           无           —           000                           # file1.txt这个文件对所有的用户均可读可写可执行 chmod 777 file1.txt # file2.txt这个文件对其他用户只可执行 chmod ug=rwx,o=x file2.txt # 与以下相同 chmod 771 fil2.txt        一般比较常用的是chmod 755和chmod 777            shebang: #!       写在脚本的第一行，用来规定该脚本的解释器。#!后接解释器的绝对路径。比如想要规定这个脚本用sh来执行，那么在第一行添加       #!/bin/sh        推荐使用/usr/bin/env python来规定该脚本解释器，这是因为env会在$PATH中查找python解释器的安装位置，这样可以不用提供一个解释器的绝对路径，从而提高程序的可移植性            后台执行shell       在shell命令的最后一个位置加&amp;       Lecture 2 Shell Tools and Scripting   Shell Scripting   Shell是一个用C语言编写的程序，是一种解释性语言。Windows Explorer是一个典型的图形界面shell   Bourne Shell /bin/sh或/usr/bin/sh   Bourne Again Shell /bin/bash           给变量赋值：       注意在定义变量时变量名不加美元符号，变量名和等号之间不能有空格。在使用一个已经定义过的变量时需要加美元符号       foo=bar # 注意不能是foo = bar，否则bash会认为这是运行了foo命令，并以=和bar作为参数传入 echo \"$foo\" # prints bar echo '$foo' # or echo \"${foo}\" # 花括号加不加可选，主要是为了清晰变量名的边界 # prints $foo # 注意在bash中\"和‘是不同的。在'中$变量不会被替换，\"中$后面的变量会被替换为其值        readonly变量是只读变量，不能被赋值       myurl=\"www.fanxiao.tech\" readonly myurl        使用unset命令可以删除变量            获取字符串的长度       string=\"abcd\" echo ${#string} # 输出4             提取子字符串       从字符串的第2个字符开始截取四个字符       string=\"runoob is a great site\" echo ${string:1:4} # 输出unoo             函数       # 以shell脚本的名称创建一个文件夹并cd到这个文件夹中 mcd(){ mkdir -p \"$1\" cd \"$1\" }               $0：脚本本身的名称       $1-$9：脚本的第1-第9个参数       $@：所有脚本的参数       $#：脚本参数的个数       $?：前一个命令的返回代码       $$：当前脚本的PID       !!：完整的上一个命令，包括参数。如果一个命令只是因为没有root权限失败，则可以执行sudo !!来重新执行该命令       $_：上一个命令的最后一个参数                数组       数组索引从0开始。用括号()来表示数组，数组元素用空格分开。定义数组的一般形式为       数组名=(值1 值2 值3 值4)       如       array_name=(value0 value1 value2)        还可以单独定义数组的各个分量，如array_name[0]=1       读取数组元素值的方式是$(数组名[下标])，使用@则可以获取数组中的所有元素，例如       echo ${array_name[@]}        获取数组的长度的方法是       length=${array_name[@]}             bash基本运算       原生bash不支持简单的数学运算，但是可以通过其他命令实现，比如expr       val=expr 2 + 2   # 注意：表达式和运算符之间一定要有空格，例如2+2是不对的，必须写成2 + 2,注意不是单引号而是反引号 echo \"两数之和为$val\"  # 输出为\"两数之和为4\"        条件表达式要放在方括号之间，并且要有空格，例如       if [$a == $b] then echo \"a等于b\" fi if [$a != $b] then echo \"a不等于b\" fi        关系运算符：                                  运算符           说明                                           -eq           检测两个数是否相等，相等则返回true                             -ne           检测两个数是否不等，不等则返回true                             -gt           检测左边的是否大于右边的，大于则返回true                             -lt           检测左边的是否小于右边的，小于则返回true                             -ge           检测左边的是否大于等于右边的，大于等于则返回true                             -le           检测左边的是否小于等于右边的，小于等于则返回true                             !           非运算                             -o           或运算                             -a           与运算                           注意：乘号前必须要加上反斜杠\\转义才是乘法运算            test命令可以用于检查某个条件是否成立       # 数值测试 num1=100 num2=200 if test $[num1] -eq $[num2] then echo '两个数相等' else echo '两个数不相等' fi # 文件测试 cd /bin if test -e ./bash then  echo '文件存在' else echo '文件不存在' fi        注意：新的test[[]]比旧的test[]更好，尽量使用[[]]            流程控制：shell编程的流程控制不可为空，即if和else的代码块里必须执行一定的动作       将if和else写成一行的方法：       if [ $(ps -ef | grep -c \"ssh\") -gt 1]; then echo \"true\"; fi # 查找当前所有进程中ssh进程的个数，如果大于1则返回true        for循环的一般格式       for var in item1 item2 ... itemN do command1 command2 commandN done        写成一行：       for var in item1 item2 itemN; do command1; command2; commandN; done;        while循环：       int=1 while [ $int -lt 5 ] do echo $int let \"int++\"  # let是bash中用于计算的工具，变量计算中不需要加上$表示变量 done # 运行结果 # 1 # 2 # 3 # 4 # 5        case选择       每个case分支从右圆括号开始，用两个分号;;表示break，跳出整个case...esac语句       echo '输入1到4之间的数字' echo '你输入的数字为' read aNum case $aNum in 1) echo '你选择了1' ;; 2) echo '你选择了2' ;; 3) echo '你选择了3' ;; 4) echo '你选择了4' ;; *) echo '你没有输入1到4之间的任何数字' ;; esac        break命令       while true do echo -n \"输入1到5之间的数字\" read aNum case $aNum in \t1|2|3|4|5) echo \"你输入的数字为$aNum!\" \t;; \t*) echo \"你输入的数字不是1到5之间的！结束\" \t\tbreak        ;;    esac done             shell函数       [function] funname [()] { action [return int] } # example demoFun(){ echo \"第一个shell函数\" } demoFun # 调用这个函数             程序的返回值：执行成功返回0，执行失败则返回其他大小的数值。注意，和C语言不同，shell中0表示true，1表示false       获取一个程序的变量值，如$(CMD)会先执行CMD，获取CMD的输出并再相应位置进行替换。比如执行for file in $(ls)，将先执行ls，然后遍历执行ls后获得的返回值            进程替换(process substitution)：&lt;(CMD)将执行CMD，然后将结果输出到一个临时文件，并把&lt;()替换为这个文件名。比如cat &lt;(ls -l)相当于ls -l | cat，diff &lt;(CMD1) &lt;(CMD2)是比较这两个CMD的区别            shell文件包含       包含外部脚本，以封装一些公用的代码作为一个独立文件       . file # 注意.和文件名中间有一个空格 # 或者 source file             shell通配符(globbing)       wildcard: 使用*或?来进行匹配，比如有foo1、foo2、foo几个文件，rm foo?将删除foo1和foo2，而rm foo*将删除foo1、foo2和foo等三个文件            花括号{用来扩展子字符串       convert image.{png,jpg} # will expand to convert image.png image.jpg      ","categories": [],
        "tags": [],
        "url": "/Lec1-and-lec2-content/",
        "teaser": null
      },{
        "title": "Injective, Surjective and bi-jective",
        "excerpt":"A small concept in discrete mathematics   函数的类别 Injective, Surjective and bi-jective   Functions, Domain, Codomain,Range   Function（函数）   函数是一种关系，在给定定义域的情况下，将定义域中的每个元素映射到唯一的值域中的元素。函数通常用 f(x) 或 g(y) 等符号表示，其中 x 和 y 是定义域和值域中的元素。函数描述了输入和输出之间的映射关系。   Domain（定义域）   The set of input values   定义域是函数的所有可能输入值的集合。它是函数可以接受的输入范围。例如，对于函数 f(x) = x^2，定义域可以是实数集合 R，因为任何实数都可以作为该函数的输入。   Codomain（值域）   The set of possible output values 那我感觉codomian就一直是$\\R$ 我有点不太理解这个codmain这个set是如何产生的，一个   值域是函数中所有可能的输出值的集合。它是函数可以产生的所有可能结果的范围。与定义域不同，值域不一定是函数实际映射到的值的集合，而是函数可能生成的所有结果的集合。例如，对于函数 f(x) = x^2，值域可以是非负实数集合 R≥0，因为平方函数的结果始终为非负实数。   Range（值域*）   The set of actual output values   Range是函数实际映射到的值的集合。它是函数在给定定义域上生成的所有实际输出值的集合。通常，值域是通过确定函数的所有可能输出值并排除任何不可达的值来确定的。例如，对于函数 f(x) = x^2，如果定义域为实数集合 R，那么值域将是非负实数集合 R≥0，因为平方函数的结果始终为非负实数。   Injective, Surjective and bi-jective定义和简明解释   Definition   来源： https://www.math.fsu.edu/~pkirby/mad2104/SlideShow/s4_2.pdf   Given $f: A \\rightarrow B$           $f$ is one-to-one (short hand is $1-1$ ) or injective if preimages(预先映射) are unique. In this case, $(a \\neq b) \\rightarrow(f(a) \\neq f(b))$. 一个a只匹配一个在set B中的b            $f$ is onto or surjective if every $y \\in B$ has a preimage. In this case, the range of $f$ is equal to the codomain.我感觉codomian和range的关系没搞明白            $f$ is bijective if it is surjective and injective (one-to-one and onto).       来源：YouTube视频      one to one / injective            For  every input there is unique output. All the elements in the domain  have to be used, but all elements in the co-domain need not be used.就是在input的元素中，映射出去，co-domain中要留有剩余。                  onto / Surjective            (Range  = co-domain)  Every element in the co-domain is the image of at least one element in  the domain.       或者说range要=co-domain           bijective            These functions are one-one functions but every element in the co-domain  is used.           我的理解   对于函数，我们考虑到A这个set和B这个set中存在的一些元素，这是背景信息要求。那么，当A的元素出发对应到了B中的元素，这就是满足函数的关系。在这个基础上有两个方向。第一个方向是one to one，这个对满足function的关系的要求基础之上，额外要求对于set A中的元素应该全部射出，set B可以留有空白的元素。 而另外一个方向，则是指明了在set B中，B中的元素全部至少被A中的一个元素被射中，甚至可以被A中的多个元素射中。而bijective 就是这两个路径的叠加，就是set A中的元素被全部射出，而set B中的元素全部被set A中的元素对应   上面的理解感觉是错的 ，红色部分one to one的理解错了   新的理解   https://www.youtube.com/watch?v=MY4-5mXfWzo&amp;ab_channel=Areallnamesgone   视频中显示，函数的定义是set A中的元素都是单个箭头出发，而不是从a1出发有两个箭头，满足了这个关系就是函数，我觉得这个好理解，因为满足了线性关系后，set A中的元素对应了两个set B中的元素(比如f(1)=2但是f(1)又可以=34)，这非常奇怪了，线性关系就被破坏了。继续到one to one，可以进行字面意思的理解，one to one就是在set A到 set B的映射关系中，一个在set A(domain)中的元素只可以唯一对应set B中的元素，比如(set A{a_1,a_2,a_3,a_4}和set B{b_1,b_2,b_3,b_4,b_5}这里的元素，假设他们下标匹配对应，a_1配备b_1…最后留下b_5是空的，没有被A中的元素中对应的，这样也是可以的。因为这个case满足了我们对于set A中的元素都要可以对应set B中的这种情况)。继续onto，这个要求是set B中的元素at least 都要被A中出发的元素射中，就是说set B没有空的元素，但是一个在set B中的元素可以被很多从set A的元素出发的箭头(关系)对应。最后bijective，满足了set A中的单一输出箭头(函数，映射关系)都有一个在set B中对应的元素(one to one)，同时set B中的元素都接受到set A的元素中出发的箭头(onto),这两个路径的分别满足，让set A和set B中的元素得到了全部的匹配和各自唯一的匹配。   Function关系的演变      https://www.youtube.com/watch?v=ojHAAqgW1_M&amp;ab_channel=TheMathSorcerer   ","categories": [],
        "tags": [],
        "url": "/Injective,-Surjective-and-Bi-jective/",
        "teaser": null
      },{
        "title": "Discrete Mathematics",
        "excerpt":"虎头蛇尾…   离散数学 CMU 2001 notes   Lec1   intro   …   Set           首先定义空集的概念  $\\empty$ Definition: the only set include no elements.            交并补的概念    补充       The Sum Principle补充   Proof of Ramsey’s Theorem   你想证明的这个等式可以通过组合数的性质和帕斯卡三角形来解释和证明。以下是证明的一种方式：   首先，我们知道组合数 $\\binom{n}{m}$ 表示从 $n$ 个元素中选择 $m$ 个元素的组合数。这可以表示为：   \\[\\binom{n}{m} = \\frac{n!}{m!(n-m)!}\\]  同样，$\\binom{n}{m+1}$ 表示从 $n$ 个元素中选择 $m+1$ 个元素的组合数：   \\[\\binom{n}{m+1} = \\frac{n!}{(m+1)!(n-m-1)!}\\]  然后，我们来看左边的等式：   \\[\\binom{n}{m} + \\binom{n}{m+1}\\]  将上述两个组合数的表达式代入，得到：   \\[\\frac{n!}{m!(n-m)!} + \\frac{n!}{(m+1)!(n-m-1)!}\\]  现在，我们的目标是将这个式子变形成 $\\binom{n+1}{m+1}$ 的形式。我们注意到，$\\binom{n+1}{m+1}$ 也是从 $n+1$ 个元素中选择 $m+1$ 个元素的组合数：   \\[\\binom{n+1}{m+1} = \\frac{(n+1)!}{(m+1)!(n-m)!}\\]  我们希望将左边的式子变形成 $\\binom{n+1}{m+1}$ 的形式。为此，我们可以考虑如何将两个分数相加，使得分母部分与 $\\binom{n+1}{m+1}$ 相同。我们观察到：   \\[\\frac{n!}{m!(n-m)!} + \\frac{n!}{(m+1)!(n-m-1)!} = \\frac{(n+1)!}{(m+1)!(n-m)!}\\]  上述等式成立是因为分子部分相等。这就完成了证明。   因此，我们证明了：   \\[\\binom{n}{m} + \\binom{n}{m+1} = \\binom{n+1}{m+1}\\]  这个等式在组合数学中被称为 Pascal’s Rule（帕斯卡规则），它在帕斯卡三角形中有重要的几何解释。   下面笔记使用北理莫斯科的lec notes   Fri. 18th   3.2 集合的概念   集合    定义 我们指元素的组合。   数学描述latex打不出空格{}  \\(A={a:a满足specific的性质}\\) 两个集合的差集    定义   3.2.2 关系   二元关系   什么是二元关系，没有被定义  我觉得是两个未知数的关系就是2元关系  问:满足函数关系的本事是不是要满足映射关系，映射关系set A 到set  B，从一个A的element出发，只对应一个set B中的b，但是没有说是满射(满射)的英文是什么onto？  onto 是set B 中的b elements at least 对应一个 set A中的a   8 20th 计数   例 3.16. 定义 $[n]^{(r)}={T \\subseteq{1,2, \\ldots, n}:|T|=r}$. 设 $\\mathcal{A} \\subseteq[n]^{(r)}$. 设 $s&gt;r$ 。定义 \\(\\mathcal{B}=\\left\\{B \\in[n]^{(s)}: \\exists A \\in \\mathcal{A}, \\text { s.t. } A \\subseteq B\\right\\} .\\) $|\\mathcal{B}|$ 有多大呢（当然 $|\\mathcal{B}|$ 依赖于 $|\\mathcal{A}|)$ ? 解答. 上界：根据定义 $\\mathcal{B} \\subseteq[n]^{(s)}$ ，所以 $|\\mathcal{B}| \\leq\\left|[n]^{(s)}\\right|=\\left(\\begin{array}{l}n \\ s\\end{array}\\right)$. 下界：考虑如下集合 \\(M=\\{(A, B) \\in \\mathcal{A} \\times \\mathcal{B}: A \\subseteq B\\} .\\) 分别采用 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的观点来计算 $|M|$ 。                                     从 $\\mathcal{A}$ 的观点：对每个 $A \\in \\mathcal{A}, A$ 包含在 $\\left(\\begin{array}{l}n-r \\ s-r\\end{array}\\right)$ 个不同的 $B$ 中。因此, $           M           =           \\mathcal{A}           \\left(\\begin{array}{l}n-r \\ s-r\\end{array}\\right)$.                                                           从 $\\mathcal{B}$ 的观点：对每个 $B \\in \\mathcal{B}, B$ 最多包含 $\\left(\\begin{array}{l}s \\ r\\end{array}\\right)$ 个不同的 $A$ 。因此, $           M           \\leq           \\mathcal{B}           \\left(\\begin{array}{l}s \\ r\\end{array}\\right)$.                           解释   $[n]^{(r)}=\\{T \\subseteq\\{1,2, \\ldots, n\\}:|T|=r\\}$  对这个定义的明晰                  $[ ]$ and $()$ 都是对set的描述，其中集合中元素的个数的表示方法是$       A       $ ，比如$       \\mathcal{P}(A)       =2^{       A       }$           ${T \\subseteq{1,2, \\ldots, n}}$   则表示$T$ 被这个set包含，T中可能有1-n之间的元素，                  $       T       =r$           说明T的size就是r   $[n]^{(r)}$   说明有r组n这个set要配对，T中的元素分别输出到n值   $[n]^{(r)}$ 表示从集合 ${1, 2, \\ldots, n}$ 中选取$r$个元素的所有可能集合。而不是 “有r组n这个set要配对，T中的元素分别输出到n值”。   $[n]^{(r)}$ 表示的是从集合 ${1, 2, \\ldots, n}$ 中选取$r$个元素的所有可能集合，而不是将元素与n进行配对。   康尔托对角线证法   主要的构造方法是在[0,1]中，使用小数表示一个自然数，或者说使用一个双射来连接关系的情况下，特使得$x_{ij}(j=i)$ 的位置设置一个不同数，使用这个方式，当一个数字的每个数上的位置，不可以被对角线上对应位置的数字表示的时候，就说明他   图 Tue.   定义4.1   欧拉通路：经过图中的每一条边一次且仅一次。   欧拉回路：经过图中的每一条边一次且仅一次，且回到出发点。   定义4.2   设 v 是图中的某个顶点，与 v 连接的边的数目叫做 v 的度数，记作 d(v)。   ###命题 4.3   对任意的图 G，如果图 G 中存在一条欧拉通路，则最多只有两个点的度数是奇数。   命题 4.4   对任意的图 G，如果图 G 中存在一条欧拉回路，则每个点的度数都是偶数   命题4.4和4.3 只可以判断在本身存在时，这里的检测方法就是说，   定义 4.5   一个（无向，即边没有方向）图 $G$  由点和边组成，点的集合通常用$V$   表示，边的集合用 $E$  表示。图 $G$ 一般写作 \\(G = (V, E)\\) 点的相邻 两个点有边   边的相邻 两个边之间有点   点的度数 点发散或者接收的条数   简单图 两点之间最多一条边（不会不复杂，但是会没有来回   **通路** 从一点到另一点由边连接的序列（点和边均允许重复）  **路径** **点**不重复的通路  **回路** 起点和终点相同的通路 (到底是一个成环的路还是？首尾链接的路可以)   连通图 任意两点之间均有通路连接   子图 由图的某些点和这些点上某些边组成的图   导出子图：由图的某些点和这些点上所有的边组成的图/感觉没什么用，就是选了一些线和点呗   连通分支：   定理4.7   对任意的连通图 G，      图 G 中存在一条欧拉通路，当且仅当要么每个点的度数都是偶数，要么恰有两点的度数是奇数。   图 G 中存在一条欧拉回路，当且仅当每个点的度数都是偶数。   充分必要？   定理4.8 &amp;4.9握手定理和其的推论   有点问题   使用对称理论证明   树与环 Aug. 25th   定义   Aug. 26th 图的同构   定义   对有些问题而言，我们只关心边的相互关系，并不对顶点做过多区分，数学上我们考虑一个新的概念，叫做同构。   满足双射，在两个图$G=(V,E)$ 和$H=(V’,E’)$中，   三个算法   1. 贪心算法 最小生成树 Kruskal 算法   具向的理解是首先把最短到最长的权重的line升序排列取出，再使用这些line上的权重值进行连接，在连接时观察是否连结成环，如果成环，则取消   2. 信息熵算法   3. Dijkstra 算法   ","categories": [],
        "tags": [],
        "url": "/CMU-Discrete-mathematics-2001-notes/",
        "teaser": null
      },{
        "title": "learning how to learn powerful mental tools to help you master tough subjects",
        "excerpt":"UCLA online  learning how to learn powerful mental tools to help you master tough subjects   Focued mode   Diffuse mode   Focued mode   什么是focusing.使用比喻来解释，a pinball game works by, you pull back on the plunger, release it, and a ball goes boinking out, bouncing around on the rubber bumpers, and that’s how you get points.   So, here’s your brain, with the ears right here, and the eyes looking upwards. And we can lay that pinball machine right down inside it. So, there you go. There’s the analogy for the focused mode.   So look at how that thought moves smoothly around on the fuzzy underlying orange neural pathway. In some sense it’s as if it’s traveling along a familiar, nicely paved road. But what if the problem you’re working on needs new ideas or approaches?   Diffuse mode   To get to this new thought pattern, you need a different way of thinking. And that’s represented here, by the diffuse mode. Look at how widely spaced the rubber bumpers are. Thought takes off, look at how it moves widely, bounces around. It could travel a long way before being interrupted by hitting a bumper. In this diffuse mode of thinking, you can look at things broadly from a very different, big-picture perspective. You can make new neural connections traveling along new pathways. You can’t focus in as tightly as you often need to, to finalize any kind of problem solving. Or understand the finest aspects of a concept. But you can at least get to the initial place you need to be in to home in on a solution.   This course is meant to help you reframe how you think about learning, to help reduce your frustration.   Video 2   To gain muscular structure, you need to do a little work every day, gradually allowing your muscles to grow. Similarly, to build neural structure, you need to do a little work every day, gradually allowing yourself to grow a neuro-scaffold to hang your thinking on a little bit, every day, and that’s the trick.   We learned about how the brain’s two different thinking modes focused and diffuse, each helps us learn but in very different ways. Finally, we learned that learning something difficult can take time. Your brain needs to alternate it’s ways of learning as it grapples with and assimilates the new material. Thanks for learning about learning. I’m Barbara Oakley.   video 3 what is learn   brain   video 4 Procrastnation   Pomodoro   25min sepqtetary working time to focusing on some thing   v 5 Procrastnation and learning step      数学和科学的抽象性：文章指出数学和科学的抽象性是这些学科有时更具挑战性的原因之一。相比之下，我们可以通过实际事物来理解和感知其他抽象概念，而数学和科学的概念往往没有直接的对应物。   实践对学习的重要性：文章强调了在学习数学和科学时实践的重要性。通过实践，可以加强神经连接，使抽象的概念更加具体和实际。文章还提到了大脑通过实践形成的神经模式的比喻。   学习方法：文章介绍了一种学习方法，即集中学习一段时间后休息或转换注意力，以便大脑在后台进行深度思考和巩固概念的理解。文章还提到了使用番茄工作法（Pomodoro Technique）来克服拖延症，并建议进行有针对性的短期集中学习。   Chunking（分块）：文章提到下一步将讨论”chunking”，即如何掌握和理解关键概念的重要要素。然而，这一点在给出的文本中并没有具体展开。   v6 记忆概论      Memory Systems: The two major memory systems are working memory and long-term memory.   Working Memory: It is the part of memory involved in immediate and conscious processing of information. It is centered in the prefrontal cortex and has limited capacity (about four chunks of information).   Blackboard Analogy: Working memory is compared to an inefficient mental blackboard that requires repetition to retain information.   Metabolic Vampires: Refers to natural dissipating processes that can cause memories to fade if not repeated or reinforced.   Long-Term Memory: It is like a storage warehouse distributed across different regions of the brain. Different types of long-term memories are stored in specific areas.   Revisiting Information: To effectively store information in long-term memory, it is necessary to revisit it multiple times to increase the chances of recall later.   Immense Capacity: Long-term memory has the capacity to store billions of items, but information can become buried and difficult to retrieve without proper practice.   Importance of Long-Term Memory: Long-term memory stores fundamental concepts and techniques relevant to learning and understanding new information.   Working Memory’s Role: Working memory is used to process and handle new information but transferring it to long-term memory requires time and practice.   Spaced Repetition: A technique that involves repeating information over time with spaced intervals. It enhances the transfer of information from working memory to long-term memory.   Effectiveness of Spaced Repetition: Research shows that spaced repetition over multiple days is more effective for memory retention compared to mass repetition in a short period.   Analogy of Building a Wall: Just as a brick wall needs time for the mortar to dry and strengthen the structure, synaptic connections in the brain require time to form and strengthen for better memory retention.   Reference to the Acropolis: The Acropolis is mentioned as an example of a lasting structure, metaphorically indicating the importance of establishing strong and enduring memories.   v7 睡眠      Awake state and brain toxins毒素: Being awake generates toxic products in the brain.   Brain cleansing during sleep: When you sleep, brain cells shrink, increasing the space between them. This allows fluid to flow and wash away toxins.   Importance of sleep: Sleep helps keep the brain clean and healthy, contrary to it seeming like a waste of time.   Effect of sleep deprivation: Insufficient sleep can result in a buildup of metabolic toxins, impairing clear thinking. It can also be associated with various negative conditions and health risks.   Sleep and memory consolidation: Sleep plays a crucial role in the memory and learning process. It helps organize and strengthen important memories while erasing less important ones.   Neural rehearsal during sleep: The brain rehearses challenging aspects of learning during sleep, reinforcing neural patterns.   Sleep enhances problem-solving: Sleep improves problem-solving abilities and understanding of learned material. Deactivation of the conscious mind in the prefrontal cortex facilitates communication between brain areas.   Dreaming and learning: Going over learned material before sleep increases the chances of dreaming about it. Dreaming about the material can enhance understanding and consolidate memories.   Focused mode and diffuse mode: Engaging in focused mode work before sleep helps activate the diffuse mode, aiding in learning and problem-solving.   Week 2 内容 9th Sept.   lec1&amp;lec2&amp;lec3   What is a chunk?   Chunking is the mental leap that helps you unite bits of information together through meaning.  The new logical whole makes the chunk easier to remember, and also makes it easier to fit the chunk into the larger picture of what you’re learning.   这种概念的运作方式类似于将计算机文件压缩成ZIP文件，将许多信息整合成一个更容易处理和记忆的单元。   chunking是一种认知策略，有助于将信息整合成更大的概念块，使学习和记忆更加高效。   在学习新的信息或技能时，形成一个”chunk”（块）的过程可以被看作是将信息逐渐整合在一起的过程，以便更有效地理解和应用。   数学和科学学习：当你学习新的数学和科学材料时，通常会提供带有详细解答的示例问题。这是因为在你尝试理解如何解决问题时，你的认知负担很重。因此，从一个解决方案的示例开始有助于你理解问题的关键特征和基本原理。但要注意，不要只关注为什么执行一个步骤，而忽略了各个步骤之间的联系，即为什么下一个步骤是你应该执行的下一步。这就像使用地图来帮助你前往新地方旅行一样，要注意使用地图时周围发生的事情，很快你就能自己到达目的地，甚至能够找到新的到达目的地的方式。   注：主要是联系   如何制作一个”chunk”（块）   “chunking”的基本原理可以应用于各种领域，无论是学习智力思维还是身体运动。   第一步：集中精力关注你想要制作”chunk”的信息。这意味着在学习过程中要避免分散注意力，不要同时看电视、查看手机或电脑消息等，因为这会妨碍大脑专注于”chunking”新材料。当你开始学习新知识时，你正在建立新的神经模式，并将它们与大脑各个区域的”preexisting patterns”连接起来。如果你的思维被其他想法占据，会影响到你的学习过程。   I’m going to lean a little more towards explaining chunking of mental ideas rather than physical body motions, but you’ll see that the two approaches are closely related.   The first step on chunking is simply to focus your undivided attention on the information you want to chunk.   When you first begin to learn something, you’re making new neural patterns and connecting them with preexisting patterns that are spread through many areas of the brain. Your octopus tentacles 章鱼触须, so to speak, can’t reach very well if some of them are off on other thoughts using up some of the limited slots in your working memory.   The second step in chunking is to understand the basic idea you’re trying to chunk, whether its understanding a concepts such as continental drift, seeing the connection between the basic elements of a plot for a story, grasping the economic principle of supply and demand, or comprehending the essence of a particular type of math problem. Students can often synthesize the gist主旨, that is figure out the main idea or ideas pretty naturally, or at least they can grasp those ideas if they allow the focused and diffused modes of thinking to take turns in helping them figure out what’s going on. Understanding is like a super glue that helps hold the underlying memory traces together. It creates broad encompassing traces that can link to other memory traces. Can you create a chunk if you don’t understand? Yes, but it’s often a useless chunk that won’t fit in with or relate to other material you’re learning. That said, it’s important to realize that just understanding how a problem was solved for example, does not necessarily create a chunk that you can easily call to mind later.   of a breakthrough and understanding with solid expertise. That’s part of what you can grasp an idea when a teacher presented in class, but if you don’t review it fairly soon after you first learned it, it can seem incomprehensible when it comes time to prepare for a test. In math and science related subjects, closing the book and testing yourself on whether you yourself can solve the problem you think you understand will speed up your learning at this stage.   You often realize the first time you actually understand something is when you can actually do it yourself. It’s the same in many disciplines. Just looking at someone else’s painting doesn’t mean you could actually create that painting yourself, and just hearing a song won’t give you the expertise you need to sing it in the same resonant fashion. Just because you see it or even that you understand it, it doesn’t mean that you can actually do it. Only doing it yourself helps create the neural patterns that underlie true mastery.   第三步：获得上下文，以便不仅知道如何使用这个”chunk”，还知道何时使用它。上下文意味着要超越初始问题，更广泛地看待问题，并进行重复和实践，不仅与相关问题一起，还与无关问题一起，以便不仅知道如何使用”chunk”，还知道何时不使用它。这有助于你看到你新创建的”chunk”如何适应更大的图景。上下文是底层和顶层学习相遇的地方。它意味着你不仅要学会使用某种问题解决技巧（底层学习），还要学会何时使用该技巧而不是其他技巧（顶层学习）。   The third step to chunking is gaining context, so you can see not just how but also when to use this chunk. Context means going beyond the initial problem and seeing more broadly, repeating and practicing with both related and unrelated problems, so that you can see not only when to use the chunk but when not to use it. This helps you see how your newly formed chunk fits into the bigger picture. In other words, you may have a tool in your strategy or problem solving tool box but if you don’t know when to use that tool it’s not going to do you a lot of good. Ultimately, practice helps you broaden the networks of neurons that are connected to your chunk, ensuring it’s not only firm but also accessible from many different paths. As you can see from this top-down, bottom-up illustration, learning takes place in two ways. There’s a bottom-up chunking process where practice and repetition can help you both build and strengthen each chunk, so you can easily access it whenever you need to. There’s also a top-down big picture process that allows you to see what you’re learning and where it fits in. Both processes are vital in gaining mastery over the material. Context is where bottom-up and top-down learning meet. To clarify here, chunking may involve your learning how to use a certain problem solving technique. Context means learning when to use that technique instead of some other technique. Doing a rapid two-minute picture walk through a chapter in a book before you begin studying it, glancing at pictures and section headings can allow you to gain a sense of the big picture, so can listening to a very well-organized lecture. These kinds of activities can help you know where to put the chunks you’re constructing, how the chunks relate to one another; just as you see here with the image of the man in the car. Learn the major concepts or points first, these are often the key parts of a good instructor or on book chapters, outline, flow charts, tables, or concept maps. Once you have this done, fill in the details. Even if a few of the puzzle pieces are missing at the end of your studies, you can still see the big picture. So, there you go.   Summing it up, chunks are best built with focused attention, understanding of the basic idea, and practice to help you gain mastery and a sense of the big picture context. Those are the essential steps in making a chunk and fitting that chunk into a greater conceptual overview of what you’re learning, but there’s more. Thanks for learning how to learn.   大纲，，理解basic info，理解和划重点学习但不是只划重点，填充内容，构建学习flow   能力错觉   The importance of recall, illusions of competence in learning.   very simple, technique. Recall. After you’ve read the material, simply look away, and see what you can recall from the material you’ve just read.   Including simply rereading the text a number of times. Or drawing concept maps that supposedly enrich the relationships in the materials under study.   When we retrieve knowledge, we’re not just being mindless robots. The retrieval process itself enhances deep learning, and helps us to begin forming chunks. It’s almost as if the recall process helps build in little neural hooks, that we can hang our thinking on.   开始分块   thought, concept mapping, drawing diagrams that show the relationship between the concepts would be the best.   Using recall, 在头脑中检索关键观点，而不是passive地重读   因为看书比回想更容易，所以学生们在学习的过程中坚持他们的 这样的学习方式效果并不好。这就提醒我们，只要想 并不能保证你能真正学会。有一个超级有用的方法，可以确保你在学习，而不是用能力的幻想来欺骗自己，那就是测试自己在学习什么。   回忆，而不是边看书边复习，但这里还有一个小窍门，在你不在通常的学习地点时回忆材料 也能帮助你加强对材料的掌握。你没有意识到，但当你学习新知识时，你常常会你在学习新知识时，往往会在潜意识中接受到当时学习材料时周围房间和空间的提示。   This can throw you off when you take tests because you often take tests in a room that’s different from the room you were learning in. By recalling and thinking about the material when you are in various physical environment, you become independent of thecues from any one given location. That helps you avoid the problem of the test room being different from where you originally learned the material.   week2 是什么激励了你？   Certainly, here is a summarized list of key points from the provided text:           Interest Facilitates Learning: Learning is easier when you are genuinely interested in the subject matter.            Neurons and Information Processing: The neurons in your cortex process information about your surroundings and actions.            Neuromodulators and Importance: Neuromodulators carry information about the importance and value of experiences rather than their content.            Three Important Neuromodulators: Acetylcholine, dopamine, and serotonin are discussed as important neuromodulators.            Acetylcholine and Learning: Acetylcholine is crucial for focused learning when paying close attention. It activates circuits related to long-term memory.            Dopamine and Motivation: Dopamine is a key chemical influencing motivation and reward learning. It plays a significant role in decision-making.            Dopamine and Addiction: Addictive drugs increase dopamine activity, leading to craving and dependence.            Loss of Dopamine Neurons: Loss of dopamine neurons can result in a lack of motivation, anhedonia, and even Parkinson’s disease.            Serotonin and Social Behavior: Serotonin affects social behavior, with higher levels seen in dominant individuals and lower levels in risk-takers and inmates with violent behavior.            Emotions and Learning: Emotions are intertwined with perception, attention, learning, and memory. The amygdala is a key brain structure for integrating emotions and cognition.            Website for More Information: Brainfacts.org is recommended as a resource for further information on acetylcholine, dopamine, and serotonin.            Conclusion: The message concludes by emphasizing the importance of keeping the amygdala “happy” for effective learning.       week 2 组块库的价值   文章主要讨论了“分块（chunking）”在学习和创新思维中的重要性。           分块的概念：分块是一种压缩信息的方法，使得我们能够更有效地存储和检索信息。            创新与分块：分块能够帮助人们以新颖和创造性的方式组合信息。例如，Bill Gates等行业领袖会专门安排长时间的阅读期，以便他们能一次性掌握多种多样的观点。            跨学科应用：分块不仅在特定领域有用，而且可以跨学科地应用。这种跨学科的应用被称为“转移（transfer）”。            专家与分块：例如，国际象棋大师能快速访问数千种不同的棋局模式；音乐家、语言学家和科学家也有自己的知识分块。            分块的训练：随着经验的增长，你所创建的分块会变得更大，更稳固。这些分块形成了你的“神经模式库”。            专注模式与发散模式：专注模式用于顺序、逐步的思考，而发散模式则更多地依赖于直觉和跨领域的联接。            不断练习的重要性：如果你不练习你所学的分块，它们可能会逐渐模糊，从而影响你对大局的理解。            直觉的局限性：虽然直觉非常有用，但它生成的解决方案需要通过专注模式来仔细验证。            学习的积累效应：一旦你掌握了第一个概念并将其加入你的“心智库”，那么第二个、第三个概念就会更容易地被加入。       这个文本主要是关于”chunking”（数据分块）以及”diffuse and focused modes of thinking”（散漫和集中的思考模式）在学习和问题解决中的作用。文本解释了如何通过分块技术将知识和概念组织成易于理解和应用的”块”，以提高问题解决和创新思考的能力。           数据分块（Chunking）: 数据分块是一种通过将复杂信息分解成易于理解和记忆的小块来增强记忆和学习的技术。这不仅有助于信息存储，还有助于在不同的情境和学科之间应用这些信息，这被称为“转移”（Transfer）。                       创新与分块: 分块能帮助人们在思维中持有多个和不同的概念，这有助于在创新思考中形成新的和原始的组合。                        跨领域应用（Transfer）: 一个分块的概念不仅在一个领域内有用，而且可以应用到其他完全不同的领域。                        散漫和集中思考模式:                       集中模式（Focused Mode）: 用于逐步、有逻辑地解决问题或学习新概念。                        散漫模式（Diffuse Mode）: 用于在看似不相关的分块概念之间建立联系，从而解决新颖的问题。                        整体直觉（Holistic Intuition）: 散漫模式通常用于形成直觉或洞见，这些直觉需要通过集中模式进行验证。                        实践与大局观:                      如果不经常使用和练习你的“分块”，它们可能会逐渐淡出记忆，导致难以组织和应用这些信息。                        贯穿全文的是一个积极的态度：坚持和努力会让学习变得更加容易。                   总体而言，分块和思考模式是高效学习和解决问题的两个关键组成部分。不仅如此，这两者相互作用，使个体能够更灵活、更具创造性地应对各种挑战。   The ability to combine chunks in new and original ways underlies a lot of historical innovation.   week-long reading periods so that they can hold many and varied ideas in mind during one time. This helps generate their own innovative thinking by allowing fresh in mind, **not yet forgotten ideas to network amongst themselves.** Basically, what people do to enhance their knowledge and gain expertise, is to gradually build the number of chunks in their mind, valuable bits of information they can piece together in new and creative ways.   The bigger and more well-practiced your chunked mental library, whatever the subject you're learning, the more easily you'll be able to solve problems and figure out solutions. As we'll discover soon, chunking isn't all you'll need to develop creative flexibility in your learning, but it's an important component.   Chunks can also help you understand new concepts. This is because when you grasp one chunk, you'll find that that chunk can be related in surprising ways to similar chunks, not only in that field but also in very different fields. This idea is called **transfer**. For example, concepts and problem solving method you learned for physics, can be very similar to chunked concepts in business. I found some aspects of language learning were very helpful for me when I later began to learn computer programming. A chunk is a way of compressing information much more compactly. As you gain more experience in chunking in any particular subject, you'll see that the chunks you're able to create are bigger, in some sense that the *ribbons* are longer. Not only are those ribbons longer, but the neural patterns are in some sense darker. They're more solid and firmly ingrained. If you have a library of concepts and solutions internalized as chunked patterns, you can think of it as a collection or a library of neural patterns.  Your diffuse mode can help you connect two or more chunks together in new ways to solve novel problems.   Another way to think of it is this, as you build each chunk it is filling in a part of your larger knowledge picture, but if you don't practice with your growing chunks, they can remain faint and it's harder to put together the big picture of what you're trying to learn.   In building a chunked library, you're training your brain to recognize not only a specific concept, but different types and classes of concepts so that you can automatically know how to solve quickly or handle whatever you encounter. You'll start to see patterns and simplify problem-solving for you and will soon find that different solution techniques are lurking 潜伏 at the edge of your memory. Before midterms or finals, it can be easy to brush up and have these solutions at the mental ready.   There are two ways to figure something out or to solve problems.   First, **through sequential step-by-step reasoning** and second, **through a more holistic intuition**整体直觉.   **Sequential thinking** where each small step leads deliberately故意的 towards a solution, involves the **f**ocused mode.   **Intuition** on the other hand, often seems to require this creative diffuse mode linking of several seemingly different focused mode thoughts.   **Mos**t difficult problems and concepts are **grasped through intuition,** because these new ideas make a leap away from what you're familiar with. Keep in mind that the diffuse modes, semi-random way of making connections means that the solutions it provides should be very carefully verified using the focused mode. Intuitive insights aren't always correct.    你可能认为题太多。没办法全学完， 这就是 law of Serendipity 将要应用的地方. Just focus on whatever section you're studying只要专注于你在学习的地方. You'll find that once you put that first problem or concept in your mental library, whatever it is, then the second concept will go in a little more easily and the third more easily still你会发现你一旦把一个概念放进思维库，无论什么概念，那么第二个相关的概念就会非常容易，第三个也是. Not that all of this is a snap, but it does get easier. 不是非常快的，但是这确实是变得容易。   Einstellung   When you’re learning a new idea, for example a new vocabulary word or a new concept or a new problem solving approach, you sometimes tend to practice it over and over again during the same study session.   当你在学的时候，你会在一个session中重复学习的倾向。   A little of this is useful and necessary, but continuing to study or practice after you’ve mastered what you can in the session is called overlearning. 但是这在大多数时候是没有用的。Overleaning can have its place.过剩也有它的道理。 It can produce an automaticity that can be important when you’re executing a serve in tennis or a perfect piano concerto.不知道 If you choke on tests or public speaking, overlearning can be especially valuable. 准备考试或者公共演讲有用。Did you know that even expert public speakers practice on the order of 70 hours for a typical 20-minute TED Talk? Automaticity can indeed be helpful in times of nervousness, but be wary of repetitive overlearning during a single session.要警惕重复练习。 Research has shown it can be a waste of valuable learning time.研究证明浪费时间。 The reality is, once you’ve got the basic idea down during a session, continuing to hammer away at it during the same session doesn’t strengthen the kinds of long term memory connections you want to have strengthened.一旦你有了basic的想法或者联系后，你应该远离它， Worse yet, focusing on one technique is a little like learning carpentry by only practicing with a hammer. 更糟糕的是，只专注于一种技巧 就像只用锤子练习木工一样。After awhile you think you can fix anything by just bashing at it.一段时间后，你会觉得 任何事情都可以靠敲打来解决。 Using a subsequent study session to repeat what you’re trying to learn is just fine and often valuable.少吃多餐学习 It can strengthen and deepen your chunked neuron patterns.可以帮助你 chunk “神经元模式”。   重复学习也是好的，下面。。It can also bring the illusion of competence that you’ve mastered the full range of material, when you’ve actually only mastered the easy stuff. 重复学习会带来以为自己掌握的错觉，但是实际上你只是掌握了简单的内容。Instead, you want to balance your studies by deliberately focusing on what you find more difficult. This focusing on the more difficult material is called deliberate practice. 相反，你要平衡你的 通过有意识地把重点放在你认为更难的地方来平衡你的学习。这种专注于较难的地方称为刻意练习。这往往是 好学生和优秀学生之间的区别。   All this is also related to a concept known as Einstellung.”Einstellung “的概念 In this phenomenon, your initial simple thought, an idea you already have in mind or a neural pattern you’ve already developed and strengthened, may prevent a better idea or solution from being found. 最开始的在心中的想法或者一个neural pattern已经被发展和强化，可能会诞生更加好的解决办法。 We saw this in the focus pinball picture, where your initial pinball of thought went to the upper part of the brain, but the solution thought pattern was in the lower part.弹球思维 是在大脑的上半部分，而解决问题的思维模式 模式却在下部。 The crowded bumpers of the focus mode and the previous patterns you built can create a sort of rut that prevents you from springing to a new place where the solution might be found. Incidentally, the German word einstellung means mindset. Basically you can remember einstellung as installing a roadblock because of the way you were initially looking at something. This kind of wrong approach is especially easy to do in sports and science, not to mention other disciplines, because sometimes your initial intuition about what’s happening or what you need to be doing is misleading. 聚焦模式的拥挤碰撞和你以前建立的模式 会让你产生一种束缚感，使你无法跳到一个新的地方去寻找解 可能找到解决方案的地方。顺便提一下，德语中的 einstellung 的意思是心态。基本上，你可以把 einstellung 就是安装路障，因为你最初看待事物的方式是 安装路障。这种错误的方法 尤其是在体育和科学领域，更不用说其他学科了、 因为有时你对正在发生的事情或你需要做的事情的最初直觉 因为有时你对正在发生的事情或你需要做的事情的最初直觉会产生误导。You have to unlearn your erroneous older ideas or approaches even while you’re learning new ones. One significant mistake students sometimes make in learning is jumping into the water before they learn to swim. In other words, they blindly start working on homework without reading the text book, attending lectures, viewing online lessons, or even speaking with someone knowledgeable. This is a recipe for sinking. It’s like randomly allowing a thought to, kind of pop off in the focus mode pinball machine, without paying any real attention to where the solution truly lies. Understanding how to obtain real solutions is important in learning and in life. Mastering a new subject means learning not only the basic chunks, but also learning how to select and use different chunks. 你必须在错误的旧观念或方法，甚至在学习新观念或方法的同时学习新的想法或方法。学生在学习过程中 学生在学习中有时会犯的一个重大错误就是 在学会游泳之前就跳进水里。换句话说，他们盲目地开始做作业 不读课本，不听讲座，不看在线课程，甚至不与有经验的人交谈，就盲目地开始做作业、 查看在线课程，甚至不与知识渊博的人交谈，就盲目地开始做作业。这是沉没的秘诀。这就好比随意让一个念头 弹球机中弹出来，而没有真正注意到解决方法的真正所在。 解决方案的真正所在。了解如何获得真正的解决方案 在学习和生活中都很重要。掌握一门新学科意味着 掌握一门新学科，不仅意味着要学习基本的知识块，还要学习如何选择和使用不同的知识块。 使用不同的语块。The best way to learn that is by practicing jumping back and forth between problems or situations that require different techniques or strategies. This is called interleaving. Once you have the basic idea of the technique down during your study session, sort of like learning to ride a bike with training wheels, start interleaving your practice with problems of different types or different types of approaches, concepts, procedures. Sometimes this can be a little tough to do. A given section in a book, for example, is often devoted to a specific technique, so when you flip to that section you already know which technique you’re going to be using. Still, do what you can to mix up your learning. In science and math in particular it can help to look ahead at the more varied problem sets that are sometimes found at the end of chapters. Or you can deliberately try to make yourself occasionally pick out why some problems call for one technique as opposed to another.最好的学习方法是 练习在需要不同技巧或策略的问题或情况之间来回跳跃。 不同的技巧或策略。这就是所谓的交错练习。一旦你在学习过程中掌握了 在学习过程中掌握了技巧的基本概念后，就像学习骑自行车一样，开始交错练习不同类型的问题或不同类型的方法、 概念、程序。有时做起来有点困难。例如，一本书中的某个章节通常会专门讨论 因此，当你翻到那一节时，你已经知道你要用到哪种技术了。 技巧。不过，还是要尽力让你的学习更加丰富多彩。特别是在科学和数学方面 在章节末尾会有更多不同的问题集。 在章节末尾。或者，你可以有意识地偶尔让自己找出为什么有些问题需要一种技巧而不是另一种技巧。 为什么需要一种技巧而不是另一种技巧。 You want your brain to become used to the idea that just knowing how to use a particular concept, approach, or problem-solving technique isn’t enough.你要让大脑习惯于 只知道如何使用特定的概念、方法或 解决问题的技巧是不够的。 You also need to know when to use it. 你还需要知道何时使用它。Interleaving your studies, making it a point to review for a test, for example, by skipping around through problems in the different chapters and materials can sometimes seem to make your learning a little more difficult, but in reality, it helps you learn more deeply.穿插学习、 例如，为了复习备考而跳过不同章节和材料中的问题，有时似乎会让你的学习变得枯燥乏味。 不同章节和材料中的问题，有时似乎会让你的学习 但实际上，它有助于你更深入地学习。 Interleaving is extraordinarily important.交错是非常重要的。 Although practice and repetition is important in helping build solid neural patterns to draw on, it’s interleaving that starts building flexibility and creativity.但交错学习才是建立灵活性和创造性的开始。 It’s where you leave the world of practice and repetition, and begin thinking more independently. 这让你离开练习和重复的世界，开始更加独立地思考。When you interleave within one subject or one discipline, you begin to develop your creative power within that discipline. When you interleave between several subjects or disciplines, you can more easily make interesting new connections between chunks in the different fields, which can enhance your creativity even further. Of course it takes time to develop solid chunks of knowledge in different fields, so sometimes there’s a trade off.权衡利弊   Developing expertise in several fields means you can bring very new ideas from one field to the other, but it can also mean that your expertise in one field or the other isn’t quite as deep as that of the person who specializes in only one discipline. 多领域发展内容可以使你在一些领域中带来新的想法创意，但是这也意味着你的经验不会比深耕于某一个领域的人的多   你要让大脑习惯于 只知道如何使用特定的概念、方法或 解决问题的技巧是不够的。你还需要知道何时使用。穿插学习、 例如，通过跳过不同章节和材料中的问题来为考试复习。 不同章节和教材中的问题，有时似乎会增加学习难度。 但实际上、 它能帮助你更深入地学习。穿插学习异常重要。尽管练习和重复对于建立稳固的神经模式有帮助，但交错学习才是建立灵活性和创造性的开始。这让你离开练习和重复的世界，开始更加独立地思考。当你在一门学科或你就开始在这门学科中发展自己的创造力。当你在多个学科或时，你就能更容易地在多个科目或学科之间建立有趣的新联系。 在不同领域的大块内容之间建立有趣的新联系，从而进一步提高你的创造力。 进一步提高你的创造力当然，在不同领域积累扎实的知识需要时间，所以有时需要权衡利弊。   下面讲多学科融合，没什么东西   Philosopher of science Thomas Kuhn discovered that most paradigm shifts in science are brought about either young people or people who were originally trained in a different discipline. They’re not so easily trapped by einstellung, blocked thoughts due to their preceding training. And of course there’s the old saying that science progresses one funeral at a time as people entrenched in the old ways of looking at things die off. Finally, don’t make the mistake of thinking that learning only occurs in the kinds of subjects you acquire from teachers or books. When you teach a child how to deal effectively with a bully, or you fix a leaky faucet, or you quickly pack a small suitcase for a business trip to Hong Kong, all of these illustrate the outcomes of important aspects of learning. Physicist Richard Feynman was inspired in his Nobel Prize-winning work by watching someone throw a dinner plate into the air in a cafeteria. Mike Rowe of the television shows Dirty Jobs and Somebody’s Gotta Do It shows how important and exciting learning can be in a variety of different, non-academic disciplines.另一方面，如果你只在一门学科上发展 另一方面，如果你只发展一门学科的专业知识，你可能会对这门学科有很深的了解，但 你可能会在自己熟悉的思维方式中变得更加根深蒂固，无法应对新的想法。 无法应对新思想。科学哲学家 托马斯-库恩（Thomas Kuhn）发现，大多数科学范式的转变都是由以下两种人带来的 要么是年轻人，要么是原本接受过不同学科训练的人。 不同学科的人。他们不那么容易被先前的训练所束缚，思维受阻。 由于他们之前所受的训练，他们不那么容易受困于 当然还有一句老话，那就是 科学是在一次次葬礼中进步的，因为那些固守旧的 看待事物的方式的人都会死去。最后，不要误 认为学习只发生在你从老师或书本上学到的科目中。 从老师或书本中获得。当你教孩子如何 当你教孩子如何有效地对付恶霸时，当你修理漏水的水龙头时，当你快速地 当你教孩子如何有效应对恶霸时，当你修好漏水的水龙头时，当你为去香港出差快速收拾一个小行李箱时，所有这些都说明了学习的重要方面的成果。 这些都说明了学习的重要方面的成果。物理学家理查德-费曼 在他的诺贝尔奖获奖作品中看到有人在自助餐厅里把餐盘扔向空中，从而获得灵感。 在自助餐厅向空中投掷餐盘时受到启发。电视 的迈克-罗在电视节目《肮脏的工作》和《总得有人去做》中 展示了在各种不同的非学术学科中，学习是多么重要和令人兴奋。 在各种不同的非学术学科中，学习是多么重要和令人兴奋。   ##WEEK3 拖延症与记忆概论   拖延症   我将教你懒人解决拖延症的方法。这意味着你将了解自己内心的 “僵尸”，即你的大脑在特定暗示下产生的常规习惯性反应。这些 “僵尸反应 “通常都是为了让此时此地变得更好。   与此类似，拖延症患者拖延的只是一件小事。他们一而再、再而三地这样做，逐渐养成了习惯。他们甚至可以看起来很健康，但长期效果并不好。      你已经学习了一个非常实用的工具来帮助你解决拖延问题，即“番茄工作法”，也就是25分钟的不间断专注时间，后面跟着一点放松时间。   学习避免拖延之所以重要，是因为好的学习是一个逐步的活动。   意志力很难获得。它消耗了大量的神经资源。   拖延可以是一个单一的、具有巨大影响的坏习惯，换句话说，它影响了你生活中许多重要的方面。   无处不在的僵尸思维   第一次开倒车很难   但是无数次后，你的大脑会进入一种僵尸模式，只对几个关键因素半知半解，而不是被所有数据淹没。骑自行车也是同样的道理。一开始很难，后来就很容易了。   Neuroscientifically speaking, chunking is related to habit.   所以习惯是什么，an energy saver   它能让我们解放思想，从事其他类型的活动。 你进入这种习惯性僵尸模式的次数远比你想象的要多，这就是习惯的意义所在。 在养成习惯的过程中，你不必集中精力思考自己在做什么，这样可以节省精力。   你可以把习惯看作由四个部分组成，第一部分是线索。(cue)   这是让你进入 “僵尸模式 “的触发器，线索可能很简单，比如看到待办事项列表中的第一项。比如看到要回家的信息。提示本身既无益也无害。重要的是常规，是我们对提示做出的反应。   Number two, the routine. 这是你的 “僵尸模式”，是你的大脑在收到提示时习惯性做出的常规反应。 僵尸反应可能有用、无害，有时也有害。   Number three, the reward.每个习惯的养成和延续都是因为它能给我们回报。 它能给我们带来直接的快感。拖延是一个很容易养成的习惯，因为它的奖励，即把你的注意力转移到更令人愉快的事情上，发生得如此迅速和容易。   第四，信念。 belief。习惯之所以有力量，是因为你相信它们。 例如，你可能觉得自己永远无法改变把学习拖到很晚的习惯。要改变习惯，你需要改变你的基本信念。   冲浪练习 结果与过程   学新大学有负面衬衣 凡吸毒面呈细 奴卡失效 就是和陈i像你风格有新的方法   急症在结果 但是一个集中在错火车上呢个上 虚像做作业 反对就5 分到日版有此案骨气·那个作业和温暖男 不要在就看i问过上而实在估测婚个上个   专注的的番茄工作法，在工作片段上而不是在工作的结果   这样就将是过程关注在过程上而不是在一个痛苦的结果。   安静的学习   利用僵尸思维   应对反应   什么开始了zombo思维   the cue   localtion   time   how you feel   Reaction   什么是拖延，比如看了眼手机然后不回了   The routine   Plan就行，   The reward   无论什么奖励。甚至设置一个目标5:00不搞别的事情就休息   应对生活与学习   weekly plan   每天新起一页写 然后在睡前写好 潜意识会帮助你构造这个目标   番茄工作 切换一个导向过程也是换一个process   时间工作量 的准确判断   5pm之前完成，休息时间非常重要   努力在生活之余保持健康和空闲   summing up procrastination   日志 比对   目标比对完成度   奖励留在最后   观测拖延症触发的开关   使用一个新度学习系统，比如使用🍅   注意休息，有时候休息，比如在5:00pp之后   记忆 虽然我感觉没什么用   深入记忆探索   祖先不记得文字，但是知道打猎的路，进化过程的需要，使用坐标位置，使用视觉记忆而不是单一背诵，甚至是感觉。   重复和复习   使用图像辅助记忆概念。重复，test，mixed 再记   如果不能学习 HM（人名）什么是长期记忆   分时段学习更加好。      fade？消失   创造记忆宫殿   记忆技巧   记忆总结   connection   长记忆   store house  几天 长时间 提早开始   working memory   4 items   记忆技巧   生动的画，感受生动有趣，简化材料   数字和有意义的数字   可视化记忆 分类方法   熟悉场景可视化记忆   有意义的缩写和 chunk 背诵理解的材料   week4   怎么更好的learner   保持运动 促进神经元存活   复兴式学习   学习不是线性增长的概念 知识体系会坍塌 新知识吸收会花时间   生动视觉比喻   f=ma的另外一种解释   不需要羡慕天才   有点重要，看文字  有大的记忆力空间，因为思维是在原有的记忆力空间上促发诞生的，我们的思维思考方法是在原有的磨具上被发展的，换句话说就是，记忆力本身是智力的容量。   ### 改变思维   觉得自己牛逼？   week 4 下   团队的价值（脑）   研究主义 左脑感觉，在做完作业以后过度自信。   首先原则不要自我欺骗   什么是自己的盲区   脑力不够 与同学一起学习 察觉忽略 给同学讲解   学习小组而不是社交小组   测试清单 final   回答问题 我要如何准备考试   认真努力学内容吗，理解吗？   自己的观点   明白所有答案   study guide   相互提问   先难后易   在开始时候难，转向简单。focus mode 转向 diifuse mode   想一个并行操作的大厨   大脑不同部位都有功能   自制力 两分钟就转向 引发牵引力   压力 有益的最后提示   注意转向呼吸 深呼吸   保持好心情而不是焦虑   选择题，盖住选项 回想知识   stuble braided   summing up module 4   转变思维转变命运   检查问题，利用大脑功能，要休息，审视   太自信，与朋友一起学习   check   先难后易 利用两个模式解题   紧张 深呼吸   足够睡眠     ","categories": [],
        "tags": [],
        "url": "/learning-how-to-learn-powerful-mental-tools-to-help-you-master-tough-subjects/",
        "teaser": null
      },{
        "title": "Inner producted 一些问题",
        "excerpt":"A record of some simple ideas.   vector inner product   向量的内积      内积的定义:                                                            设A和B为两个向量,它们的内积定义为A在B上的投影长度,用公式表示为&lt;A,B&gt; =               A                               B               cosθ,其中θ是两向量之间的角度。                                                        内积的几何意义:内积反映了两个向量间的相似程度,当两个向量方向相同时内积最大;when两向量垂直时内积为0。            如何计算两个向量的内积:设两个n维向量为A=(a1,a2,…,an),B=(b1,b2,…,bn),则两向量的内积为&lt;A,B&gt; = a1b1 + a2b2 + … + anbn,即对应元素相乘并求和。       内积为何等于零:当两个向量垂直时,cosθ=0,从内积公式可知此时&lt;A,B&gt;=0。也就是说,内积等于零表示两个向量互相垂直。   matrix inner product      矩阵内积的定义:对于两个矩阵A和B,它们的内积定义为:&lt;A,B&gt; = tr(A^TB),即矩阵A与B转置矩阵的乘积的迹,什么是迹，就是从左到右的对角线。   矩阵内积与迹的关系:矩阵内积用矩阵迹来表达,迹反映了矩阵的特征,内积刻画了两个矩阵间的相关性。   如何计算两个矩阵的内积:对两个矩阵A和B,计算A^TB,然后求出其迹(对角元素之和)即可。   矩阵内积的性质:            对称性 &lt;A,B&gt; = &lt;B,A&gt;       双线性形式                                                       范数不等式$               &lt;A,B&gt;               &lt;=                               A                               ·                               B                               $ 这啥                                                   matrix的变换。   f(x) and g(x) inner product           函数内积的定义:对于区间[a,b]上的连续函数f(x)和g(x),它们的内积定义为\\(\\langle f, g\\rangle=\\int_a^b f(x) g(x) d x\\)            如何利用积分计算函数内积:将函数相乘后对区间[a,b]积分,即可计算内积。            函数内积的双线性形式:满足α&lt;f,g&gt; = &lt;αf,g&gt; = &lt;f,αg&gt;            函数内积与区间长度的关系:内积与区间长度成正比,因此也会normalize由 $\\int ab$  改为 $\\int \\frac{ab}{L}$。            函数内积的应用:反映函数间相关性,应用于信号处理、模式识别等领域。       \\[\\langle f, g\\rangle=\\int_a^b f(x) g(x) d x\\]  or another choice   \\[\\langle f, g\\rangle=\\frac{1}{b-a} \\int_a^b f(x) g(x) d x\\]  第一种表示法:   \\[\\langle f, g\\rangle=\\int_a^b f(x) g(x) dx\\]  直接对函数的乘积在区间[a,b]上进行积分即可得到内积。   第二种表示法:   \\[\\langle f, g\\rangle=\\frac{1}{b-a}\\int_a^b f(x) g(x) dx\\]  在积分中额外引入了比例系数$\\frac{1}{b-a}$,目的是归一化区间[a,b]的长度,使得内积不直接依赖于区间长度。   应用：傅立叶级数  ","categories": [],
        "tags": [],
        "url": "/inner-producted/",
        "teaser": null
      },{
        "title": "Notes for ‘How To Read MTF Curves’ article",
        "excerpt":"文章翻译, 阅读, 解读MTF曲线, 笔记总结  Preface   The following context and material are sourced from Dr. H.H. Nasse’s articles.  Many many thx for sharing and explain MTF in the preview articles.  However, I still had no understanding at all when I read those articles during my first year of bachelor studies. I will attempt to write this article in my own words without using any grammar or AI tools.  I have to say in some places, I will use some “direct copy” sentences. I don’t consider this as a form of academic plagiarism, as I am writing this post for learning in photography as an enthusiast.   I listed the all used article below. Again, I will use some images and words directly without any citations. If you have any concerns about this post, PLZ contact me at (notmyemailcode@gmail.com) for specific sections throughout the entire article. I will do it for you ASAP.   [1]Intro. (ZEISS page)   https://lenspire.zeiss.com/photo/en/article/measuring-lenses-objectively-why-do-we-need-mtf-curves-by-dr-hubert-nasse-part-1   [2]How to Read MTF Curves(December 2008)   http://lenspire.zeiss.com/photo/app/uploads/2018/04/Article-MTF-2008-EN.pdf   [3]How to read MTF curves? Part II(March 2009)   https://lenspire.zeiss.com/photo/app/uploads/2018/04/CLN_MTF_Kurven_2_en.pdf   [4] 如何閱讀MTF(一), Weifu Lin 林渭富 如何閱讀mtf-f85a60cf59c   (以上引用都没有被作者授权, 如有侵权, 本人会尽快删除, 文章承认前置的引用的文章的所有贡献.  本文章没有传播价值和商业价值, 是我看了引用的文章的笔记, 理解上的总结与整理 )   The main purpose for this article is helping me in realizing how designers made an excellent lens in the past and to understand which are favorable features for a good lens. And Iet me select suitable apertures not only by weather and sun   I  wanna finish this work in 2 days (2024.06.17-2024.06.18).  太难了, 最后看了一个礼拜才看了一半不到.   The final date for this article is 7.6   淦, 还是用中文写吧.   START here   photographers want to take a very natural-looking picture of a subject, the lens should be sharp, which means that the lens will reflect the correct image of the view. As you know, the light line can gather in a light point on the frame plane. The perfect lens needs to show this “point” in the image correctly, but the truth is that the lens cannot do that on every surface. In the article, the Dr. H.H. Nasse give the example pic below, I attached this image below.      原始圖片來源 &amp; Credit：Hubert H. Nasse, How to Read MTF Curves, page 6, 《LENSPIRE》   this pic indicates 8 satuations in a size comparison, which all input a small white square light, but totally output different light results in cmos(tip: pic7 should be the perfect example)   调制传递（Modulation Transfer）   Sinusoidal brightness distribution   下面是文章中第一次出现的需要解释的图      原始圖片來源 &amp; Credit：Hubert H. Nasse, How to Read MTF Curves, page 6, 《LENSPIRE》 。   在原文中, 作者提到 Since we are primarily interested in how extended objects are imaged, objects which, unlike stars, comprise an infinite number of points, we must find another way to quantitatively describe the image quality. 于是我们使用sinusoidal brightness distribution来量化. 正弦波是一个连续的、周期性的波形，它在数学上非常容易描述和分析.   如果把原始的光源场景想象成一个简单的正弦函数的话(sinusoidal brightness distribution是明亮和黑暗条纹的图案), 那么在比如说光线在经过镜头以后, 如果成像系统是完美的，我们应该得到一个清晰的、与原始正弦波相同的图像。也可以说, 使用正弦图像我可以得到一个稳定的结果进行分析. 这里的稳定不是指成像质量.   但是成像系统总是存在一些缺陷，玻璃的折射, 空气灰尘, 传感器的噪声. 实际得到的图像正弦波会与原始的场景的正弦波有所不同。 但是Several of its properties also remain stable or at least have nothing to do with imaging quality: The direction of the stripes does not change and the frequency – the number of stripes per unit length – only changes according to the imaging scale.   由于成像系统的缺陷, 光线不会完美地聚焦在它应该聚焦的地方。这就导致了所谓的“点扩散”. 也就是说, 某光线不仅照亮了应该照亮的区域, 也让周围应该是暗的区域发亮了.  这种光线的扩散效应改变了明暗条纹之间的亮度差异，使得图像的对比度降低。   回到需要解释的图,      黑色曲线代表了实际风景的正弦条纹的亮度分布(which means 原始的一个简单明暗分布), 而空心圆组成的image线条是实际成像后的结果图, 可以认为是照片的实际效果, 这里埋一个伏笔.   然后是红色点和红色线, 蓝色点和蓝色线, 这是一开始我没有理解的部分, 我以为有什么特殊含义, 但是, 其实这组Point Spread Profile说明了有一个点光源分别在成像的时候, 落在了某个地方对原始光线造成了影响, 点作为亮度最高的部分, 下面的曲线则代表了光源的Profile, 很好理解. 从最亮的地方, 逐渐减弱,这样.   于是定义来了,   1⃣️ the difference between bright and dark is referred to as “contrast”.   2⃣️ the difference between maximum and minimum for all sinusoidal, periodically changing quantities is called “modulation.”   那么这两个相似表述的区别是什么, GPT说, 对比度通常通过比较图像中最亮和最暗部分的亮度来衡量。调制通过信号的最大值和最小值的差异来衡量，反映信号的幅度变化。OK还是没理解, 回到图.   如果我们使用对比度的相关概念来描述, 在一张高对比度的照片中，太阳的亮光和树影的黑暗之间的差异非常明显。而使用调制的逻辑是, 在一个调制较高的正弦波信号中，波峰的高度和波谷的深度之间的差异非常大。   我感觉这里光学的modulation和信号处理的modulation的差距有点大, 虽然学的不精, 但是依稀记得是改变载波信号的幅度/频率/相位来传递信息(AM,FM,PM), 好的按下不表.   在这里之前的伏笔来了, “对比度”指图像中明暗区域之间的差异, 但是在事实上, 是我们判断镜头分辨率的关键.   我们引出了   调制传递函数（MTF）   用来评估光学系统将物体上的调制传递到图像中的能力。通过比较图像的调制(图像)与物体的调制(图像)来计算，结果是一个介于0和1（或0%到100%）之间的数值。   在这里文章给出了一个列子,有点难理解的, The photographer is used to expressing bright-dark differences in aperture stops, which is also very reasonable as the perception of our eyes follows such logarithmic scales. But, what, for example, does a modulation transfer of 50% mean if our pattern of stripes consists of a difference of 6 aperture stops between the brightest and darkest points, i.e. a brightness ratio of $1 : 2^6  = 1 : 64 $ ? Is the difference in the image 3 aperture stops or $1:32$, which would correspond to 5 aperture stops? 神来一笔, 我们如何理解 modulation transfer of 50%, 调制传递率（Modulation Transfer）的50%并不是指整体画面会变暗50%。 Both would be wrong. In reality, we would then still have approximately 1.5 aperture stops in the above-mentioned case.   给出Contrast计算的公式 \\(\\text{Contrast} = \\frac{\\text{Maximum} - \\text{Minimum}}{\\text{Maximum} + \\text{Minimum}}\\) 我觉得还是挺容易理解的公式, 带入Contrast值=50%, 然后文章给出了例子, 没看懂从哪里来的, Therefore, in our example, the contrast of the object is 63 divided by 65, or approx. 哦, 就是突然给了两个值, Maximun-Minmun=63, Maximun+Minmun=65, 再63÷65≈0.97. 作为经过调制传递率为50%的成像，我的理解是满足上面那个公式上的定义, 然后MTF=50% 开始定义, 这时候文章下面有一个图我去比较了, 结果又理解错了. 正常的步骤是直接得出0.97的一半, 大约是0.48, 然后 (x-1)/(x+1)=0.48得到我们的估算值为 x≈2.846, 所以我们的档位从6掉到了2.846, 光量从64掉到了7.19004101289. 我在这里还是没有理解档位的问题…见下图      圖３：將反差對比的定義以正弦波圖形表示，兩者比値與測量位置情報共同構成MTF曲線。本圖經過翻譯及重繪。原始圖片來源 &amp; Credit：戶村賢一、〈MTF：MTF曲線から読み取るレンズ特性の正体〉、page 128、《ライカ通信》 Vol.1, 2000年4月。      原始圖片來源 &amp; Credit：Hubert H. Nasse, How to Read MTF Curves, page 6, 《LENSPIRE》 。   为什么这么像PN结放大…   我感觉这个地方, 一开始也有理解错的地方, 我的疑惑点有这些      MTF在图像中, 在曲线上是一个固定的参数, 好像是不会变化的, 但是在调整光圈档位的时候, 传感器接收到的maximum和minimum的光会减少, 我感觉这个在光圈变化后会变化, 但是MTF没有变.   如果研究的目标是Object, 那我感觉光圈变化不会影响MTF啊, 因为环境的maximum和minimum的光量不会变啊   对档位的理解有点奇怪, 0,1,2,3,4,5档, 如果使用$2^n$ 来理解的话, 可能可以想通, 我之前一直没搞明白, 就是越调档, 光量越小, 结果更加好? 反正我觉得有点奇怪.   这里档位, 有个点没有明白就是比如, object contract有10档, image contact只有最高有6档, 那么object contract的10档中的第6档和image contact的最高档第6档的光照亮度强度是一样的么   横坐标从1开始的, 有点没理解   OK我觉得这里的混淆, 主要是由于参照坐标系的混乱造成的.   那么这个图怎么看呢, 比如取最高点(MTF=97%进入平缓发展的点), 取(10,6)点为例, Object Contrast (Aperture Stops) = 10, Image Contrast (Aperture Stops) = 6, 使用contact的定义, 在 $2^n$带入6和10, 得到64和1024(最高亮度值), 他们的最低值都是1, 所以 \\(\\text{Object Contrast} = \\frac{L_{\\text{max, object}} - L_{\\text{min, object}}}{L_{\\text{max, object}} + L_{\\text{min, object}}} = \\frac{1024 - 1}{1024 + 1} = \\frac{1023}{1025} \\approx 0.9976\\) 再得 \\(\\text{Image Contrast} = \\frac{L_{\\text{max, image}} - L_{\\text{min, image}}}{L_{\\text{max, image}} + L_{\\text{min, image}}} = \\frac{64 - 1}{64 + 1} = \\frac{63}{65} \\approx 0.9692\\) 这里引出/使用计算mtf的公式 \\(\\text{MTF} = \\frac{\\text{Image Contrast}}{\\text{Object Contrast}} = \\frac{0.9692}{0.9976} \\approx 0.97153167602\\) 下面是作者对于这个图的简单总结, 一开始没懂怎么来的, 感觉还挺难理解的.   我们可以从三个方面来理解 MTF 曲线的特性：      高 MTF 值的小差异在高物体对比度下特别显著：            当物体对比度很高时（例如光圈档位为 9 或 10），MTF 曲线的高值（例如 90% 或 97%）之间的细微差异对图像质量有很大影响。即使 MTF 仅从 90% 增加到 97%，在高对比度的物体下，这种小的差异也能显著提高图像的对比度。                  弱的色调变化（小于一个光圈档位）不需要高 MTF 值：            当物体对比度较低（例如低于一个光圈档位时），高于 70-80% 的 MTF 值差异几乎没有实际意义。这意味着在低对比度场景中，即使 MTF 下降到 70-80%，也不会对图像质量造成明显影响。                  非常低的 MTF 值下，物体对比度的高低几乎没有影响：            当 MTF 值非常低（例如低于 20%），无论物体的对比度多高，图像的对比度总是很低。这表明系统在低 MTF 下对比度还原能力很差，导致图像质量大幅下降。                  OK, 在看了这三个特征总结之后,  object光圈档位调整的含义和具体的光圈档位感觉可以澄清, 就是不同的光量, 如果两个contract都是处在Aperture stop=6的情况下的话, 他们的光量都是$2^6=64$, 也就是说是一样的. 我一直有一个错误的想法, 就是把一个环境和一个传感器上的光均匀切割, 然后分成不同的等份作为不同档位的错误理解.   调制传递函数（MTF）表示成像系统在不同空间频率下传递对比度的能力,   这个位置的小节, 我觉得我从这段文本得到的新的认知是, 我在之前从来不知道,对比度, 明暗,对清晰度的影响, 或者说对比度在直接控制清晰度? 感觉这个说法有问题, OK, 在对岸的一个作者的一篇文章上找到了答案.   下面小结的内容来自Weifu Lin在Medium平台上的文章, 链接在 如何閱讀MTF(一) , 是作者结合蔡司的这篇文章和日本的一个作者的文章的翻译. 有两个图非常非常好, 指明了contract和Resolution的区别.      繪圖：Weifu Lin   这里的内容是直接引用的[5]. 兩種不同設計取向的鏡頭的比較。鏡頭B的解像力較高，但實際拍攝時，鏡頭A會給觀賞者「更銳利」的觀感。箭號位置所指的虛線，是人眼能分辨細節的最低臨界點。本圖經過翻譯、修訂以及重繪。原始圖片來源 &amp; Credit：戶村賢一、〈MTF：MTF曲線から読み取るレンズ特性の正体〉、page 129、《ライカ通信》 Vol.1, 2000年4月。      到这一步的时候, 我有点没看懂…   Modulation transfer function, resolving power   It is obvious that one single stripe pattern is not sufficient to characterize the quality of a lens. A very coarse pattern with large separations between bright and dark stripes could, of course, also be imaged well by a lens with a relatively large point spread function. If we decrease the separation between the stripes, however, so that the separation between bright and dark approaches the size of the point spread, then a lot of light from the bright zone is radiated into the darker zones of the pattern and the image contrast becomes noticeably lower. 我觉得可以理解成原来黑白粗线条的object contract比较高, 因为每个亮区和暗区之间的光线干扰较少. 但是在原本光照环境没有改变的情况下, 一些细小的线条远看是灰色, 导致亮区变暗，暗区变亮, 这会造成对比度的降低这样. 有一个形象的解释是, 写毛笔字, 尺寸比较大, 气势磅礴的字体可以使用大的毛笔, 但是涓涓小楷, 甚至在鼻烟壶里面写字, 这些艺术家为我需要更精细的书写工具. 菜刀如何雕刻核舟记?   为了比较不同精细度的条纹图案来研究镜头的成像能力(其实就是复杂环境)，我们使用调制传递函数（MTF）来量化. 而且, 为了研究镜头如何成像不同精细度的条纹图案，我们需要为每一个条纹图案确定一个调制传递。将这些调制传递值绘制为一个描述条纹图案精细度的参数的函数，这些值就形成了一条曲线，即调制传递函数(MTF)。   我们如何量化条纹图案精细度   条纹图案的精细度：通过计算图像中每毫米包含多少个条纹周期来测量。一个周期是两个亮条纹或两个暗条纹之间的距离，或由一个暗条纹和一个亮条纹组成的线对的距离。   空间频率(Spacial frequency)：图像平面上每毫米的周期数称为空间频率，单位是每毫米线对数，简称为 lp/mm。   下面是文章中的一个图片例子,      GPT对图注的解释如下      Measurement Aperture 2: 测量是在光圈 f/2 下进行的。这意味着镜头的实际成像性能是通过在 f/2 的光圈下拍摄条纹图案来测量的。   Diffraction-limited Aperture 5.6: 理论上，光圈 f/5.6 是受衍射极限影响的最佳光圈。也就是说，在 f/5.6 光圈下，成像质量接近衍射极限，表现出最佳的清晰度和对比度。   Measurement Aperture 5.6: 测量是在光圈 f/5.6 下进行的。镜头的实际成像性能是通过在 f/5.6 的光圈下拍摄条纹图案来测量的。   Diffraction-limited Aperture 16: 理论上，光圈 f/16 是受衍射极限影响的最佳光圈。在 f/16 光圈下，成像质量接近衍射极限，但由于光圈较小，衍射效应变得显著，从而影响图像清晰度。   For purposes of comparison the diffraction-limited transfer functions for f/5.6 and f/16 are also shown (solid line without circular dots). The diffraction-limited image is the best possible one.   什么是Diffraction-limited, 衍射受限(通常用于描述光学系统的成像性能)   Diffraction-limited 是描述光学系统在没有像差和其他缺陷情况下，由于光的衍射效应所能达到的最佳成像质量。当光线通过光学系统（如镜头）时，会由于光的波动性而发生衍射，这种效应会限制系统的分辨能力. 衍射：与光的波动性质有关，当光波遇到障碍物或狭缝时发生偏折. 看到这个地方的时候, 我不知道光的衍射是什么, 也不知道为什么会影响光学系统的能力, 但是这里的意思, 因为这个性质的存在, 有了极限, which is 图示中的黑色实线(接近直线).   OK, 换用新图, Winfu前辈做的图真的是好,      下面是文章的内容的直接复制(没有换成简体中文)   為了作為比較，也標示了f 5.6（橄欖綠色虛線）以及f 16（灰藍色虛線）的繞射極限的轉換函數。達到繞射極限上限的影像是理論上的最佳影像品質，在圖表上呈現幾近完美的直線，下降率與空間頻率成比例。在到達極限頻率（limited frequency）時，MTF趨近於０，其頻率由光的波長與光圈値兩個因素決定。   这里又想补充一点就是, 突然想到, 超过了这个原来的黑色实线(新图的蓝绿虚线,也就是繞射極限), 就会因为衍射的问题, 让成像崩了, 这样?  現實中的鏡頭即使校正，仍帶有殘餘像差，因此MTF曲線一開始會快速下降，然後緩慢趨近於０。曲線明顯向下彎折，就像上面圖６中光圈為f 2的洋紅色曲線一樣；至於光圈縮至f5.6後的藍色曲線和理論最佳値的差距就相當接近了。   當MTF曲線降至０或低於一個臨界點（Threshold） — — 例如10％，其空間頻率即為光學鏡頭在空氣中的解像力，這意味著一旦超過臨界點，黑白條紋的明暗結構整體變成灰色而難以辨識，這又是另一個問題。   圖測量鏡頭在光圈f 2時的曲線，空間頻率到達120 lp/mm時，幾乎是一片平坦，即使空間頻率增加，反差對比也幾乎沒有變化，這樣的測量非常不精確，鏡頭解像力可能達到160 lp/mm以上，也可能只有120 lp/mm。   這樣的判準，並不適合用來評判一隻鏡頭的影像品質。此外，空間頻率與數位時代的影像感測器（image sensor）的「解析度」，兩者也不能混為一談。   像差 (Aberration)   在低空间频率下，像差影响相对较小(为什么?), 随着空间频率的增加，像差的影响逐渐显现(我觉得是容易变灰, 高频下间隙太小的原因)   像差的类型(GPT生成)   球差（Spherical Aberration）      发生在球面透镜中，中心和边缘的光线不能聚焦到同一点。   导致图像中心和边缘的清晰度差异，影响整体锐度。   色差（Chromatic Aberration）      由于不同波长的光在通过镜头时折射角度不同，导致不同颜色的光聚焦在不同位置。   产生彩色边缘和色散现象，尤其在高对比度边缘处明显。   彗差（Coma）      主要影响图像的边缘部分，使点光源呈现为彗星形状。   影响图像边缘的锐度和对比度。   像散（Astigmatism）      垂直和水平方向的光线不能同时聚焦在同一点。   导致图像在某一方向模糊，影响细节表现。   场曲（Field Curvature）      图像平面不是平坦的，而是弯曲的。   中心清晰，边缘模糊或相反。   畸变（Distortion）      图像几何形状变形，直线变弯曲。   “其空间频率即为光学镜头在空气中的解像力”   表示在MTF（调制传递函数）曲线下降到某个低阈值（例如10%）时，对应的空间频率就是这个镜头在空气中的解像力。换句话说，这个空间频率是镜头能够有效分辨的最高频率。      举个例子, 假设某个镜头的MTF曲线在30 lp/mm时降到10%，那么这个30 lp/mm就是这个镜头在空气中的解像力。也就是说，这个镜头能够分辨的最细微的细节是每毫米30对条纹。超过这个频率，图像细节将无法分辨，变成灰色或模糊。   下面是文章的内容的直接复制(没有换成简体中文)   測量鏡頭在光圈f2時的曲線，空間頻率到達120 lp/mm時，幾乎是一片平坦，即使空間頻率增加，反差對比也幾乎沒有變化，這樣的測量非常不精確，鏡頭解像力可能達到160 lp/mm以上，也可能只有120 lp/mm。      我怎么感觉文章里面没解释过下图      這樣的判準，並不適合用來評判一隻鏡頭的影像品質。此外，空間頻率與數位時代的影像感測器（image sensor）的「解析度」，兩者也不能混為一談。   這也是蔡司為何決定採用MTF來描述成像品質的原因之一。我們並不直接用眼睛觀察相機鏡頭成像，鏡頭後方總是需要一個媒介：傳統銀鹽底片、CCD、CMOS、掃描器、投影機……等等，有類比式的也有數位式的。   所有媒介（包括人的眼球在內）都有自身的影像特性，每一種影像特性也可以用一組轉換函數來分別描述。MTF的優勢在於：整體的光學成像鏈的MTF是（接近於所有）個別MTF的乘積。   Let us consider a few typical examples:      有两个图, 第二张没放, 这的意思感觉就是要注意镜头素质和传感器的素质.   Product of two modulation transfer functions: Very good 35mm format lens and color negative film. The product is always smaller than the smallest factor in the imaging chain.   In this case, the total modulation is essentially limited by the film. If one specifies a minimum of 10% modulation transfer, one must expect a resolving power of 80100lp/mm. If further elements such as projection optics or the eye are taken into account, the product is even slightly smaller.   在评价光学系统性能时，为什么通常不需要考虑非常高的空间频率   理由   在这里, 我们观察使用的是MTF product lens x film, which is 看到整体的光学系统对不同空间频率的响应, 乘积后MTF越小，表示系统在该空间频率下的表现越差。涉及更多的传递函数(MTF) 往往只会使乘积变小.   数码传感器的空间频率限制和实际使用下的限制   24百万像素的35毫米全画幅格式传感器和15百万像素的APS-C格式传感器，其奈奎斯特频率大约为90lp/mm, 这是如何计算出的   同时, 40lp/mm已经足以提供足够的细节和清晰度，超出这个范围的细节对于大多数实际用途来说并不显著, 我不知道为什么. 40lp/mm被认为是一个合理的上限，因为它既能够提供足够的图像细节，又不会受到高频混叠等问题的影响。   Nyquist Frequency   想起来在信号系统里面学过, Nyquist Frequency是采样率的一半, 只要离散系统的奈奎斯特频率高于被采样信号的最高频率或带宽，就可以避免混叠现象。对于CMOS传感器而言，它表示传感器能可靠捕捉到的最高空间频率。在图像处理中，采样频率是指传感器的像素密度。 \\(f_{\\text{Nyquist}} = \\frac{1}{2} f_{\\text{sampling}}\\)   90lp/mm（每毫米90对线对）   每毫米有90对黑白线对   Edge definition, image contrast   以下繁体文字内容为直接复制[5].   但是，這些「數據變化」對實際影像品質而言有何意義？當我們談論「清晰銳利」、「明亮度」、「細節解析力」時，和這些數據之間有什麼關連？   我們拍攝的主體本身顯然不是正弦波。它們只能在實驗室中透過大量測試階段生成，使用其他目標對象進行測試，並以數學方式推導出正弦波的調變。   蔡司使用的是一種「明暗變化明顯的長方形黑白條紋圖案」的特製測試圖表，來評估相機鏡頭的有效解像力。   精細的，重複變化的圖案，僅僅只佔據我們的視覺功能中用來辨識影像品質的一小部分。重點是明暗不同亮度區域之間的邊界。因此，蔡司還必須研究MTF與邊界再現（reproduction）兩者的關係。說到這，我們不得不回到起點：點擴散函數。   然后在文章中就出现了4个图, which is由三并排图组而呈现的设计(有点绕口), 我一次性按顺序放在下面,   1⃣️      2⃣️      3⃣️      4⃣️      The following images show from left to right:   Intensity profile of the point spread function, 也就是每个图中的第一个section, 这个是光的点扩散函数, which means  that 点光源/或者说一束光在通过镜头折射穿透后, 在传感器上的分散的切面, 我觉得可以理解成底部的位置就是传感器的感光位置, 以图1⃣️为例子, 这是一个很好的镜头成像, 非常清晰, 点光源只扩散到了-20µm和20µm之间,   Intensity profile of two edge images, 对于这里的Edge profile我的理解是主要关注0µm处如何变化, 还是以图1⃣️为例, 发现在0µm初变化的非常快速, 这是一个好的镜头的表现.   The corresponding modulation transfer 这里是柱状体描述的MTF图示, 拜文章所赐, 文章会见到不同厂商各式各样的格式,方法和规格的MTF的图片. 还是以图1⃣️为例, 这是一个非常好的MTF小图, 不知读者有没有意识到, 从5Lp/mm开始接近100%的MTF率, 到80Lp/mm下仍然保持了接近于50%的MTF率. 原作者评价$\\rightarrow$ The image of the edge is sharp. In the language of modulation transfer, this characteristic is recognized by the fact that all values at the important spatial frequencies are very high and do not decrease so strongly towards the higher frequencies.   For a lens with such imaging performance, the image quality achieved is usually limited by the sensor or by other factors such as focusing accuracy, camera movements etc. 好家伙, 镜头不会限制图像的表现, 差的结果是一些客观的因素.   下面是针对不同的图的结果分析   Pic 1      Intensity profile of the point spread function for Pic 1.1   这个图在之前解释过, 可以被考虑成是一个竖直光线照在传感元件上的横切面, 可以想象成一个千层蛋糕的切面. 同时也在之前提及过, 这个数据非常非常优秀,      Intensity profile of two edge images for Pic 1.1   在之前的解释中, 没有被明确指出, 现在, 可以注意在新的图示中我划出来的红色竖线, 这表示明暗的交接线. 可以看到, lens有非常非常好的过度变化, 在经过0的时候瞬间过了,   Modulation transfer MTF for Pic1.1   非常非常优秀的MTF曲线表示, 在5lp/mm情况下,  正常的随着条纹更加精细而不断下降的数值, 合理而正常.   小注   本来是计划把4个表都解释一遍, 但是发现, 好像没有现实意义, 因为文章的意图是理解MTF曲线, 但是这种形式的MTF曲线已经不再使用. 但是在这个部分要指出的是, MTF测量表会不准, 显示 出错误的数据信息, 在文章中的例子就是, 比如现实图像在影射后, 在cmos上位移会没有显示清晰的图像, 但是位移的图像在某个Lp/mm的条纹下, 移动到了下一个位置, 导致图像没有变模糊, 仍然清晰. 虽然镜头的素质没有提高, 但是由于测试的是这种特殊的条纹, 让MTF的曲线的数据提高, 这是虚假的成绩. 这个现象在文章中被专门指出. 我在这里直接引用, But: There is no contrast at 40lp/mm! The curve of the modulation transfer can drop to zero and then increase again.  This is then called “spurious resolution”, which is a somewhat unfortunate expression because the structure with 60lp/mm is reproduced with a clear resolution.   文章中的第四种MTF曲线(The MTF values of this 4th type)   ","categories": [],
        "tags": [],
        "url": "/how-to-read-mtf-curves.md/",
        "teaser": null
      },{
        "title": "签证准备材料list",
        "excerpt":"记录签证准备材料的list   美国签证准备材料 (详细版本)      护照原件、旧护照原件（如有）   签证预约确认页   SEVIS FEE缴费收据   DS160 表格确认页   i20表格（签名）   2张白底签证照(51mm*51mm)   存款证明原件   英文简历（研究生必须）   学校录取信   Study Plan （研究生必须）   导师个人简历 （如有）   个人论文（如有）没有   自带包裏 透明没有拉链, 塑料A4纸夹片   学校成绩单中英文版本   在读证明中英文版本（如在读)   毕业证/学位证中英文版本（如毕业）   标化考试（如TOEFL、IELTS、GRE、GMAT、SAT 等）成绩单复印件或原件   父母双方收入证明   户口本/身份证   房产、车产证明   美签学签证-简单版本      护照   身份证   DS-160（含照片和条码）   签证预约信（含条码）   SEVIS缴费证明   i-20表格   录取的offer   个人简历   学习计划(study plan在美国大使馆官网可以下载)   经济文件: 做i20时候的存款证明   户口本   签证预约缴费单   51*51 mm的签证照（两张）   加粗的部分是最后使用到的文件.   日本签证           1、彩色白底免冠证件照电子版;            2、护照首页（照片信息页）及末页（第46-47页）的清晰完整对开扫描件（护照剩余有效期需大于回程半年以上）；            3、户口本扫描件（除空白页都要）；            4、非上海领区户籍需额外提供居住证正反面扫描件/在职证明+6个月以上社保/ 在职证明+近1年内税单（上海市内居住证需额外提供验证页面截图电子版）；            5、结婚证、离婚证扫描件（如有）            6、出生证扫描件（未成年必须提供）            7、日本签证受理表（电脑编辑填写，必须填写完整）            9、个人信息处理同意书（需填写好日期，姓名和手写签字，打印出来签字提供扫描件，一家人可以写在一起）            10、提供以上基本资料+经济材料（7选1）       温馨提示：领馆有权利让您补充材料，若被通知请及时配合提交；拿到签证之后再定机酒。  ","categories": [],
        "tags": [],
        "url": "/visa-preparation/",
        "teaser": null
      },{
        "title": "Life goals before the age of 30 (in addition to finding a job)",
        "excerpt":":)      Suceived in GT in the QCF and CSE program(wanna dual degree)…   Use the money which I earned to buy two leica cams (m1x, mp)   Be proficient in using a sewing machine to make small bags and edc backpack design under your own understanding (difficult)   Japanese level is equivalent to IELTS 5.5 in Eng(I think it is enough), and I can communicate with native jp people.  (difficult)   Reasonable understanding of the process, materials, steps, installation and design of house decoration   launched two self-developed independent apps, iOS / web OS   时间很长却也很快.   感觉人还是要活的现实一点, 不要太有梦想, 做一些本身有实际价值事情, 而不是感动自己/他人的事情.  ","categories": [],
        "tags": [],
        "url": "/Life-goals-at-30/",
        "teaser": null
      },{
        "title": "2025-03-07-How I use LLMs by Andrej Karpathy 看后总结",
        "excerpt":"https://www.youtube.com/watch?v=EWvNQjAaOHw   当然这个视频, 感觉是一个挺好的材料, 从一个很简单的角度切入, 快速判断什么任务可以实现 两个多小时的视频, 结果花了三天的时间还比较碎片的时间才看完, 之前通宵两天熬夜太狠了, 感觉看的时候一直没有很专注, 然后还没干什么事情, 笑死. 注:  回到这个这个视频, 回顾的想, 这个视频的信息含量没有涉及什么复杂的步骤和内容, 更多的是一个科普向的视频, 或是一个操作演示一样. 当然打开之前我就知道, 干货应该不多, 肯定不如Let‘s build 那个系列, 我只是想比较一下我使用LLM和Andrej Karpathy使用LLM的区别. 接下来的步骤都是我看过之后一次性回忆的, order可能有问题, 然后一些sub subjects会有遗漏. 加上实在感觉我看完了不等于是会了, 我认为会了仍然还有大量可能的错误, 不如直接写下来. 相信即使是最简单的逻辑和常识也需要被整理. 花一点时间, 把要大量使用的工具理解清楚我觉得还是对我重要和有价值的.   笼统介绍   一开始，Andrej Karpathy 提到，他几乎订阅了所有主流 LLM 模型的会员服务(2025 年 3 月) 接着，用了一个特别形象的比喻which is zip 文件，来形容大模型预训练后的状态。这个比喻很有意思，因为 zip 文件如果不解压，里面的内容是没法直接用的。换句话说，预训练后的模型并不像字典那样，能瞬间（O(1) 时间）找到你想要的东西, 所以模型不是pageRank那样的搜索?  LLM good at writing. 然后，Karpathy 花了一些时间解释 token 的概念，简单了解一下非常有意义。 无论是文字、标点符号，还是输入输出，都会消耗 token。我们看到的是连贯的文章，但对模型来说，它处理的是一串 token。LLM 的输入是 token，输出也是 token，它的任务就是预测下一个可能的 token，一个接一个地生成文本。（这里我打个比方：假设人 A 对美国人 B 说中文，但 B 听不懂中文。于是，A 的中文被转换成了一串数字（比如 Unicode 编码），B 看到的是这串数字。虽然 B 不懂中文，但他已经通过学习知道这些数字之间的关系，通过不断的预测, 最后他也用一串数字来回应。最后，这串数字被解码成中文，A 听到了 B 的回复。）然后视频演示了下面这个开发者做的网站，它能展示输入输出时 token 的具体的展现： https://tiktokenizer.vercel.app/   继续讲了预训练和后训练   预训练很贵, 预训练使用大量的历史上的材料, 互联网上已经有的内容, 组成了一个由token之间的关系的size很大的zip文件. 但是这个zip文件是有损失和有概率 zip 文件. 是sequence of token.   后训练 much much cheaper. Using SFT（Supervised Fine-Tuning, 监督微调）、RLHF（Reinforcement Learning from Human Feedback, 人类反馈强化学习） 和 RL（Reinforcement Learning, 强化学习）on Conversations, 这里Andrej认为post train类似于角色扮演, 后训练的重点是调整模型的行为，而不是再教知识, 我感觉对于使用者来说了解到这里就OK了? GPT讲, 换句话说，预训练教会了模型语言，而后训练教会了它如何像人一样“扮演”某种对话风格或人格。  开始演示使用大模型  展示了不同模型随便输入一个问题后, 显示了一些输出什么的. 这里我记得没什么好着重提及的, 可能Andrej使用了perplexity来使用deepseek的模型有点难评.   这里有一个细节, 我之前没有注意, 我之前以为perplexity这家公司没什么技术, 但是在演示中发现, perplexity搜索速度蛮快的, 这里有我不知道的点.   还有这里解释为什么会出现不正确输出的原因, 因为预测本身就不会是100%正确的, 很正常很自然.  还讲了GPT模型不透明的情况, 比如虽然选了GPT-4o但是你怎么知道运行的是不是4o, 明确的提及了在用户没有登录的时候, GPT提供的是一个size非常小的模型.  上下文搜索  现在的模型, 只要用过, 就会发现模型具有一定的上下文记忆能力，即它能够理解并引用同一对话中的先前输入和输出。此外，如果持续在同一对话中交流，token 消耗会非常快，这也意味着模型实际上是在不断地处理和整合之前的对话内容。每次新的输入都会与之前的输入输出结合.   看视频的时候直接跳过了, 但是这里有个疑问, 就是, 模型是怎么选择之前的记忆的呢? 首先这个window肯定有一个size, 如果已经输入的内容和输出的内容过长, 这个window肯定是装不下的, 但是抛弃之前的内容的策略是? 之前就知道会丢和能保存.      问了GPT,  LLM 的上下文窗口(context window)是有限的，有一个固定的大小(如 4096、8192 或更大的 token 数), 当输入和输出的内容超出上下文窗口的容量时，模型必须决定如何舍弃部分旧内容。策略有, 滑动窗口（Sliding Window）, 基于重要性截取（Relevance-Based Pruning）, 层次化摘要（Hierarchical Summarization）, 基于提示（Prompt Engineering）    OK, 策略这么多, 如何这里如何选择也是一个黑盒…  不同模型的价格  这里就是说一下不同模型的价格和提供的服务不同, 没有涉及到api.   联网搜索 * Search  为什么有联网搜索的功能? 本质上因为创建那个zip file的过程是一个由时间限制的东西, 决定使用23年4月为止的内容, 那么之后的内容当然涉及不到. 比如询问大模型一个今天的天气, 有点强模型所难了. 如果使用搜索改变了什么呢, 相当于爬虫爬取了网上搜索得到的内容, 作为输入, 那么这里有新的输入了, 这部分知识, 在输出回答的时候, 就可以回到诸如今天的天气和气温这类的问题.   Reasoning Model  由GPT-o1开始, 强化学习来解决数学编码等逻辑问题, 这个思路被做出来了一个产品. 原理我不懂,  有空有必要了再学吧. 这里浅显的说法是, 对于数学推理、代码生成等任务，我们希望模型一步步地推理，而不是仅仅根据概率生成看起来正确的文本.   所以之前一个高中生的prompt让claude 3.5变reasoning model的新闻有点扯淡. 两个看似相似的输出, 输出的文本背后的逻辑都不一样.  deepResearch  有点感觉是一个set of reasoning model的组合, 忘了, 回去看的时候再补一下, 这里好像说到了现在grok和gpt都有这种能力, 挺有潜力的一个功能, 但是我目前没感觉特别厉害, 或者我没有使用场景. 我冲了20美元的GPT会员使用下来(可以用10次), 我用了两次, 我使用的情况是没跑出来什么特别有用的东西…  LLM 文件上传(主要是pdf)  这里在使用claude的时候, 提及了在pdf中的插图应该是不会被使用的. 识别的都是大量的文字, 应该也是类似于多模态调用OCR的能力? 虽然我事后搜索了一下Claude有识别读取pdf中图片的能力, 所以, 不知道是不是 Karpathy 讲错了还是在使用中没有发现对插图有识别和作为输入. 我在之前使用的时候没意识到这个细节, 但是我记得在只输入图片的情况下是可以识别的(which就是一张图中有示意图和文字解释或文字内容), 但是我记得之前输入一个graph, 让GPT判断后输出BFS或者DFS还是挺费劲的, 这里不知道是为什么. 感觉要了解图片是怎么被LLM读取的才可以搞明白.   这样就有一个疑问就是, 我希望LLM在处理图片输入的时候, 希望它在使用正确的信息, 这里在prompt的地方可以说一下, 提及一下就是, 让模型输出OCR后, 它看到了什么, 这样可以在输出OCR结果不对的情况下发现错误.  Code interpreter(容器?) data analy  LLM的代码能力非常好, 我猜测是因为代码有一条类似于flow的处理逻辑, 是不是非常容易被强化学习拟合啊? 就是, 输入的数据啊, 个数啊, 类型啊, 都是设计好的, hh无端猜测了.  这里想到一个经典问题, 好像是9.9大还是9.11 大什么的,这里如果使用python解释器来输出结果应该是没问题的, 但是我记得有新闻是说, 这里GPT会比较的是圣经的章节, 所以 9section的第9 subsection当然小于9section的第11 subsection的, 这里想到这是一个简单的小计算过程, 如果在一个大型项目中, 使用LLM认为过于简单没有自动调用Code interpreter使用python输出来比较的话, 在这一步给出一个正确的答案会怎么办呢? 过程如果过于多, 还挺难定位这个问题的 在这里视频的最后写了一下直接使用GPT做data analysis其实网上有大量的资源, 不展开介绍.然后在演示的时候, GPT还出现了数据的错误. 我记得是一个最大值的数据点搞错了  One more related tool , Claude Artifacts(apps, diagrams)   这里Andrej Karpathy甚至生成了一个anki卡片的网页应用, 然后来检测从wiki上复制的亚当斯密的知识点, 神了, 反正我之前是没有这样使用过和没想过的. 有交互还有反馈. 然后在这个section, Karpathy介绍了一下用Claude生成mermaid流程图, 然后展示出来, 这个事情其实我在GPT3.5的时候就做过, 生成完code再粘贴到mermaid模拟器上, 在视频中看到Claude就是多了一个出完code的时候及时生成展示图片的功能, 好确实好, 但是没感觉多好,hhhhh, 可能就是有没有想到这样去用LLM来得到这个输出的区别. 我想想当时好像是软件工程的作业要画流程图, 我发现一个geek朋友非常喜欢用代码写那个软件的工程图, 我们大部分是用线上的画图软件自己画的, 我画了第一个图之后,感觉非常非常麻烦. 我没学过那个流程图的代码框架, 这时候想到why not 用GPT生成那个结构代码, 刚好typora也支持mermaid, 用代码写这种流程图完全不需要关注框图的间距, 箭头的粗细这种问题, 然后我就一直使用这样方法写这部分的任务. 后来的latex也是. 一次一次的任务然后习惯了使用md的语法, 最开始是发现gpt一直在使用这种格式的文本输出, 觉得挺轻便和美观的就在b站学了半小时之后写到了现在.  这里我想到LLM直接跑D3.js还有Graphviz应该都蛮棒的 这里Andrej Karpathy的用法是, 使用mermaid梳理文本, 梳理人物关系网, 梳理故事发展过程, 这个用法我之前没想到过, 以后可以多加使用.   (我加的)任务处理  下次再说吧, 是Claude开发的一个工具.  cursor  之前看课代表立正的视频的时候, 里面一个视频讲, 对于自己的粉丝做了一个调查是, 认为过去一年ai没什么发展的和ai发展飞速的好像是一半一半, 但是认为过去一年ai发展快速的人中, 绝大部分的人是使用过cursor的,  这里, 就是感觉, 虽然明面上说是整个文件都是一个上下文窗口中读入的, 但是项目一大, 应该仍然是会丢失的, 那么那些神奇的策略们的选择黑箱, 该如何选择, 这里的策略好像不可以被人为调控.  Karpathy飞速的做了两个demo, 挺不错的. 有点神奇的是, 如果本地环境下, asset中没有一些文件, (视频中是一个点击的声音特效, 还是胜利音频特效来着), cursor居然会联网爬一个库下载下来, 我真是一脸正经, 之前一直以为是巧妇难为无米之炊的, 这还可以自己下一个文件过来? 但是转念一想, 好像这和运行一些项目要使用一些lib然后自动下了一些pip好像没什么不同, 然后又不觉得特别神奇了.  使用声音输入 / 输出   这里只是使用speech-to-text模型输入文本给GPT, 然后得到文本的返回, 没有使用下面的语音模型. 很多时候, 其实我们只需要一轮问答, 比如, 现在的Karpathy 在视频里面讲使用快捷键F5快速调用(类似于siri), 询问一个问题, 得到LLM的回复, Karpathy 说现在接近一半他都是在这个使用场景下使用的 我想了想, 确实比较方便, 打算下个礼拜在自己的电脑上整好这个操作.  使用语音模型, 伦理   这里我记得使用grok可以沟通出一些怪怪的声音和对话内容, 是在使用GPT的语音对话无法实现的, 而且GPT语音还是拒绝回答一些类型的问题, “利用怪怪的声音快速说1-20”我记得视频里面好像说了这个例子.  NotebookLM  输入一些文件, pdf, 然后gemini出一个talk. 我之前有听说这个功能十分惊艳, 但是没引起重视, 或者觉得这个功能无法帮助到我.  1, 是想不出这个功能的实用价值, 首先我不用播客类节目变现, 再是在油管和xhs加上一个B站这些视频流媒体平台提供给我的内容, 无论是否优质, 已经足够足够多了, 我现在的精力甚至是减弱这些平台给我的影响, 减少注意力分散, 我不应该去关注太多信息来源, 虽然很多优质的信息还是从这方面来的, 没办法无法隔离.  2, OK回到这个NoteBook播客功能上, 还有一个原因就是语言(英语水平)还是存在问题, 虽然CS, 科技, 数码 or 摄影相关的英文内容我可以做到95%的时间follow上和听得懂, 但是非母语还是有问题就是, 感觉脑子在输入英语的时候有一层decoder一样, 还是要慢一些, 我脑子在输入英语的时候对专注要求更加高, 这样听这个英语博客还是挺累的, 中文还是我的舒适区无法改变了, 不然这篇文章我肯定是想用英文打出来的.  OK, 所以这个应用就对我来说有点鸡肋, 我不需要使用这个function来获取信息, 加上流畅使用还是有语言门槛, 如果想快速了解某个领域的知识, 为什么不直接上网搜索一个general的介绍呢? 整10篇文章输入, 做一个博客有点奇怪. 或者说, 泛泛的去了解一个domain的知识这样做有点出力太多, 但是如果想认真学习一下, 这样做是远远不够的, 所以我困惑于对我自己的使用场景. 我感觉是不是deep research加上这个NotebookLLM podcast的功能是不是有使用价值一点, 其实我希望连选都不要选topic, 用一个推荐算法什么的, 网易云音乐式得生成高质量podcast如果有一个这样的产品我会愿意去用, 我连花时间喂Notebook的生成podcast的操作都懒.   然后Karpathy说, 整了一些podcast可以在开车路上听, 哈哈好吧, 碎片时间, 可惜我没有车.   图片输入          之前提到过, 图片输入往往有输入的错误, 这里Karpathy说他 使用方法是分成两步, 而不是直接让LLM输出结果, 比如使用prompt, 分成两步, 第一步输出OCR的输出结果, 第二步使用这些结果输出我想要的答案            转成公式 simpleTex, Math Snipping Tool这种识别论文中的公式使用OCR的小而美的business应该都快寄了, 我真感觉用一个LLM的api再使用一个prompt做一个小应用就行, 用cursor做可能不用半小时.       突然有点好奇LLM的输入, 图片和文字的区别, 是直接图片使用分配给OCR的模型读取么  ChatGPT的个性化助手和应用市场, 提示词?  这里涉及了好多Karpathy的个人信息啊哈哈哈哈, 看到网上的comment是他谈了一个韩国女友, 所以在各种姿势学习韩文. OK离开八卦的话是, LLM在翻译任务上简直是出人意料的好, 逻辑流畅舒适, 甚至可以输出一些引申的含义.  还有我其实有个疑问是, 要不要使用xml类似的指令? 使用自然语言和xml语言有对文本读取来说有多大的差别, 如果有差别的话, 我自己感觉是各模型厂家在post train的时候调整的, 还有如果使用api去调用的话, 在code界面, 大家使用的都是xml的结构. 我自己感觉是有区别但是区别不大, 不然其中的一种方法会被大肆宣传的. 比如下面是我希望解释Leetcode问题时候的prompt  &lt;task&gt; 编写一个解决这个问题的程序 &lt;context&gt; 我希望你像一个人类程序员一样思考这个问题。请展示完整的思考过程,包括: - 初步理解问题时的想法 - 尝试解决时遇到的困惑 - 思路的演变和修正 - 为什么最终选择这个实现方案 &lt;/context&gt;  &lt;format&gt; 请用以下格式输出: 1. 初步分析 - 用简单的语言描述你对问题的第一理解 - 列出可能需要考虑的关键点  2. 解决方案构思 - 写下你能想到的几种方案 - 分析每种方案的优缺点 - 解释为什么选择或放弃某个方案  1. 代码实现 - 每写一个代码块前,先解释你准备做什么 - 用注释记录当时的思考 - 如果发现问题需要修改,说明原因 - 展示完整的思维过程,包括错误的尝试  2. 最终总结 - 回顾整个解决过程 - 指出关键的思考点 - 分享可能的改进方向  &lt;style&gt; 注释风格: # 初步想法 # 发现问题 # 修正思路 # 实现原因 &lt;/style&gt;   另外一个疑问是在customize ChatGPT的时候 What traits should ChatGPT have? 和 Anything else ChatGPT should know about you? 在输入的时候到底有什么区别.   演示了一些图片生成, 视频生成   Andrej Karpathy翻了一个twitter的po文, 我本人对这部分功能兴趣不大, 但是看的时候, Google 生成的视频非常真实, 感觉还挺厉害的.   Finally  08-March-2025 本来想用20min写一点想法和总结就行, 但是居然写了快2个小时半才写了一半内容, 麻了, 太废了, 实在是太笨了, 效率太低了,打字速度也慢.  今天先不写了, 不然任务完不成了. 看了一眼左下角字数统计, 一边想, 一边整理两个小时写2100多字, 好像也不算是非常非常慢的整理速度, 好吧, 平衡了.  09-March-2025 今天好像也是花了好久在写这个东西…   希望 llya 这位LLM真神可以在5年之内弄出一个更加逆天的模型.  ","categories": [],
        "tags": [],
        "url": "/How-I-use-LLMs-by-Andrej-Karpathy-%E7%9C%8B%E5%90%8E%E6%80%BB%E7%BB%93/",
        "teaser": null
      },{
        "title": "Recursive problem and Thinking Process",
        "excerpt":"The article contains mistakes and misunderstandings, as it is a record of my own incorrect notes rather than a proper summary document.   First Module 1 - Arrays, ArrayLists, Recursion   Given this recursive method and an input of n = 7, what will the output be? Please enter your answer as a Comma Separated List (e.g 1, 2, 3, 4)  public void recursiveMethod(int n) {       if (n &lt;= 0) {           return;       } else {           System.out.println(n);           recursiveMethod(n - 2);           System.out.println(n - 1);       }   }    Your Answer:7,5,3,1   Correct Answer: 7, 5, 3, 1, 0, 2, 4, 6     The answer relies on a recognition that the output will come before the recursive call, and that the output after all recursive calls are made will output from smallest to largest instead of largest to smallest   Then a CMU web, Towers of Hanoi   In this puzzle, we have three pegs and several disks, initially stacked from largest to smallest on the left peg. (See the 6-disk picture below.) The rules are simple:      Our goal is to move the entire tower to the middle peg.   We can only move one disk at a time.   We can never place a larger disk on a smaller one.                  If JavaScript 1.2 is enabled on your browser, you can try it yourself. Just click on the disk you want to move, and then click on the peg you want to put it at. Although technically you are only allowed to move one disk at a time, the program will move several disks if it is necessary to complete the move.   We’ll answer both of these questions in sequence.   To describe how the monks should solve this puzzle, the concept of recursion will be useful. We look at that next.   Next: Recursion.   Then a simple leetcode Q 104. Maximum Depth of Binary Tree   104. Maximum Depth of Binary Tree eng ver   But most people did like this, correct, but…, not friendly for me at first   class Solution:     def maxDepth(self, root: Optional[TreeNode]) -&gt; int:         return 1 + max(self.maxDepth(root.left), self.maxDepth(root.right))           Then two prob   Reverse Linked List  https://neetcode.io/problems/reverse-a-linked-list Not the normal Iteration  class Solution:     def reverseList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:         if head is None or head.next is None:             return head        \t# weird place         node = self.reverseList(head.next)                  head.next.next = head         head.next = None         return node   And 104. Maximum Depth of Binary Tree again , simple question again I think it is a good way to understand?  # Definition for a binary tree node.  # class TreeNode: #     def __init__(self, val=0, left=None, right=None): #         self.val = val #         self.left = left #         self.right = right   class Solution:     def maxDepth(self, root: Optional[TreeNode]) -&gt; int:          # I'm starting from the root node now          if root is None:  # if there is no root, there is no root node at all, so I should return the value 0              return 0          # OK, the logic now is that there is at least one root node. Now, based on the logic of the recursive function,          # we can know the depth of the left and right nodes starting from the root          # which is          left_depth = self.maxDepth(root.left)          right_depth = self.maxDepth(root.right)          # OK, now, we start to think, we have the depth of the root nodes root.left and root.right, now we want to add          # the depth of the root layer itself, so we need to add 1. At the same time, we want the maximum depth, so we have the max function          return max(left_depth, right_depth) + 1    Right now, the problems are     Reinforce point   Return type, and helper method (create a new helper method)   A simple Leetcode Q,  The thinking flow of recursive Q in BST 617. Merge Two Binary Trees  # Definition for a binary tree node. # class TreeNode: #     def __init__(self, val=0, left=None, right=None): #         self.val = val #         self.left = left #         self.right = right class Solution:     def mergeTrees(self, root1: Optional[TreeNode], root2: Optional[TreeNode]) -&gt; Optional[TreeNode]:         if not root1:             return root2         if not root2:             return root1         def merge(node1,node2):             # if two entering roots are None, direct return one of trees,              # return node2 # ? wired, wanna change [root1s] val, but it return a value,               if node1 is None and node2 is None:                 return                           if node1 and node2:                 node1.val += node2.val                          if not node2:                 return              if not node1.left and node2.left:                 node1.left = TreeNode(0)             if not node1.right and node2.right:                 node1.right = TreeNode(0)             merge(node1.left,node2.left)             merge(node1.right,node2.right)                                   merge(root1,root2)         return root1   # Definition for a binary tree node. # class TreeNode: #     def __init__(self, val=0, left=None, right=None): #         self.val = val #         self.left = left #         self.right = right class Solution:     def mergeTrees(self, root1: Optional[TreeNode], root2: Optional[TreeNode]) -&gt; Optional[TreeNode]:          def merge(root1,root2): # -&gt; treeNode             if not root1:                 return root2             if not root2:                 return root1             # if root1 and root2 # adding value logic                          root1.val += root2.val             root1.left = merge(root1.left,root2.left)             root1.right = merge(root1.right,root2.right)                          return root1         return merge(root1,root2)           Leetcode 450  450. Delete Node in a BST  # Definition for a binary tree node. # class TreeNode: #     def __init__(self, val=0, left=None, right=None): #         self.val = val #         self.left = left #         self.right = right class Solution:     def deleteNode(self, root: Optional[TreeNode], key: int) -&gt; Optional[TreeNode]:          def dfsFinder(node,val): # Unsure of the return type, changing the tree structure, so should the return type be treeNode?          # Or do I only need a single return, because I'm only changing the tree structure?  Confused              # no root , return None             if node is None:                 return None # Confused here. The question says if found, delete it; if not found, return None or the original?                  # I don't know whether to write return or return None, I can't convince myself.                           # if node: ... so right now this place has a node, first one is root             # from root perspective             if node.val == val:                 # using successor, that is the minimum value of the right subtree.                  node = successorHelp(node)                                  # Oh, it can be using if statment here.                 # if node.right and not not node.left ....               elif node.val &gt; val: # node.left                 temp = dfsFinder(node.left,val)                 return temp #?             elif node.val &lt; val:                 return dfsFinder(node.right,val)                  def successorHelp(node):             # When calling itself, the input cannot be None, but I think it's okay to check?  \t\t\t# Wow, it seems that a check is necessary because of the recursive boundary problem, but can we directly check if it's a leaf node? No, what if there's only a left subtree and no right subtree?              if node is None:                 return None # I suddenly don't know what to return.  Returning None feels strange.                                   # It was written as None at the end, but after thinking about it, I don't think it will be called.              # .right once             node = node.right             if node.right:                 while node.left:                     node = node.left                                  # delete that node                 new_value = node.val                 node = None                 return new_value             else:                 return node.left                          # I think the above can be adjusted.  It's about `val` and an impossible `val`. If it's an impossible `val`, it means there's no right subtree starting from the node, so connect to the left subtree starting from the node, that is, `node = node.left`.                                               ","categories": [],
        "tags": [],
        "url": "/recursive-problem/",
        "teaser": null
      },{
        "title": "Thinking and Re-thinking of Learning and Happiness",
        "excerpt":"Using LLMs to refine context through chaotic thinking, but I think GPT-4.5 (2025-04-22) is quite good.   下面的内容是我胡乱打的, 也就是说想到什么写什么说什么, 但是大概follow了我在想一些事情时候的本意.      感觉学习递归的过程, 认为让我理解了学习为什么困难or为什么成绩不好什么的. 一个本质的问题是, 你是否认为一个人可以学会任何东西. 相信大脑的能力,相信神经. 我相信这一点, 不过在学习时间够长的情况下. 为什么在一个阶段学不懂, 我认为是之前其实有东西没学懂.  基础出现问题. (补充) 但是可能硬件不同, or正常理解框架下的智商, 我感觉是大脑全链路激活层的学习速度, 我觉得学习速度应该是真有快慢的, 这是事实. 有些人, 有些快, 在比较心理, 认为自己的不行的暗示下, 距离就逐渐拉开了. 知识没有被分成足够小的chuck.     学习的目的非常重要, 我认为需要正反馈. 不可以骗自己, 可以骗一个月, 不可能骗10年, follow自己的内心, 非常重要. 感觉这也是另外一种版本的价值回归.     回到递归, 做递归, 最忌讳的就是思考递归的细节, 但是学习为什么感觉都是一个逆过程, 追求成绩, 违反本身的追求真理的逻辑, 学习过程的正反馈是, 在相同时间内学的比别人快, 学的比别人多, 而不是增加探究世界现象的手段, 当然痛苦.     我感觉学习, 读书, 好像在不断获得和学习认识这个时间的工具, 就像一个天生近视800度的人, 可以逐渐看清越来越远的东西, 像镜头的景深越来越深, 这个过程当然是快乐的, 而且是逐步快乐的, 这个时候检测 考试, 好像就是解读一些观察时候的corner case. 但是push的手段却是比较时间断内, 谁看到的世界比较大, 谁获得的知识比较多, 奇怪, 不过如果检测集足够大, 确实可以筛选出来那些本身获得正反馈的, 和本身速度就比较快的人…    使用的简单prompt是, 这个是一个混乱的思路文稿,{上面的文本内容},解读,组织成一个结果完整的文章    claude 3.7      递归学习与思维过程的反思     您分享的文稿表达了对学习过程、特别是递归思维的一些深刻反思。我将整理您的想法，并从编程与学习思维的角度进行解构分析。     递归思维的本质与编程应用     在编写代码时，尤其是面对递归问题时，人脑应该关注的核心不是递归的具体执行细节，而是问题的整体结构和解决方案的框架。这与您所说的”最忌讳的就是思考递归的细节”相呼应。     在编程思维过程中，我们应该关注：          问题分解 - 将复杂问题拆解为更小、更易管理的子问题     模式识别 - 发现问题中的重复模式，这是递归的基础     边界条件 - 明确递归的终止条件，防止无限循环     信任抽象 - 相信子问题的解决方案，而不陷入执行细节       编码思维流程     当我们编写递归代码时，思维流程通常是：          首先识别问题是否适合递归解决（是否有重复的子问题结构）     定义基本情况（边界条件）- 递归必须有终止点     设计递归关系 - 如何将大问题分解为同类型的小问题     实现递归逻辑，相信递归会正确处理子问题     验证解决方案，检查边界情况       这个过程与您提到的”学习的逆过程”有相似之处。递归编程要求我们相信函数能够完成其承诺，而不是纠结于每一层的执行细节。     学习过程的启示     您的文稿中提到了几个关于学习的重要观点：          学习基础的重要性 - “在学习时间够长的情况下，为什么在一个阶段学不懂，我认为是之前其实有东西没学懂，基础出现问题”     内在动机与正反馈 - “学习的目的非常重要，我认为需要正反馈”     知识分块 - “知识没有被分成足够小的chuck”       这些观点与有效的编程学习特别相关。编程学习也需要将复杂概念分解为可管理的小块，建立在坚实的基础上，并通过解决问题获得正反馈。     编程与学习的哲学联系     编写代码，尤其是递归代码，可以视为一种认知训练，它培养了以下思维能力：          抽象思维 - 看到问题的本质而非表面细节     系统思考 - 理解部分与整体的关系     信任机制 - 相信系统各部分能够协同工作       这与您描述的学习过程相似：”学习，读书，好像在不断获得和学习认识这个时间的工具，就像一个天生近视800度的人，可以逐渐看清越来越远的东西，像镜头的景深越来越深”。     实践建议     在面对递归等复杂编程概念时：          先理解问题的整体结构，而非实现细节     确定最简单情况的解决方案（基本情况）     设计递归关系，而不是试图在脑中跟踪整个执行过程     编写代码时关注逻辑结构，而非执行顺序     通过测试简单情况验证代码，建立对解决方案的信心       这种方法类似于您所提倡的学习方式：关注整体理解，而非机械记忆；追求内在价值，而非外部评判；将复杂问题分解为可管理的部分。     通过这种方式，编程不仅是一种技能获取，更是一种思维培养，帮助我们”看清越来越远的东西”，扩展我们认知的”景深”。    deepseek - R1      递归思维与学习困境：一场关于认知本质的思辨     在计算机科学的递归算法里，程序员最忌惮陷入函数调用的细节迷宫。这个编程隐喻恰如其分地揭示了人类认知的深层困境——当我们面对知识体系的建构时，常常迷失在表层现象的追逐中，却遗忘了思维递归的本质特征：每个层次的突破都建立在坚实基础之上。     一、知识大厦的地基危机     现代教育体系如同加速运转的传送带，将标准化知识模块源源不断地输送给学习者。在这个过程中，多数人陷入”知识假性完整”的认知陷阱：微积分公式的机械套用掩盖了极限概念的模糊，编程语法的熟练操作遮蔽了算法思维的缺失。这种基础性认知漏洞如同建筑地基的裂缝，在知识体系不断叠加的过程中产生级联效应，最终导致整个认知结构的崩塌。     认知神经科学的最新研究证实，大脑新皮层的神经可塑性并非均匀分布。前额叶皮层与海马体构成的记忆环路，其信息处理效率直接影响知识模块的整合质量。当基础概念网络存在结构缺陷时，高阶认知功能的神经表征将出现拓扑紊乱，这正是”阶段式学习困境”的神经生物学基础。     二、认知时钟的速率谜题     在慕尼黑大学的天才追踪研究中，发现顶尖学者的大脑白质纤维束呈现出独特的拓扑结构。这些增强的神经连接通道，使得他们的信息处理速度比常人快3-5个数量级。这种硬件差异如同CPU的时钟频率，决定了知识编译的基本速率。但神经可塑性研究同样表明，持续的正向刺激能使大脑胼胝体的髓鞘厚度每年增加0.5-1%，这意味着学习速度本身具有可塑性。     教育评估体系制造的比较陷阱，本质上是将马拉松改造成百米冲刺的竞技场。当知识检测简化为碎片信息的提取速度测试，真正的认知深度反而被排除在评价体系之外。这种异化机制催生出大批”解题运动员”，他们熟练操作知识符号却丧失了概念建构能力。     三、认知觉醒的正向递归     真正的学习革命始于元认知的觉醒。当学习者将注意力从外部评价转向内在建构，知识获取就转化为持续的正反馈循环。这个过程遵循着严格的递归逻辑：每个新概念必须完全内化为认知基元，才能支撑更高层级的思维跃迁。MIT媒体实验室的认知增强实验证明，采用这种递归学习法的被试者，其知识留存率是传统学习者的4.7倍。     建立认知增强系统需要重构学习范式：将知识体系分解为原子化概念单元，通过间隔重复形成稳定的神经突触连接；构建概念之间的多维联结，形成抗干扰的认知拓扑网络；最终在问题求解中实现知识的创造性重组。这个过程如同3D打印认知结构，逐层构建却始终保持整体一致性。     站在认知革命的门槛上，我们需要重新理解学习的本质。知识获取不是信息的简单堆砌，而是认知结构的递归升级。当我们将每个知识节点都锻造成坚实的认知基元，就能在思维空间实现真正的维度跃迁。这种递归式认知进化，终将带领我们突破智能边界，见证思维宇宙的奇点时刻。    GPT - 4.5      学习递归的过程让我意识到为什么学习对很多人来说如此困难，也让我进一步理解了成绩表现不佳背后的原因。     首先，我们必须承认一个基本问题：人类是否能够学习任何内容？我个人相信大脑具备这样的能力，相信神经网络能够通过不断训练和刺激逐渐掌握知识。然而，即便学习时间充足，某些阶段的知识却依然难以理解，这背后往往是因为之前的基础知识并没有真正掌握透彻。     然而，这里还存在另一个重要因素：硬件的不同，或者我们常规理解中的智力差异。每个人大脑中神经元连接的效率，即所谓的“全链路激活层”，确实存在差别。这导致了不同人的学习速度存在客观上的差异。这种差异本身是真实存在的事实。有些人学得更快，有些人则相对慢一些。当人们开始将自己与他人比较，并暗示自己能力不足时，这种差距便会逐渐扩大。此外，知识没有被分解成足够小的知识单元（chunks），也进一步加剧了学习困难。     明确学习的目的至关重要。我认为最重要的是持续的正反馈。一个人可能短暂地欺骗自己一段时间，比如一个月，但绝不可能持续地自欺欺人十年之久。因此，诚实地追随内心真正的兴趣与价值观尤为重要。这实际上也是一种价值回归——回归到学习本质的乐趣与探索。     以递归为例，递归最大的忌讳便是陷入细节，却忽略整体思维。可是现实的学习过程却经常与这种理念背道而驰。追求分数和表面成绩的过程中，往往偏离了对真理与知识本身的追求。真正的正反馈应该来源于自己对世界现象的深入探索和理解，而不是在相同时间内比别人学得更多或更快。然而，现行的学习模式与评估机制却经常以知识量和学习速度为衡量标准，这自然带来学习的痛苦与焦虑。     我感觉学习的过程，就像不断地获得观察世界的工具。就好像一个天生视力不足、近视800度的人，通过不断调整镜片逐渐看清更远处的风景，视野变得越来越清晰，景深也逐渐增加。这种逐步清晰的过程本应是快乐且满足的。而考试或者测试，就像是观察世界中的一些特殊案例（corner cases），本质上是一种检验我们观察能力的方式。     但令人奇怪的是，我们往往用一种截然相反的标准去评价——在特定的时间段内，谁观察得更多，掌握的知识更广。这种评价方法尽管表面上合理，但实际上却忽略了真正重要的事情：人们学习过程中自发的正反馈和内在的学习速度。不过如果评估的范围和规模足够大，确实能够从中挑选出那些本身就能获得正反馈、学习速度较快的人。     最终，学习不应只是一场速度与数量的竞争，而更应该是一场逐渐拓宽视野、深入理解世界的旅程。理解并尊重学习的本质，才能真正享受学习的快乐与成长。    2025-04-27   好像也解决了那个所谓买菜为什么要数学的问题, 如果对探知的要求只有百元以内的菜价, 那么好像确实没有持续学习的必要了. 再引申开来一点的话, 如果课本里的知识没有在帮助解决现实中的问题，只提供难和奇怪的过程, 就很容易让人觉得它们抽象且难以消化。而且，我认为一部分学习带来的成就感来源于和他人的比较，一旦身边的同路人越来越少，或者遇到天赋更高的人，这种动力就会逐渐消失. 所以我感觉做事的时候更应该想清楚目的.   一点粗浅的感想罢了.   And我现在感觉读书还挺有趣的.   ","categories": [],
        "tags": [],
        "url": "/learning/",
        "teaser": null
      },{
        "title": "Linear Algebra Starting from Matrix Multiplication",
        "excerpt":"好像写成一个的线代的内容了, 有很多未完待续的内容   我可以算出矩阵乘法的题目，但当涉及到矩阵乘法的交换顺序的意义，或者说矩阵乘法本身的内在含义时，我往往无法在第一时间作出直觉反应。   在做矩阵乘法的时候，我依赖的是一种背诵的技巧, 提醒自己这个东西叫 “行列式”, 所以“前一个矩阵的行, 乘以后一个矩阵的列”。但我并不知道这个操作实际上意味着什么。 经过自己的反思，我发现，我之所以缺乏这种直觉，是因为我对“矩阵乘法”这一概念没有真正理解。在除了得到一个结果外, 矩阵乘法到底在干什么。这个很基本的数学定义 or 数学概念想要解决现实中的什么问题?   这种基于死记硬背的方式，让我在进一步思考矩阵相关的概念时(如旋转、投影、缩放等操作, 或者是去括号, 转置, 求可逆矩阵的一些变式)，总是感觉模糊和经常出错。因为从最底层开始，我对矩阵的理解就是模糊的，甚至可以说是错的，导致我没法基于它构建准确的思维。   我想解决这个问题。   线性变换的复合 (Composition of Linear Transformations)   出现问题的一个comment的场景  下面是一个 $5 \\times 5$ 的矩阵相乘的例子, 这是一个很好的例子就是,  \\(\\begin{bmatrix} A_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} &amp; a_{15} \\\\ A_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} &amp; a_{25} \\\\ A_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} &amp; a_{35} \\\\ A_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44} &amp; a_{45} \\\\ A_{51} &amp; a_{52} &amp; a_{53} &amp; a_{54} &amp; a_{55} \\\\ \\end{bmatrix} \\times \\begin{bmatrix} B_{11} &amp; b_{12} &amp; b_{13} &amp; b_{14} &amp; b_{15} \\\\ B_{21} &amp; b_{22} &amp; b_{23} &amp; b_{24} &amp; b_{25} \\\\ B_{31} &amp; b_{32} &amp; b_{33} &amp; b_{34} &amp; b_{35} \\\\ B_{41} &amp; b_{42} &amp; b_{43} &amp; b_{44} &amp; b_{45} \\\\ B_{51} &amp; b_{52} &amp; b_{53} &amp; b_{54} &amp; b_{55} \\\\ \\end{bmatrix} = \\begin{bmatrix} C_{11} &amp; c_{12} &amp; c_{13} &amp; c_{14} &amp; c_{15} \\\\ C_{21} &amp; c_{22} &amp; c_{23} &amp; c_{24} &amp; c_{25} \\\\ C_{31} &amp; c_{32} &amp; c_{33} &amp; c_{34} &amp; c_{35} \\\\ C_{41} &amp; c_{42} &amp; c_{43} &amp; c_{44} &amp; c_{45} \\\\ C_{51} &amp; c_{52} &amp; c_{53} &amp; c_{54} &amp; c_{55} \\\\ \\end{bmatrix}\\)   疑惑的点是, 如果认为任意一个matrix都是列向量的话, 那么其实在这个显而易见的式子中, $A   x = b$ 中, 一个认知是$A$  可以被认为是一个大的系统(如果是 n by m的matrix), $x$作为一个vector(拆分成竖直方向上的单列) (woc这里应该是 1 乘 多少的单列vector我不可以一秒判断出来), 由于疑惑, 我回到传统想法上的记住行列式的乘法的方法, 会发现, 这个结果也是一列的其实, 但是我对于怎么乘是模糊的, OK再想 行乘以列这个口诀, 也就是说, vector的第一个, 乘了A的第一个行, 这是也是第一个b上的结果, 那么这个第一个位置上的结果到底是什么东西呢, 不是最后要写的结果, 而是搞不明白代表什么东西   {3Blue1Brown} Linear transformations and matrices | Chapter 3, Essence of linear algebra   回到一个简单的问题, 下面是一个在二维平面中的vector, \\(\\vec{v} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\\) 如何看待这个对象呢, 可以认为是, 使用两个单位矩阵构造的, which is  $\\hat{j} = \\begin{bmatrix} 0\\ 1 \\end{bmatrix}$ 和$\\hat{i}= \\begin{bmatrix} 1\\ 0 \\end{bmatrix}$, 这里可以注意到, $\\hat{j}$ 在 $y$ 轴上, 而 $\\hat{i}$ 在 $x$ 轴上, 那么我们的 $\\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$ 是如何得到的呢?  使用线性变换(注:要补充)的概念非常简单, 我们可以得到 \\(\\vec{v} = (-1) \\cdot \\hat{i} + 2 \\cdot \\hat{j} = (-1) \\cdot \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} + 2 \\cdot \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}\\)   OK, 再进一步的视角是,  \\(\\)   我原来以为, $\\begin{bmatrix} -1 \\ 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 &amp; 0 \\0 &amp;1 \\end{bmatrix}$ 是可以得到 $\\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$ , 但是现在发现我错了(而且这也是非法的矩阵相乘), 应该是$\\begin{bmatrix} 1 &amp; 0 \\0 &amp;1 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$因为和下面的逻辑混淆了, 3Blue1Brown说, 对于(这里我随机写了一个例子) $\\begin{bmatrix} -1 &amp; 4 \\ 1 &amp; 2 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$ 这里其实是让新的基底变成 $\\begin{bmatrix} -1 &amp; 4 \\ 1 &amp; 2 \\end{bmatrix}$ 而不再是原来的 $\\begin{bmatrix} 1 &amp; 0 \\0 &amp;1 \\end{bmatrix}$ 我在想是不是原来对于 $\\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$ 是不是后面都藏着一个 $\\begin{bmatrix} 1 &amp; 0 \\0 &amp;1 \\end{bmatrix}$ 代表着原来的基底?   哦哦, 不是的, 应该说一个结果的基座向量会写在结果的前面, $I \\cdot \\vec{v} = \\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\ 2 \\end{bmatrix} = \\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$, 这样反而是恰当的, 合乎逻辑的, 但是在前面乘一个单位矩阵太傻了, 但是可以发现的是, 在前面乘就是提供 一个要变换的指令, 只不过单位矩阵恰好是没有任何变化要求的那个. 而 $\\begin{bmatrix} -1 &amp; 4 \\ 1 &amp; 2 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$ 可以被认为是$\\begin{bmatrix} -1 &amp; 4 \\ 1 &amp; 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$   这个表述错了 $\\rightarrow$ 其实在$\\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$的后面乘是逆变化, 不过这是后话了. 变化可以认为就是在前面乘, 在前面乘就是在做各种线性操作, 比如转回去就是在前面的位置乘一个逆就行了   但是我好像还是没搞懂为什么能变成这样, $(-1) \\cdot \\begin{bmatrix} -1 \\ 1\\end{bmatrix} \\cdot + 2 \\cdot  \\begin{bmatrix} 4 \\ 2\\end{bmatrix}$  3Blue1Brown的视频中, 是这样写的, $\\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$  被我们看成了单独放置的点 or 一个向量的箭头指向这个点的位置, 这样的意义也可以这样表示$\\vec{v} = -1 \\cdot \\vec{i}_1 + 2 \\cdot \\vec{j}_2$,  这个点的表示分, 我突然看到感觉好陌生, 之前好像没有只是顺着写了一下, 自己不知道为什么可以这样写. OK, 如果可以这样写的话, 就相对清晰了, 就是, 直接换了一套基座, 对于这个式子而言, $\\begin{bmatrix} -1 &amp; 4 \\ 1 &amp; 2 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$ 不是吗,   再描述一个更加抽象的例子   \\[A \\cdot B = C\\]  这里的$A$ 和 $B$ 都是 $4 \\times 4$ 的matrix   从列的角度看这个问题后, 好吧, 突然我也不知道这里从列的角度看是什么意思, 我姑且认为在做的开始, $A$ 和 $B$ 都被认为是一列一列呈现的   $A$就被我们看成从$$   A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \\ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} \\ a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} \\ a_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44} \\end{bmatrix} \\(​​​​ 列向量视角的 $A$：\\) A = \\left[ \\vec{a}_1 \\quad \\vec{a}_2 \\quad \\vec{a}_3 \\quad \\vec{a}_4 \\right] $$   \\[\\vec{a}_1 = \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{bmatrix}, \\quad \\vec{a}_2 = \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ a_{42} \\end{bmatrix}, \\quad \\vec{a}_3 = \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ a_{43} \\end{bmatrix}, \\quad \\vec{a}_4 = \\begin{bmatrix} a_{14} \\\\ a_{24} \\\\ a_{34} \\\\ a_{44} \\end{bmatrix}\\]  OK, 我们得到 $A = \\left[ \\vec{a}_1 \\quad \\vec{a}_2 \\quad \\vec{a}_3 \\quad \\vec{a}_4 \\right]$ 后开始处理 $B$ , 同样列向量视角 ​​​​  从\\(B = \\begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} &amp; b_{14} \\\\ b_{21} &amp; b_{22} &amp; b_{23} &amp; b_{24} \\\\ b_{31} &amp; b_{32} &amp; b_{33} &amp; b_{34} \\\\ b_{41} &amp; b_{42} &amp; b_{43} &amp; b_{44} \\end{bmatrix}\\) ​到 \\(B = \\left[ \\vec{b}_1 \\quad \\vec{b}_2 \\quad \\vec{b}_3 \\quad \\vec{b}_4 \\right]\\)   \\[\\vec{b}_1 = \\begin{bmatrix} b_{11} \\\\ b_{21} \\\\ b_{31} \\\\ b_{41} \\end{bmatrix}, \\quad \\vec{b}_2 = \\begin{bmatrix} b_{12} \\\\ b_{22} \\\\ b_{32} \\\\ b_{42} \\end{bmatrix}, \\quad  \\vec{b}_3 = \\begin{bmatrix} b_{13} \\\\ b_{23} \\\\ b_{33} \\\\ b_{43} \\end{bmatrix}, \\quad  \\vec{b}_4 = \\begin{bmatrix} b_{14} \\\\ b_{24} \\\\ b_{34} \\\\ b_{44} \\end{bmatrix}, \\quad\\]  写完后, 我们的 $B$ 作为之前的数值位置, $A$中的向量都是作为新的坐标系来构建, 那么我们可以得到   3. “列中列”展开法的具体表示   矩阵乘积 $C = A \\cdot B$ 的完整展开形式：   \\[C = \\left[ \\begin{array}{cccc} b_{11} \\cdot \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{bmatrix} + b_{21} \\cdot \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ a_{42} \\end{bmatrix} + b_{31} \\cdot \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ a_{43} \\end{bmatrix} + b_{41} \\cdot \\begin{bmatrix} a_{14} \\\\ a_{24} \\\\ a_{34} \\\\ a_{44} \\end{bmatrix} &amp; b_{12} \\cdot \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{bmatrix} + b_{22} \\cdot \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ a_{42} \\end{bmatrix} + b_{32} \\cdot \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ a_{43} \\end{bmatrix} + b_{42} \\cdot \\begin{bmatrix} a_{14} \\\\ a_{24} \\\\ a_{34} \\\\ a_{44} \\end{bmatrix} &amp; b_{13} \\cdot \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{bmatrix} + b_{23} \\cdot \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ a_{42} \\end{bmatrix} + b_{33} \\cdot \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ a_{43} \\end{bmatrix} + b_{43} \\cdot \\begin{bmatrix} a_{14} \\\\ a_{24} \\\\ a_{34} \\\\ a_{44} \\end{bmatrix} &amp; b_{14} \\cdot \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{bmatrix} + b_{24} \\cdot \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ a_{42} \\end{bmatrix} + b_{34} \\cdot \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ a_{43} \\end{bmatrix} + b_{44} \\cdot \\begin{bmatrix} a_{14} \\\\ a_{24} \\\\ a_{34} \\\\ a_{44} \\end{bmatrix} \\end{array} \\right]\\]  $\\rightarrow$   \\(\\left[ \\begin{array}{cccc} \\begin{bmatrix} b_{11} a_{11} \\\\ b_{11} a_{21} \\\\ b_{11} a_{31} \\\\ b_{11} a_{41} \\end{bmatrix} + \\begin{bmatrix} b_{21} a_{12} \\\\ b_{21} a_{22} \\\\ b_{21} a_{32} \\\\ b_{21} a_{42} \\end{bmatrix} + \\begin{bmatrix} b_{31} a_{13} \\\\ b_{31} a_{23} \\\\ b_{31} a_{33} \\\\ b_{31} a_{43} \\end{bmatrix} + \\begin{bmatrix} b_{41} a_{14} \\\\ b_{41} a_{24} \\\\ b_{41} a_{34} \\\\ b_{41} a_{44} \\end{bmatrix} &amp; \\begin{bmatrix} b_{12} a_{11} \\\\ b_{12} a_{21} \\\\ b_{12} a_{31} \\\\ b_{12} a_{41} \\end{bmatrix} + \\begin{bmatrix} b_{22} a_{12} \\\\ b_{22} a_{22} \\\\ b_{22} a_{32} \\\\ b_{22} a_{42} \\end{bmatrix} + \\begin{bmatrix} b_{32} a_{13} \\\\ b_{32} a_{23} \\\\ b_{32} a_{33} \\\\ b_{32} a_{43} \\end{bmatrix} + \\begin{bmatrix} b_{42} a_{14} \\\\ b_{42} a_{24} \\\\ b_{42} a_{34} \\\\ b_{42} a_{44} \\end{bmatrix} &amp; \\begin{bmatrix} b_{13} a_{11} \\\\ b_{13} a_{21} \\\\ b_{13} a_{31} \\\\ b_{13} a_{41} \\end{bmatrix} + \\begin{bmatrix} b_{23} a_{12} \\\\ b_{23} a_{22} \\\\ b_{23} a_{32} \\\\ b_{23} a_{42} \\end{bmatrix} + \\begin{bmatrix} b_{33} a_{13} \\\\ b_{33} a_{23} \\\\ b_{33} a_{33} \\\\ b_{33} a_{43} \\end{bmatrix} + \\begin{bmatrix} b_{43} a_{14} \\\\ b_{43} a_{24} \\\\ b_{43} a_{34} \\\\ b_{43} a_{44} \\end{bmatrix} &amp; \\begin{bmatrix} b_{14} a_{11} \\\\ b_{14} a_{21} \\\\ b_{14} a_{31} \\\\ b_{14} a_{41} \\end{bmatrix} + \\begin{bmatrix} b_{24} a_{12} \\\\ b_{24} a_{22} \\\\ b_{24} a_{32} \\\\ b_{24} a_{42} \\end{bmatrix} + \\begin{bmatrix} b_{34} a_{13} \\\\ b_{34} a_{23} \\\\ b_{34} a_{33} \\\\ b_{34} a_{43} \\end{bmatrix} + \\begin{bmatrix} b_{44} a_{14} \\\\ b_{44} a_{24} \\\\ b_{44} a_{34} \\\\ b_{44} a_{44} \\end{bmatrix} \\end{array} \\right]\\)  $\\rightarrow$   其中我们拿出一条来 \\(b_{11} \\cdot \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{bmatrix} + b_{21} \\cdot \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ a_{32} \\\\ a_{42} \\end{bmatrix} + b_{31} \\cdot \\begin{bmatrix} a_{13} \\\\ a_{23} \\\\ a_{33} \\\\ a_{43} \\end{bmatrix} + b_{41} \\cdot \\begin{bmatrix} a_{14} \\\\ a_{24} \\\\ a_{34} \\\\ a_{44} \\end{bmatrix}\\) 是这样的,  \\(\\begin{bmatrix} b_{11}a_{11} \\\\ b_{11}a_{21} \\\\ b_{11}a_{31} \\\\ b_{11}a_{41} \\end{bmatrix} + \\begin{bmatrix} b_{21}a_{12} \\\\ b_{21}a_{22} \\\\ b_{21}a_{32} \\\\ b_{21}a_{42} \\end{bmatrix} + \\begin{bmatrix} b_{31} a_{13} \\\\ b_{31}a_{23} \\\\ b_{31} a_{33} \\\\ b_{31} a_{43} \\end{bmatrix} +\\begin{bmatrix} b_{41}a_{14} \\\\ b_{41}a_{24} \\\\ b_{41}a_{34} \\\\ b_{41}a_{44} \\end{bmatrix}\\)   再线性相加 \\(= \\begin{bmatrix} b_{11}a_{11} + b_{21}a_{12} + b_{31}a_{13} + b_{41}a_{14} \\\\ b_{11}a_{21} + b_{21}a_{22} + b_{31}a_{23} + b_{41}a_{24} \\\\ b_{11}a_{31} + b_{21}a_{32} + b_{31}a_{33} + b_{41}a_{34} \\\\ b_{11}a_{41} + b_{21}a_{42} + b_{31}a_{43} + b_{41}a_{44} \\end{bmatrix}\\)   这是一个$4\\times 1$的矩阵, 然后我们有4条, 最后是 一个$4\\times 4$ 的结果   \\[\\begin{bmatrix} a_{11} b_{11} + a_{12} b_{21} + a_{13} b_{31} + a_{14} b_{41} &amp; a_{11} b_{12} + a_{12} b_{22} + a_{13} b_{32} + a_{14} b_{42} &amp; a_{11} b_{13} + a_{12} b_{23} + a_{13} b_{33} + a_{14} b_{43} &amp; a_{11} b_{14} + a_{12} b_{24} + a_{13} b_{34} + a_{14} b_{44} \\\\ a_{21} b_{11} + a_{22} b_{21} + a_{23} b_{31} + a_{24} b_{41} &amp; a_{21} b_{12} + a_{22} b_{22} + a_{23} b_{32} + a_{24} b_{42} &amp; a_{21} b_{13} + a_{22} b_{23} + a_{23} b_{33} + a_{24} b_{43} &amp; a_{21} b_{14} + a_{22} b_{24} + a_{23} b_{34} + a_{24} b_{44} \\\\ a_{31} b_{11} + a_{32} b_{21} + a_{33} b_{31} + a_{34} b_{41} &amp; a_{31} b_{12} + a_{32} b_{22} + a_{33} b_{32} + a_{34} b_{42} &amp; a_{31} b_{13} + a_{32} b_{23} + a_{33} b_{33} + a_{34} b_{43} &amp; a_{31} b_{14} + a_{32} b_{24} + a_{33} b_{34} + a_{34} b_{44} \\\\ a_{41} b_{11} + a_{42} b_{21} + a_{43} b_{31} + a_{44} b_{41} &amp; a_{41} b_{12} + a_{42} b_{22} + a_{43} b_{32} + a_{44} b_{42} &amp; a_{41} b_{13} + a_{42} b_{23} + a_{43} b_{33} + a_{44} b_{43} &amp; a_{41} b_{14} + a_{42} b_{24} + a_{43} b_{34} + a_{44} b_{44} \\end{bmatrix}\\]  这时候有一些经典的问题, 就是从这个转动的性质出发了           矩阵相乘是否满足交换率? 当然不行, 都完全是两个东西了            矩阵相乘成立的条件是前一个matrix的什么要等于后一个matrix的什么? 完全不用急, 想着 $Ax = b$ 在这个时候,  $A$ 是我们的旋转/变换系统, 想象每一列都是独立的, rank是满秩的(其实不用只要操作的时候有那个维度就OK了), 那么在这个情况下, 我们的 $x$ 中的 $[x_1 \\dots x_n]$ 当然是要和前面的 $A$ 有多少列要匹配了            这里理解错了, 说明有知识点没懂     $Ax = b$ 中的 b是 几乘几点结果, 感觉x的维度是怎样, b就长怎样? 如果从 $A$ 是提供旋转功能的角度出发的话, 为什么会因为旋转 b相比原先旋转的对象 x 增加维度呢?  这其实是更广义的「线性变换」，不仅仅是旋转, 不一定是严格的旋转，它可以是 旋转 + 拉伸(放大缩小) 投影 (把高维变低维) 嵌入 (把低维映射到高维) 剪切(shear)       如果说 3 by 2的matrix, 如何0.5s内说出 2 是2行还是2列? 之前我是使用行列式这样的背住然后套的, 但是这里提供一种更加直觉的想法, 是, 我们定义这个名字的时候, 更加关注输出, 什么是输出, 变换后矩阵的维度, 那么变换后矩阵的维度是什么, 是由什么确定的, 这样想下去就是, 当然是  $A$ 这个提供变换的方案的matrix确定, 然后再想象一下, 这里的A的维度是什么, 是A的行数(row), 因为我们考虑的是A每一列vector的维度, 这样我们这个变化系统的row就确定了, 所以3 by 2的matrix的话, 3 $\\rightarrow$  row, 所以是3 行 2 列的矩阵.   未完待续     矩阵乘法的剩下三种理解方法   变化的特色case, 为什么有些乘上有些矩阵的时候, 在一个维度上没有变动   第二个问题在看3Blue1Brown发现他希望不动的x维度上, 对应的 A的位置, 好像都是单位矩阵的part, 虽然不知道再哪个位置(哪一层)放1   之后要详细讨论的点是, 在3这个视频中, 所谓的 单个 $A$ 矩阵如果是变换作用的话, 居然直接是单位矩阵想要最后变成的固定状态(停在那个位置).   左乘看成新坐标系的基向量，右边的原向量看成标量     标准基底下的矢量                                                                                                                                                                                                                                  î (1,0)          ĵ (0,1)          v (1,2)                                                                变换基底后的矢量                                                                                                                                                                                                                             î' (1,0)          ĵ' (1,1)          v' (3,2)                                                                   还有细节是对于提供变化的matrix $\\begin{bmatrix} -1 &amp; 4 \\ 1 &amp; 2 \\end{bmatrix}$ , 我们可以直接认为其中$\\begin{bmatrix} -1  \\ 1  \\end{bmatrix}$ 就是 unit vector $i$ 的新位置, 而$\\begin{bmatrix} 4 \\2 \\end{bmatrix}$ 就是 unit vector $j$ 的新位置   #  Chapter 4, Essence of linear algebra, Chapter 5 说了一下三维, 没讲什么  章节中对于$x$ 多维过程计算一个batch的过程, 其实就是拆开一步步的过程   $$ \\begin{align*} &amp;\\underset{M_2}{\\begin{bmatrix} \\textcolor{purple}0 &amp; \\textcolor{purple}2 \\ \\textcolor{purple}1 &amp; \\textcolor{purple}0 \\end{bmatrix}}  \\cdot  \\underset{M_1}{\\begin{bmatrix}  \\color{green}{1} &amp; \\color{orange}{-2} \\  \\color{green}{1} &amp; \\color{orange}{0}  \\end{bmatrix}}  =  \\begin{bmatrix}  ? &amp; ? \\  ? &amp; ?  \\end{bmatrix}  \\[10pt]   &amp;\\begin{bmatrix}  \\textcolor{purple}0 &amp; \\textcolor{purple}2 \\  \\textcolor{purple}1 &amp; \\textcolor{purple}0  \\end{bmatrix}  \\cdot  \\begin{bmatrix}  \\color{green}{1}\\  \\color{green}{1} \\end{bmatrix}  = \\textcolor{green}{1} \\cdot \\begin{bmatrix}  \\textcolor{purple}0 \\  \\textcolor{purple}1 \\end{bmatrix}     \\textcolor{green}{1} \\cdot  \\begin{bmatrix}  \\textcolor{purple}2 \\  \\textcolor{purple}0  \\end{bmatrix}  =  \\begin{bmatrix}  2 \\  1  \\end{bmatrix} \\end{align} \\(我们得出 $\\rightarrow$\\) \\begin{align} &amp;\\underset{M_2}{\\begin{bmatrix} \\textcolor{purple}0 &amp; \\textcolor{purple}2 \\ \\textcolor{purple}1 &amp; \\textcolor{purple}0 \\end{bmatrix}}  \\cdot  \\underset{M_1}{\\begin{bmatrix}  \\color{green}{1} &amp; \\color{orange}{-2} \\  \\color{green}{1} &amp; \\color{orange}{0}  \\end{bmatrix}}  =  \\begin{bmatrix}  2 &amp; ? \\  1 &amp; ?  \\end{bmatrix}  \\[10pt] \\end{align*}   $$ 这里其实有一个小疑惑, 就是$ABCDEF \\cdot x$ 中, 第一个$F$, 对于 $x$ 的坐标映射, 是不是就是遵循$F$ 中的样子来就可以了, 但是对于剩下的步骤, 因为大概率不orthogonal, 所以其实多维的都有影响?   1. 矩阵乘法的剩下三种理解方法   图设计来自gwave 知乎答主, 使用svg重新绘制, 突出版权, 在重制中仍然保留水印.   矩阵-向量乘法-列视角：矩阵右乘列向量，向量对矩阵的列进行线性组合                                                  ×            a      b      c         =                        a         +                        b         +                        c         =                        知乎 @waywa  矩阵-向量乘法-行视角：矩阵左乘行向量，向量对矩阵的行进行线性组合           a      b      c         ×                                                   =               a                  +            b                  +            c                  =                        知乎 @waywa  还没理解完, 未完待续   RREF(Reduced Row-Echelon Form)   感觉是一个很小的概念不在ML中常用到,  意思是“是不是高斯消元法最后让每个基向量尽可能简单”，那可以说是的。每一行代表一个单位基方向，其他方向全0, 可以理解成“最小单位向量”，但其实数学上是标准基, 但是注意，它们不一定是整个空间的 unit vector，而是 在矩阵的列空间或行空间中形成了一个基底。   彼此线性无关，数量就是行空间的维度，也就是矩阵的秩。 列空间的秩=行空间的秩, 这个概念好像是有点背的, 还有下面这个公式 \\(rank(A)=rank(A^\\top)\\) 秩，GPT说其实代表这个系统“真正有多复杂”，而不是表面的行和列数量。   Reference   矩阵乘法核心思想（2）：行空间 - gwave 知乎 https://zhuanlan.zhihu.com/p/348551903 Essence of linear algebra 3Blue1Brown  https://youtu.be/fNk_zzaMoSs?si=UlVZ_LHcoih4kQvu NTU 线性代数 Hung-yi Lee (李宏毅) https://googly-mingto.github.io/LA_2022_fall/2022-fall.html MIT18.06 Linear-Algebra  https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/  ","categories": [],
        "tags": [],
        "url": "/%E4%BB%8EMatrix-%E4%B9%98%E6%B3%95%E8%AE%A1%E7%AE%97%E5%87%BA%E5%8F%91%E7%9A%84Linear-Algebra/",
        "teaser": null
      },{
        "title": "2025-05-01 Dot products点积 and duality Chapter 9/16",
        "excerpt":"3Blue1Brown   后记前置: 还挺沮丧的, 这个15min的视频, 我学了快三个小时. 从sc到lib的电脑屏幕前坐下, 已经3小时了 不过看评论区说也有物理phd常看常新, 还有花了几小时才理解的comments, 给我安慰就是说我虽然笨还不是那么笨hh, 然后虽然要逃出和他人比较的逻辑, 但是我好像还是在这个体系中啊, 哎… 我觉得难的原因是(claude refine)     教学视频受到时长限制，3Blue1Brown为了保持视频简洁，往往需要压缩内容，这就导致某些关键解释可能被简化或省略, 或者是被快速的讲解。   作为斯坦福数学博士，Grant对数学概念的理解已经达到了非常深入的层次。这种专业洞察力有时会让他无意识的忽略初学者的思维盲区，使他在解释时跳过了对新手来说可能至关重要的中间步骤。   一个视频需要面向不同水平的受众，从初学者到专业人士，这使得内容很难同时满足所有人的需求。   2025-05-02记: 又花了差不多4小时, 不过一半的时间在svg gpt作图上, 这个时候我想搭一个agent工具, or找找有没有现成的好的工具, 我觉得现在有的一个问题是现在LLM识别图像的能力非常非常弱, 我的截图对gpt输入, 让他使用svg来复刻, 基本无法识别, OK, 在打字的时候问题来了, 我输入一个png的截图, GPT是怎么读取的我的图片的, 这么短的时间内读取我的每一个像素点嘛, 不太可能, 应该还是Transformer 或者vit之类的, 我觉得我输入prompt如果是要求OCR识别的话, 好像都还蛮准的, 这里如果做agent的话, 要从原始目的出发研究一下. 或者说增强图片, 改变图片的分割方法. 为了减小文件体积, 和冗余, 文章尽量不使用任何png截图, 使整体md文档更加干净.   哎好失落啊, 这点东西配学这么久吗   概念回顾 什么是线性变换linear transformation, 核心的两个形式性定义（formal properties），也叫线性性公理：  ① 加法保持性（preservation of addition）：   \\[L(\\vec{v} + \\vec{w}) = L(\\vec{v}) + L(\\vec{w})\\]  ② 数乘保持性（preservation of scalar multiplication）：   \\[L(c \\vec{v}) = c \\cdot L(\\vec{v})\\]  第一个有疑问的地方对于对称性, 我没有理解3Blue1Brown这里对于对称性的困惑   经过想之后的理解是   在一开始简单说了一下点乘的概念, 但是在2:17的时候, 说不对称, 没有理解这里不对称的意思, 其实是, 我们知道点乘是可以交换的, 但是这里交换后, 我们的几何意义改变了, 但是对于2维的vector点乘, 我们对于同样的对象(两个都是vector), 我们对于这个变化下, 其实两个vector的地位是不一样的, 一个是变化方, 一个是作为ground, 理论上我们交换位置, 这个formula应该会变好, 这里在几何意义上不同的object在交换后, 没有发生改变, 这里神奇的对称了. 顺序好像不重要了   GPT回复below      📌 “这里说的不对称是说，两个向量，但是操作好像是不同的地位。”    没错，作者用 “weirdly asymmetric（奇怪地不对称）” 来形容的，就是这种 心理上的不对等感，虽然实际上点积是数学上对称的操作。我们来把这个“不对称感”彻底拆清楚。 数学上：点积是对称的, 公式永远成立：   \\(\\vec{v} \\cdot \\vec{w} = \\vec{w} \\cdot \\vec{v}\\) 这说明：     两个向量在数学上的地位是完全一样的；   交换它们顺序，计算结果不会变。      但是几何解释上会让人感觉不一样    让我们回顾那个几何解释的套路： 把 $\\vec{w}$ 投影到 $\\vec{v}$ 所在的直线上，然后乘上 $|\\vec{v}|$。这个解释其实默认了两件事：$\\vec{v}$ 是被投影的参考方向，而 $\\vec{w}$ 是主动参与投影的对象。它把两者的角色区分成了不同的几何意义：$\\vec{v}$ 作为投影的目标、坐标轴和参考方向，而 $\\vec{w}$ 则是被投影的向量和参与者。 所以你心里可能会产生这个疑问：为什么 $\\vec{v}$ 是坐标轴一样的东西，而 $\\vec{w}$ 是被压上去的？ 如果你说：”那我反过来，把 $\\vec{v}$ 投影到 $\\vec{w}$ 上，不也一样吗？”当然可以，而且你最终也会得到：$\\vec{w} \\cdot \\vec{v} = \\vec{v} \\cdot \\vec{w}$。但你会发现，这时候你投影的路径、过程完全不一样，虽然数值结果相等。就像你走了另一条路，但是到了同一个终点。 为什么这种”不对称”让人不舒服？因为我们习惯上会以为：如果两边平等，那解释方式也应该是平等的。但这里几何上的投影，总要选择一个方向来作为参考（轴），另一个投过去。这就导致你在心里赋予了一个向量”更高的地位”，这跟数学上的”完全对称”感觉不一致。这就是作者说的 “weirdly asymmetric” 的来源。   下面是看到一半时候, 我的疑惑(后来解决了)  后续在视频里面说, 如果 $\\vec{v}$ 和 $\\vec{w}$ 是本身对称的话, 这个投影的长度, 无论谁对谁, 好像值是不好变的, 然后如果改变一点的话, 比如,  $2\\times \\vec{v} \\times \\vec{w}$ 如果按照原来的理解的话,  $\\vec{v}$ 变成了原来的两倍, 这个时候$\\vec{w}$ 去逼近 $2\\vec{v}$这个vector的话, 感觉应该值是不会变的, 因为对于ground vector来说, 肯定不可以影响阴影啊   gpt回答… 使用Claude refine   上面的问题切中了 点积几何解释中投影与向量长度的关系 的核心。   你提出的疑问总结成一句话：”我感觉点积是 ‘影子长度’，那如果我把地面（ground vector） $\\vec{v}$ 变长，比如变成 $2\\vec{v}$，那影子的长度不应该变啊，因为地面变长不影响投影到它的 ‘影子长度’ —— 为什么点积的值却变了呢？”   这其实是一个常见直觉误区，也是这个视频中解释得最细致的地方。我们来用物理类比和直觉来破解这个问题：   先把点积的几何解释再说清楚：点积 = 投影长度 × 被投影方向的长度。也就是说：   \\[\\vec{v} \\cdot \\vec{w} = \\left|\\vec{v}\\right| \\cdot \\text{(投影长度 of } \\vec{w} \\text{ onto } \\vec{v})\\]  如果我们换个方式写：   \\[\\vec{v} \\cdot \\vec{w} = \\left|\\vec{v}\\right| \\cdot \\left|\\vec{w}\\right| \\cdot \\cos\\theta\\]  这里的意思是：你把 $\\vec{w}$ 投影到 $\\vec{v}$ 上（或反过来）；然后乘上 $\\vec{v}$ 的长度（或 $\\vec{w}$ 的长度）；得到一个带符号的投影值。     类比一：投影 ≠ 影子   你在想：”如果我只是测影子长度，地板变长了（$\\vec{v}$ 变成 $2\\vec{v}$），影子不该变啊！” —— 对！如果你只是测 “影子长度”，确实不变。但问题是：点积不是 “影子长度”，而是 “影子长度 × 地板的长度”。   你可以把它类比成：力 × 位移 = 做的功（Work）。力 $\\vec{F}$ 施加在一个方向；位移 $\\vec{d}$ 在另一个方向：   \\[\\vec{F} \\cdot \\vec{d} = \\left|\\vec{F}\\right| \\cdot \\left|\\vec{d}\\right| \\cdot \\cos\\theta\\]  如果你把 $\\vec{F}$ 加大一倍，虽然 $\\vec{d}$ 没有变，角度没有变，做功变成了两倍。因为你推得更用力了。     回到你的问题：      原来：$\\vec{v} \\cdot \\vec{w} = L$   改成：$2\\vec{v} \\cdot \\vec{w} = ?$   发生了什么？投影长度（$\\vec{w}$ onto $\\vec{v}$）不变；但 $\\vec{v}$ 的长度变成了两倍，所以最终点积值也变成原来的两倍：   \\[2\\vec{v} \\cdot \\vec{w} = 2 \\cdot (\\vec{v} \\cdot \\vec{w}) = 2L\\]  这并不奇怪，因为你换了参考线：你不是只测 “影子的长度”，而是 “影子长度 × 地板的长度”，自然会变大！     类比二：放大尺子   想象你用尺子测一个影子长度。这个 “尺子” 就是 $\\vec{v}$：      原来：尺子是 1 米；你投影 $\\vec{w}$，影子是 0.7 米；点积 = 0.7   你把尺子放大两倍：尺子变成 2 米；同样的影子 0.7 “尺”，但现在 1 尺 = 2 米 → 点积 = 1.4   因为你改变了参考系！点积确实是关于 “投影” 的操作，但它还乘了一个 “参考向量的长度”，所以当你改变参考（地板）的时候，值当然会变。   不能把点积误认为是 “单纯的影子长度” —— 它其实是 “影子 × 标尺的大小”。   继续   视频中的另外一个疑问(是作者本身的提问), 我之前没有注意到这个问题, 为什么下面这个形式, 乘以对并将它们相加的数值过程与投影有关？ \\(\\begin{bmatrix} 2 \\\\ 7 \\\\ 1 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} 8 \\\\ 2 \\\\ 8 \\\\ \\end{bmatrix} = 2 \\cdot 8 + 7 \\cdot 2 + 1 \\cdot 8 = 16 + 14+8= 38\\) 于是, 在这里, 引出了duality   OK, 还要讨论 linear transformations from multiple dimensions to one dimension：   \\(\\vec{v} = \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix} \\xrightarrow{\\quad L(\\vec{v}) \\quad} \\begin{bmatrix} 1.8 \\end{bmatrix}\\) 在这里, 我们的  $\\begin{bmatrix}2 \\ 7\\end{bmatrix}$ 从二维变成了一个数(which is 一维的),  就是  $[1 \\quad  3]$ 这种样子, 可以认为是在一维上的两个值, 从原点出发, 指到了1和指到了3, 这样.          0    -1   -2   -3   -4   -5    1   2   3   4   5      v₁     v₂  所以, 我们几乎可以认为, 1和3是纯数值, 然后这个躺下来的 vector其实 T 一下, 就是一个正常的vector, 这个非常common的操作, 其实建立起了数值 和 vector(2维/3维下的坐标点)之间的关系,   就是说, 如果我们忘记对于 dot product 公式机械的记忆, 从几何意义开始探讨的话, 那个$[x_1 \\quad  x_2 \\dots x_n]$ (个数未定义) 的横向的object, 可以被认为是一个 $f(x)$ 把我们可观察的vector们(当然也包括不可观察的), 从2维3维空间, 拉到一个一维的穿过原点的坐标轴上(这里  $x_1\\dots x_n$在上面, 就是实际值),这个过程or变化是线性的, 注意这个 $f(x)$ 的output是 1个值.   一开始对10:04对称的那个截图, 有疑惑, 为什么这里  $\\vec{u}$ 我们就默认是1, 哦, 这里又是这个数轴上的单位向量, 然后,  x横线的  $[x_1 \\quad x_2 ]$ 其实是什么呢, 是我们的2D的坐标系下的单位向量  $\\vec{i} , \\vec{j}$ , 投影到那个数轴上时候, 得到的数值,                       x    y   O       -5 -4 -3 -2 -1 0 1 2 3 4 5     v₂    v₁   OK, 这里我们记住了这个特性之后, 会发现对于 2D的空间来说, $[x_1 \\quad x_2 ]$ 是我们的单位向量  $\\vec{i} , \\vec{j}$ 在那个数轴上的投影, 那么现在, 我们比如有一个正常的arbitrary的matrix后, 比如 $\\begin{bmatrix}2 \\7\\end{bmatrix}$ 我们当然发现2和7就是对于x_1和x_2的, 而没有之前的vector地位不等价的疑惑了, 因为从分开的角度来说, 我们的2确实就和 $\\vec{i}$ 有关系, 我们的7确实就和 $\\vec{j}$ 有关系, 这是最后发生在一个数轴上的   在之前 乘 3, 其实对于  $\\vec{i},\\vec{j}$ 来说, 我们对于每一行都乘了3, which 回到之前的问题, $2\\times \\vec{v} \\cdot \\vec{w}$这个地方, 我们其实是$\\begin{bmatrix}2 \\cdot v_{row1} \\ 2 \\cdot v_{row2}\\end{bmatrix}$ 所以, 这里二维的单位向量都乘了3,   这个横向的vector又是怎么竖过来的呢?   Claude 回答   点积的本质可以从两个角度理解：几何投影和线性变换。 从几何角度看，当我们计算 $\\vec{v} \\cdot \\vec{w}$ 时，可以理解为 $\\vec{w}$ 在 $\\vec{v}$ 方向上的投影长度乘以 $\\vec{v}$ 的长度。这就是为什么当我们把 $\\vec{v}$ 放大两倍变成 $2\\vec{v}$ 时，点积值会变成原来的两倍 - 因为参考向量的长度变了。   从线性变换的角度看，横向的向量 $[a \\quad b]$ 定义了一个从二维到一维的映射规则：把空间中的任何向量 $\\begin{bmatrix} v_1 \\ v_2 \\end{bmatrix}$ 映射到数轴上的 $av_1 + bv_2$ 这个点。这不是描述空间中的一个位置，而是描述如何将其他向量映射到一维空间。   当我们计算点积 $[a \\quad b] \\cdot \\begin{bmatrix} v_1 \\ v_2 \\end{bmatrix} = av_1 + bv_2$ 时，实际上是在应用这个线性变换。当我们把 $[a \\quad b]$ 变成 $[2a \\quad 2b]$ 时，我们改变了映射规则，所以结果自然变为原来的两倍。   这种视角消除了点积中向量角色不对称的困惑 - 它们本来就是不同的：一个是变换规则，另一个是被变换的对象。这就是线性代数中”对偶性”的本质 - 同一个数学对象可以有两种不同但等价的解释。   我们得出来, 下面的想法, 不是把向量当作空间中一支“箭头”, 而是把它理解为“某种线性变换的具象表现” ,它代表的是一个作用规则，不是一个位置或方向   到12:37学不动了, 又回到了duality   2025-05-02 新的一天, 图书馆继续   首先是回顾, 我在refine完上面的文章之后, 我感觉又回到了之前的问题, 在这个文章中, 哪些内容, 还不够有intuition, 我感觉从二维到1维的时候, 还是有点困惑是, 为什么  $[x_1 \\quad x_2 ] \\cdot \\begin{bmatrix} x_m \\ x_n \\end{bmatrix}$ 为什么这里 $x_m$ 只需要和  $x_1$ 乘,  $x_n$ 只需要和  $x_2$ 乘, 这里还是有点疑惑  Claude say   从二维到一维的映射 首先，让我们回顾一下点积的核心问题：为什么在 $[x_1 \\quad x_2] \\cdot \\begin{bmatrix} x_m \\ x_n \\end{bmatrix}$ 中，$x_1$ 只与 $x_m$ 相乘，$x_2$ 只与 $x_n$ 相乘？ 这个问题涉及到向量的对偶性理解。从传统视角看，点积似乎是一个神奇的规则，但从线性变换的角度来看，这个规则非常自然。   从线性变换的角度理解点积   当我们把行向量 $[x_1 \\quad x_2]$ 看作是一个线性变换时，它定义了一种从二维空间到一维数轴的映射规则：   $f([a, b]) = x_1 \\cdot a + x_2 \\cdot b$   这个函数 $f$ 将二维空间中的任何向量映射到数轴上的一个点。这里的直觉是：      行向量 $[x_1 \\quad x_2]$ 不再是空间中的”箭头”，而是一个变换规则   这个规则告诉我们如何将二维空间压缩到一维空间   $x_1$ 和 $x_2$ 分别是基向量 $\\hat{i}$ 和 $\\hat{j}$ 在这个映射下的”影像”  OK,上面这个概念出现不懂, or 有问题, 跳 当我们计算 $[x_1 \\quad x_2] \\cdot [x_m \\\\ x_n]$ 时，我们实际上是在应用这个变换规则到向量 $[x_m, x_n]$ 上： $f([x_m, x_n]) = x_1 \\cdot x_m + x_2 \\cdot x_n$   对于 3. $x_1$ 和 $x_2$ 分别是基向量 $\\hat{i}$ 和 $\\hat{j}$ 在这个映射下的”影像”, 不懂, 再想   点积的本质理解 点积 $[x_1 \\quad x_2] \\times \\begin{bmatrix} x_m \\ x_n \\end{bmatrix} = x_1x_m + x_2x_n$ 之所以是这样计算，本质上是因为这是标准内积的定义方式。 但为什么是这样定义的呢？   1.从坐标分解的角度   标准基向量 $\\hat{i}$ 和 $\\hat{j}$ 是相互正交的。当我们将向量 $\\vec{v} = \\begin{bmatrix} x_m \\ x_n \\end{bmatrix}$ 表示为 $x_m\\hat{i} + x_n\\hat{j}$ 时，$x_m$ 和 $x_n$ 实际上是向量在这两个基方向上的投影长度。   同样，向量 $\\vec{u} = [x_1 \\quad x_2]$ 也可以视为 $x_1\\hat{i} + x_2\\hat{j}$。   当我们计算 $\\vec{u} \\cdot \\vec{v}$ 时，我们在计算： $(x_1\\hat{i} + x_2\\hat{j}) \\cdot (x_m\\hat{i} + x_n\\hat{j})$   展开后得到： $x_1x_m(\\hat{i} \\cdot \\hat{i}) + x_1x_n(\\hat{i} \\cdot \\hat{j}) + x_2x_m(\\hat{j} \\cdot \\hat{i}) + x_2x_n(\\hat{j} \\cdot \\hat{j})$   由于 $\\hat{i} \\cdot \\hat{i} = 1$，$\\hat{j} \\cdot \\hat{j} = 1$，而 $\\hat{i} \\cdot \\hat{j} = 0$（正交性），所以结果简化为： $x_1x_m + x_2x_n$   这就是为什么我们只需要将对应分量相乘并求和 - 正交基消除了交叉项！   2. 从线性变换的角度   当我们将 $[x_1 \\quad x_2]$ 视为线性变换时，它完全由对基向量的作用确定：      将 $\\hat{i}$ 映射到 $x_1$   将 $\\hat{j}$ 映射到 $x_2$   对任意向量 $\\begin{bmatrix} x_m \\ x_n \\end{bmatrix} = x_m\\hat{i} + x_n\\hat{j}$ 的作用是： $x_m$ 的部分贡献 $x_m \\cdot x_1$ $x_n$ 的部分贡献 $x_n \\cdot x_2$   总和是 $x_1x_m + x_2x_n$   我感觉这里LLM模型的解释是shit, 我觉得从svg的地方再开始自己理清楚一下应该可以清楚. 我们得到了一个数轴上的1和3, 和 $[1 \\quad 3]$ 然后这个数轴是在二维坐标系上的, 我重复一遍这个svg如下, 那么, 其实在这个数轴上, 等于1和等于3的vector本身, 就是使用二维平面上的标准基向量 $\\hat{i}$ 和 $\\hat{j}$ 构建的,                       x    y   O       -5 -4 -3 -2 -1 0 1 2 3 4 5     v₂    v₁   OK, 再回过来注意什么, 我们的 标准基向量 $\\hat{i}$ 和 $\\hat{j}$ 在所谓的第一次变换后, 变成了1, 在第二次变换后变成了3(我觉得这里还是可以按照一列一列来理解, 只不过这里一列就一个数字, 有两列), 那么是怎么在这个位置上的, OK, 这是我之前没解决的点, 我重新看了视频, 虽然, 在这个位置上, 我的意思是, 虽然  $v_1$  $v_2$ 在那个位置上好像要用到标准基向量 $\\hat{i}$ 和 $\\hat{j}$ 来固定, 但是想想是怎么固定的, 就是应该被怎么算出来在那个位置, 好像怎么都想不到只有一行的raw的 $[1 \\quad 3]$   视频5:46到6:04, 这里说想象一个线性变换, 然后直接把j拉到一个一维的数轴上-2位置, i还保持原来的位置(1,0), 只不过刚好是数轴的1, 这里的i的0其实没有体现出来     基向量 $\\hat{i} = \\begin{bmatrix} 1 \\ 0 \\end{bmatrix}$ 被映射到数轴上的位置1        基向量 $\\hat{j} = \\begin{bmatrix} 0 \\ 1 \\end{bmatrix}$ 被映射到数轴上的位置-2 这是因为这个变换将二维空间压缩成了一维空间，所以必然会丢失一些信息。 这个线性变换可以用行向量 $[1 \\quad -2]$ 来表示，意味着：       任何向量 $\\begin{bmatrix} a \\ b \\end{bmatrix}$ 会被映射到数轴上的 $1 \\cdot a + (-2) \\cdot b = a - 2b$ 问题… 如果按照 列空间的想法的话, 这里 $[1 \\quad-2] \\times \\begin{bmatrix} a \\ b \\end{bmatrix}$ 然后,  $a * [1] + a*[-2]$ , 哦这里混淆了, 不可以这样写, 这个矩阵乘法是不可以成立的, 点乘和矩阵乘法理论上是两种东西?                Transform       [   1   -2   ]                                     4   3       Vector       =            4     ·     1      +      3     ·     -2      寄了, 我感觉我没法在逻辑上理解为什么这个转秩一下是一样的   虽然过程混乱, 但是问题被总结为, 为什么对于$\\begin{bmatrix} 3 \\ 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 \\ 2 \\end{bmatrix}$和 $\\begin{bmatrix} 3 \\ 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 5 \\ 2 \\end{bmatrix}$ 是不是地位等价?  为什么3 只可以和5乘呢? 这个从2维到1维的变换中,  $\\begin{bmatrix} 3 \\ 4 \\end{bmatrix}$ 只被认为是变换器的角色   视频的8:45                Projection matrix:                                 ?   ?          Where î lands          Where ĵ lands                                               为什么要这样, 就是在这个穿过原点的数轴被建立之后, 这里的 $[x_1,x_2]$ 其实就是下面的 $\\hat{u}$ 的作为在二维坐标轴上的值, 但是注意同时也是标准基向量 $\\hat{i}$ 和 $\\hat{j}$ 到那个一维数轴上的投影长度,                                                                                                                      Line of symmetry                                      2                                 -1                                 -2                            î           û                                   ux                ux             Where do î and ĵ land?     [      ]     o 1     -1     1     -1       ![[Screenshot 2025-05-02 at 11.10.30 PM.png]]   OK, 那问题好像解决了, 就对于二维 $\\begin{bmatrix} a \\ b \\end{bmatrix}$ 来说, 它中作为一个点or一个箭头, 我们的 $a$ 和 $b$ 各是标准基向量 $\\hat{i}$ 和 $\\hat{j}$ 构建的, 如果要 onto 到我们的数轴上, 当然, 只能 grab 我们对应的数值                Transform       [        u          x           u     y      ]                                  x   y       Vector       =                   u       x          ·     x      +             u       y          ·     y      first projecting onto that vector   这里建立了数值和vector的联系, 就是说, 我们横向的一维度的object(指这里的 $u_x \\dots to\\dots u_y$), 可以认为是一个个变换运动(跳出2维), 但是同时, 这里把这个动作(们)对称回去, 也可以是我们的向量(也就是竖着的),  这里从直觉上不对称的投影行为, 被我们整理成一个个可以理解的变换了, 然后注意, 对称也实现了如何实现的, 在倒数第二个截图的位置, 在$\\hat{i}$ 在  $\\hat{u}$ 所在的数轴上是一个数值(这描述一个变换), 同时 $\\hat{u}$在 $\\hat{i}$ 的方向上, 如果被表示成一个数, 是$\\hat{u}$ 向量在二维坐标系中, $\\hat{i}$方向上的分量.  这里的数值是一样的, 这里的变换是对称的, 只是我们从图像上给他们赋予的实际意义不同, 太美了,这是duality.   Appendix   一张被淘汰的作图                                                                                                                      Line of symmetry                                     1                                  2                                 -1                                 -2                            î           û                                   ux                ux             Where do î and ĵ land?     [      ]     o 1     -1     1     -1      ","categories": [],
        "tags": [],
        "url": "/Dot-products%E7%82%B9%E7%A7%AF-and-duality-Chapter-9/",
        "teaser": null
      }]
